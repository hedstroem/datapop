[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.11.0","content-config-digest","4552fe1e1641c354","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://yourusername.netlify.app\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,25,26,37,38,49,50,61,62,72,73,84,85,95,96,106,107,118,119,130,131,144,145,155,156],"brunton",{"id":11,"data":13,"body":20,"filePath":21,"digest":22,"legacyId":23,"deferredRender":24},{"title":14,"description":15,"pubDate":16,"tags":17},"Chaos as an intermittently forced linear system","Steve Brunton \u003C3",["Date","2025-09-11T00:00:00.000Z"],[18,19],"modeling","system-identification","A few years ago I encountered [this article](https://arxiv.org/pdf/1608.05306) by Brunton et. al. which brought an interesting perspective to time series modeling.\nAt the time I was working with how to create effective time series features for regression models (linear regression, random forest, multilayer perceptron).\nThe type of features included but where not limited to:\n* Moving averages (MA) and exponential moving averages (EMA)\n* Moving variance / standard deviation\n* Windowed binning\n* Rolling autocorrelation at different lags\n* Batched fast Fourier transform (FFT)\n* [Sliding discrete Fourier transform (DFT)](https://quod.lib.umich.edu/cgi/p/pod/dod-idx/sliding-is-smoother-than-jumping.pdf?c=icmc;idno=bbp2372.2005.086;format=pdf)\n* Wavelet decomposition using filter banks\n* Training convolutional neural networks (CNN) and taking their filters\n\nWhat was interesting about the article was that they were using singular value decomposition (SVD) to extract signal modes\nand they studied how models of those can, for example, reconstruct latent attractors.\nUsing SVD is a good idea; to create features of orthogonal reoccuring modes.\nThe latent attractors were also highly interesting and seemed like a powerful tool for system identification.\n\n\n\n## Hankel alternative view of Koopman - HAVOK\n\nStart by picking a window size $w$. Take your time series $y_t, t = 0, 1, ..., N-1$ and organize it in a Henkel matrix $H(y_t, w)$.\nA Henkel matrix is of shape $(N - w) \\times w$ where the first row contains $y_0$ to $y_{w-1}$. \n\nDoing a SVD on $H = U \\Sigma V^T$ yields modes $V$ and their variances $\\Sigma$.\n\nTo contextualize this decomposition we exemplify using the Lorentz attactor as in the article.\n\n![...](/images/brunton/brunton_1.png)\n\nNow, having all three signals $x$, $y$ and $z$ would be too easy so the authors settle with only knowing $x$.\n\n![...](/images/brunton/brunton_2.png)\n\nNot that the sign of $x$ and $y$ tell which lobe we are currently in.\nTo highlight this we colored the \"butterfly\" using with a rainbow conditional on $x$.\nOne lobe has a red ear ($x > 0$) and the other a blue ear ($x \u003C 0$).\nArranging $x$ in a Henkel matrix and taking the SVD yields $U, \\Sigma$ and $V$.\nBy dividing the cumulative sum of $\\Sigma$ by the sum of $\\Sigma$ we get the cumulative share of variance explained,\nsimilar to we look at in a PCA. For this example we are interested when we have explained 99.9% of the variance \n(another way of saying it is that we leave 0.1% variance).\n\nFor $w = 32$ we get that $n_{features} = 6$ modes explains 99.9% of the variance:\n![...](/images/brunton/brunton_3.png)\n\nAnd for $w = 100$ the corresponding number is $n_{features} = 16$:\n![...](/images/brunton/brunton_4.png)\n\nIt is easier to get an overview of 6 rather than 16 modes so for this example we will continue with that.\nThe modes are visualized as weights over the window size $w$:\n![...](/images/brunton/brunton_5.png)\n\nAnd how those modes are weighted over time by $U$ for mode $i$ as $u_i(t)$ to compose $x$ can be seen below:\n![...](/images/brunton/brunton_6.png)\n\nThere is like a dance of harmonic modes, which is part of the appeal of the approach.\nUsing the first 3 modes $u_0, u_1$ and $u_2$ in a 3D plot one can reconstruct the attractor.\nSo without direct access to $y$ and $z$ one can still deduce that there are probably two poles of attraction.\n![...](/images/brunton/brunton_7.png)\n\nHarmonic problems of this nature are typically a more friendly environment for linear models like $\\frac{du}{dt}(u) = A u(t)$.\nHowever, the authors propose to add an unknown forcing term, which in this case would be $u_i(t)$ indexed outside of the feature set $i = n_{features}$ ($n_f$ for short).\nThis modifies the model as $\\frac{du}{dt}(u) = A u(t) + B u_{n_{f}} (t)$.\n\nThe authors motivate that $u_{n_{f}}$ can act as a forcing term with a non-Gaussian distribution. \nIts amplitudes seem to correspond to lobe changes as one can see by looking at the power $\\|u_{n_{f}}\\|^2$ in the graph below.\n![...](/images/brunton/brunton_8.png)\n\nI wonder if this is analogous to appoximating a sharp edge using sine waves.\nHigher and higher frequencies are needed to resolve the sharpness.\nWe will see.\nFor now lets focus on the modeling.\nFitting a sparse linear model yields the weights visualized in the heatmap below.\nLike in the article there is a clear dominance on the off-diagonals.\nAlso note that the forcing is primarily impacting the last $\\frac{du_{n_f-1}}{dt}$.\n![...](/images/brunton/brunton_9.png)\n\nThis particular model has a coefficient of explaination of $R^2 = 0.996$.\n\n## Noisy view of the problem\n\nThe insights from this method are cool. We have:\n* Linearlized $x$ using mode decomposition\n* Discovered that there is a probable two-pole attractor\n* Identified a signal for incoming lobe changes\n\nThe question is how well those insights work when we are dealing with real data.\nOne way to build an intuition is to add noise to the Lorentz attractor $x' = x + Z, Z \\sim \\mathcal{N}(0,1)$.\nHow will the methodology handle it?\n\nFirst of all, our 99.9% explained variance way of selecting $n_f$ will not work.\nWhite noise can not be decomposed by the SVD so it will show up in all modes.\nThere are [articles](https://arxiv.org/abs/1305.5870) for optimal thresholds for filtering $\\Sigma$.\nSince we know $\\sigma_{noise} = 1$ we can use the formula $\\tau_* = \\lambda_*(\\beta) \\sqrt{n} \\sigma_{noise},\\:\\beta = \\frac{N-w}{w}$ where\n$\\lambda_*(\\beta)$ is a shape parameter that depends on $\\beta$.\n$\\tau_* \\rightarrow n_f = 4$ for $w = 32$ and $\\tau_* \\rightarrow n_f = 9$ for $w = 100$.\nThis is the regime where $Z$ begins to dominate in size over the dynamics encoded in $x$.\nYou can observe this by noting that $u_0(t)$ is relatively smooth and $u_3(t)$ carries noise in the decomposition below:\n\n![...](/images/brunton/brunton_10.png)\n\nBecause the first modes have strong signal-to-noise ratio (SNR) one can still observe the latent attractor:\n\n![...](/images/brunton/brunton_11.png)\n\nHowever, this is where our luck \"ends\".\nWe know from the pure Lorentz attractor that we needed $n_f = 6$ to capture 99.9% of the signal modes.\nEffectively we have lost two modes ($w = 32$) due to noise.\nAlso, the assumed forcing term $u_{n_{f}}$ will by this definition also be dominated by noise.\n\n![...](/images/brunton/brunton_12.png)\n\nThere is still some correlation but it is not as clean as the previous example.\nThe performance of the model also drops to an $R^2 = 0.76$.\nThe coefficients show the same behavior with the alternating off-diagonals:\n\n![...](/images/brunton/brunton_13.png)\n\nThis means that the linearlization and attractor discovery still works okay.\nThe signal for quantifying forcing has lost some utility but perhaps there are ways of altering the method.\n\n## Altered HAVOK?\n\nWe know that the answer lies within the $\\geq n_f$ modes.\nInstead of just using $u_{n_f}$ we could try to condense $u_{n_f}, u_{n_f+1}, ..., u_{w}$ into one signal.\nOne naive way of doing it would be to sum up their powers $\\alpha = \\sum_{i = n_f}^w \\|u_i\\|^2$.\nThe answer to that is no, no amount of threshold calibration will save that signal.\n![...](/images/brunton/brunton_14.png)\n\nThe second idea $\\alpha = \\|\\sum_{i = n_f}^w u_i\\|^2$ also does not work.\n![...](/images/brunton/brunton_15.png)\n\nAnother idea is to train a neural network $f$ as a jointly with the linear model to automatically pick up on anything\n$\\frac{du}{dt}(u) = A u(t) + B f(u_{n_{f}} (t), u_{n_{f}+1} (t), ..., u_{w} (t))$. \nHowever, after testing the idea we found that the forcing term that the neural net can cook up freely is not included in the sparse linear $B$.\n![...](/images/brunton/brunton_16.png)\n\nHindsight is 20/20 and one probably should have started by just including all potential forcing features.\n![...](/images/brunton/brunton_17.png)\n\nThe model improves $R^2 = 0.85$ but now the forcing term is distributed.\nWeighting all the forcing terms together using the corresponding coefficients yields a signal which unfortunately does not do the job.\n![...](/images/brunton/brunton_18.png)\n\nFrom this I have deduced that there is likely no good instantaneous estimator for the forcing term.\nInstead it would have to be filtered over time.\nDetecting forcing in a system would be a wonderful vector of analysis if achievable.\nHowever it is unlikely that good general methods exist.","src/content/blog/brunton.mdx","8c552a1f5ce156ee","brunton.mdx",true,"anomaly",{"id":25,"data":27,"body":33,"filePath":34,"digest":35,"legacyId":36,"deferredRender":24},{"title":28,"description":29,"pubDate":30,"tags":31},"Anomaly detection","Methods for outlier identification",["Date","2025-09-23T00:00:00.000Z"],[32],"statistics","import ImageGallery from '../../components/ImageGallery.astro';\n\nAnomaly detection is the art of scoring observations $x$ by how anomalous they are.\nThe definition is that anomalies or outliers do not follow the distribution of the rest of the data.\nSee two univariate examples of anomalies below.\n\n![...](/images/anomaly/anomaly_1.png)\n\nThe first is the type most people think of (for example, detecting abnormally high pressures or temperatures) but one has to remember that the second one is equally anomalous.\nThis becomes clearer in a multivariate setting where anomalies can be a mix of inside and outside the data range.\n\n![...](/images/anomaly/anomaly_2.png)\n\nAnomaly 0 matches the anomaly *outside* univariate example while anomaly 2 matches anomaly *inside* on the axis between bimodal distributions.\nAnomaly 1 and 3 will not show up as anomalous until one considers the correlation that the bimodal distribution has.\n\nIn higher dimensions there are more and more combinations of \"dead space\" where anomalies can appear.\nGenerally one want to reduce the problem space to as few dimensions as possible.\n\n## Anomaly detection on a single big blob by distribution fitting\n\nIf one is lucky enough to have a *single big blob distribution* (SBBD) then anomaly detection can be done robustly by fitting\na probablity distribution that allows one to score the likelihood of incoming observations.\nThe most commonly used distribution is the Gaussian.\nIt occurs in a lot of places due to the law of large numbers and even if it does not perfectly fit the data it is still good-enough for a lot of purposes.\n\nActually, when working with the Gaussian one does not even bother with likelihood as the z-score has an intuitive interpretation.\nThe z-score is the standardized observation $z = \\frac{x - \\mu}{\\sigma}$, where $\\mu$ is the mean and $\\sigma$ the standard deviation.\nBoth of these parameters can be efficiently estimated from samples $\\mu_s, \\sigma_s$.\nA deviation of $\\pm 1$ z-score means that the observation is one standard deviation away.\nOne typically thresholds the z-score $|z| \u003C \\tau_z, \\tau_z > 3$.\nThis is because at 4 standard deviations you only expect 0.006% of the data to fall outside of that interval during normal circumstances.\n\nHere are the z-scores for the anomaly *outside* example after 100 observations of calibration of $\\mu_s, \\sigma_s$.\n\n![...](/images/anomaly/anomaly_3.png)\n\nAnd for the anomaly *inside* example:\n\n![...](/images/anomaly/anomaly_4.png)\n\nThe Gaussian is a reliable workhorse and great for anomaly detection for SBBD.\nIts most critical quality for anomaly detection is how it tapers off.\nThe likelihood of an observation $p_{\\mathcal{N}}(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{\\sigma^2}}$ decays as $e^{-z^2}$.\nThis means that Gaussian assumes that the observations do not come from a fat tailed distribution.\n\n... but what if they are?\n\nThis has always bothered me, so let's try something!\nTwo fat-tailed distributions come to mind, the Laplace distribution and the Cauchy distribution.\nLaplace tapers off as $e^{-|z|}$ and Cauchy as $\\frac{1}{z^2}$.\nOut of these two one prefers to work with the Laplace distribution because the Cauchy distribution is pathological.\nThis means that there is no \"exact\" estimators for its parameters.\n\nThe likelihood of the Laplace distribution is \n$p_{\\mathcal{L}}(x | \\mu, b) = \\frac{1}{2b} e^{-\\frac{|x - \\mu|}{b}},$\nwhere one typically fits $\\mu$ using the median and the scale parameter $b$ as $\\hat{b} = \\frac{1}{n}\\sum |x_i - \\hat{\\mu}|$.\nInstead of directly comparing likelihoods $p_{\\mathcal{N}}$ to $p_{\\mathcal{L}}$ we typically use the log-likelihoods $log\\:p_{\\mathcal{N}}$ and $log\\:p_{\\mathcal{L}}$.\nThe joint probablility of a sequence of observations becomes a sum of log likelihoods, which is easier computationally.\n\nIf one wants a zero-centered process then one can work with the centered log-likelihood (CLL), which is $log\\:p_{X}(x) - \\mathbb{E}(log\\:p_{X}(x))$.\nThis means that we are correcting for the amount of log-likelihood we expect to see.\nBy definition this is just the negative entropy $H_X = -\\mathbb{E}(log\\:p_{X})$ so\n$\\text{CLL}_X = log\\:p_X + H_X$.\n\nFor a Gaussian distribution we have\n$$\n\\begin{align*}\nlog\\:p_{\\mathcal{N}}(x,\\mu,\\sigma) &= -\\frac{1}{2} log(2 \\pi \\sigma^2) - \\frac{1}{2}\\bigg(\\frac{x - \\mu}{\\sigma}\\bigg)^2,\\\\\nH_{\\mathcal{N}}(\\mu,\\sigma) &= \\frac{1}{2} log(2 \\pi \\sigma^2) +  \\frac{1}{2},\\\\\n\\text{CLL}_\\mathcal{N}(x,\\mu,\\sigma) &= - \\frac{1}{2}\\bigg(\\frac{x - \\mu}{\\sigma}\\bigg)^2 + \\frac{1}{2},\n\\end{align*}\n$$\nand for a Laplace distribution\n$$\n\\begin{align*}\nlog\\:p_{\\mathcal{L}}(x,\\mu, b) &= -log(2 b) - \\frac{|x - \\mu|}{b},\\\\\nH_{\\mathcal{L}}(\\mu, b) &= log(2 b) + 1,\\\\\n\\text{CLL}_\\mathcal{L}(x,\\mu, b) &= - \\frac{|x - \\mu|}{b} + 1.\n\\end{align*}\n$$\n\nWith the $log\\:p_{X}$ we can determine if a Gaussian or Laplace distribution is more appropriate on some non-calibration data.\nIf $\\sum log\\:p_{L}(x, \\mu_s, b_s) > \\sum log\\:p_{\\mathcal{N}}(x,\\mu_s,\\sigma_s)$ then the Laplace distribution assumption dominates.\n\nFor our anomaly *outside* example before the $\\sum log\\:p_{\\mathcal{N}}(x,\\mu_s,\\sigma_s) \\approx -579.3$ and $\\sum log\\:p_{L}(x, \\mu_s, b_s) \\approx -612.6$,\nmeaning that is it more likely Gaussian than Laplace, which is what we simulated the problem using. \nIf we simulate a version of the same problem using a Laplace distribution we get a problem that looks something like this:\n![...](/images/anomaly/anomaly_5.png)\n\nNote that in the calibration window there is an observation $x > 10$ which is not anomalous.\nThis is the fat-tailed nature of Laplace distributions examplified.\nFor this problem the $\\sum log\\:p_{\\mathcal{N}}(x,\\mu_s,\\sigma_s) \\approx -788.9$ and $\\sum log\\:p_{L}(x, \\mu_s, b_s) \\approx -725.3$,\nmeaning that the Laplace distribution assumption dominates.\n\nSo what is the consequence on the outlier status? For the anomaly *outside (Laplace distribution)* example the anomaly has \n$p_\\mathcal{L} = 1.23 \\cdot 10^{-7}$, $p_\\mathcal{N} = 1.14 \\cdot 10^{-20}$, \n$LL_\\mathcal{L} = -15.9$, $LL_\\mathcal{N} =-45.9$.\nFor $\\mathcal{L}$ the probablility of such an observation is about 1 in 10 million while for $\\mathcal{N}$ it is an improbable observation.\nTo contextualize; the rare observation $x \\approx 10$ at index 24 has \n$p_\\mathcal{L} = 1.38 \\cdot 10^{-5}$, $p_\\mathcal{N} = 1.56 \\cdot 10^{-10}$, \n$LL_\\mathcal{L} = -11.2$, $LL_\\mathcal{N} =-22.6$.\n\nA 1 in 100000 is still a rare event to probably want to be flagged as an outlier.\nEspecially when another rare event happens relatively soon afterward.\nBy keeping a rolling sum or average in a window one can gauge how likely a sequence of observations are.\nThis type of methodology looks for clustered anomalies or even change instead of single anomalies.\n\n## Anomaly detection using histograms\n\nThe method described above will not work for the anomaly *inside* example because its anomaly is not an extreme value.\nAnother staple is to model the distribution using a histogram and use that for anomaly scoring or change detection.\nBy building up the histograms over a calibration window we can use it to score incoming observations.\n\nDefining an anomaly in this context could be something as *something that does not fit into one of the bins*.\nAn example of this can be seen below.\nNote that this method also captures outliers in bins $x \\sim 2$ and $x \\sim 7$.\nBootstrapping with noise can somewhat mitigate it but it is generally a problem with histogram based methods.\n\n![...](/images/anomaly/anomaly_6.png)\n\nOf course, one can always modify the distribution like smoothing it with a Gaussian kernel.\nThis can be set up so that $1\\:\\sigma = 1\\:\\text{bin}$.\nThat works fairly well for this example because $x \\sim 2$ and $x \\sim 7$ are close to bins with observations\nbut $x \\sim 5$ is to far away that the smoothed frequency $\\approx 0$.\n\n![...](/images/anomaly/anomaly_7.png)\n\nAnother problem with histogram based methods is that they become harder to construct in higher dimensions.\nUniform grid sizes makes the number of bins explode and it is not feasible for most real world problems.\n\n## Anomaly detection using other machine learning methods\n\n### Isolation forests\n\n[Isolation forests](https://www.lamda.nju.edu.cn/publication/icdm08b.pdf) are a classical example of anomaly detection.\nDuring training each tree gets a random subset of data and randomly splits it along random features (that's a lot of randomness).\nThe core idea of the anomaly detection is based on how long it takes to isolate the data point.\nThe theory is based on binary search trees (BST) where the average path length of an unsuccessful search is\n$$\nc(n) = 2 H(n-1) - 2(\\frac{n-1}{n}),\n$$\nwhere $n$ is the dataset size and $H(i)$ is the harmonic number, \nwhich can be estimated using Eulers constant $\\gamma$ as $H(i) \\approx log(i) + \\gamma, \\gamma = 0.5772...$ \nThis they use to construct the anomaly score as $s(x, n) = 2^{-\\frac{E(h(x))}{c(n)}},$ \nwhere $E(h(x))$ is the average of path lengths $h(x)$ from a collection of isolation trees.\n\nUsing the scikit-learn implementation of isolation forests yields the following decision function (before thresholding).\nNote that most points around the Gaussian modes are outlier-esque and there is no additional penalty for $x \\sim 5$.\n\n![...](/images/anomaly/anomaly_8.png)\n\nFor this type problem it seems to be the way isolation forests scores anomalies.\nTo verify this we generate the same example but with Laplace distributions instead of Gaussians.\nNote once more the banded nature of the decision function.\nThere are no more hyperparameters to configure to better model our true distribution.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/anomaly/anomaly_9.png\",\n      alt: \"...\",\n      caption: \"Anomaly *inside* but with Laplace distributions instead of Gaussian\"\n    },\n    {\n      src: \"/images/anomaly/anomaly_10.png\",\n      alt: \"...\",\n      caption: \"Isolation Forest decision function\"\n    },\n  ]}\n  layout=\"carousel\"\n/>\n\n### One-class support vector machines\n\nOne-class support vector machines (SVM) transform data into a higher-dimensional space \nand finding the smallest region that encloses most of the data (up to some slack variable).\nFor a radial basis function (RBF) the transformation is $e^{-\\epsilon\\|x - x_b\\|_2^2}$,\nwhere $\\epsilon$ is the inverse variance (for kernel scaling) and $x_b$ is the \"basis\",\na reference data point or a support vector.\nDuring training an optimizer finds support vectors that are used to define a (hyper)margin.\nDistance to this margin is what is used for the decision function.\n\n![...](/images/anomaly/anomaly_11.png)\n\nSimilarly to isolation forests this mainly leads to a banded decision function.\n\n### Local outlier factor\n\nIn a nutshell, local outlier factor (LOF) looks at the local density of the $n$ nearest neighbors.\nA point is more likely to be anomalous if its nearest neighbors are further away than normal data points.\nThis works well for our example as we get less banding and more focus on local phenomena,\nnote that $x \\sim 5$ stands out clear with no neighbors.\n\n![...](/images/anomaly/anomaly_12.png)\n\nThe downside of LOF is that nearest neighbor methods are slow on big datasets.\nIt also does not handle clustered outliers well as clusters means neighbors.\nSo the definition makes it appropriate for some scenarios but not for all.\n\n### Clustering by K-means\n\nAlternative methods for anomaly detection can be clustering the data \nand using *distance from nearest cluster* or *isolated clusters* as a heuristic for anomalous observations.\nMethods such as K-means does this well for Euclidean distances and can be adapted\nto work with streaming data.\n\nThis methodology is the most (visually) intuitive in 2D:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/anomaly/anomaly_13.png\",\n      alt: \"...\",\n      caption: \"Centroids (n = 64) for K-means. Calibration size: 200\"\n    },\n    {\n      src: \"/images/anomaly/anomaly_14.png\",\n      alt: \"...\",\n      caption: \"Distance to closest centroid\"\n    },\n  ]}\n  layout=\"carousel\"\n/>\n\nAnd the result for anomaly *inside* example (we plot the centroids at the end of the calibration window):\n![...](/images/anomaly/anomaly_15.png)\n\nThis method essentially constructs a histogram of [Voronoi](https://en.wikipedia.org/wiki/Voronoi_diagram) regions.\n\n## The biggest hurdle\n\n*What is normal?*\n\nClustering by K-means yields is our best method yet but lets break it!\nAdding noise dimensions breaks the analysis because the clustering will focus on compressing uncompressible noise.\nIn the example below we have added 30 noise Gaussians $u_i \\sim N(0, 10)$:\n\n![...](/images/anomaly/anomaly_16.png)\n\nThe centroids no longer capture any structure in the first two dimensions.\nThis yields our anomaly detection useless as we are no longer accurately representing what we care about the data.\nWhat we just did is an extreme example but it hints at something that one has to be aware of with real-world data.\n\nIn a nutshell; data is noisy and correlations are weak.\nAnother thing is that most outliers are simply extreme values, which are easier to detect than distribution deviations.\nIn the literature people talk about carefully selecting signals, transforming them with PCA to compress correlations\nand looking for high-kurtosis features when enabling anomaly detection. No free lunch.\n\n### Sparse K-means\n\nOf course people have thought about ways of adressing this.\nBy weighting the distance contribution for each dimension one could in theory get a clustering that partially ignores noise.\nThe distance between an observation $x$ and the centroid $c$ with corresponding weights $w$ can be written as $w\\|x-c\\|_2^2 = \\sum_d w_d(x_d - c_d)^2$,\nwhere $d$ is the number of dimensions. We do reserve for the fact that one can have other distances than Euclidean, for example Manhattan, cosine or others more suitable for categorial variables.\n\nSparse K-means can iteratively be solved as:\n* Initialize $w$ uniformly.\n* Standardize data $z = \\frac{x - \\mu_s}{\\sigma_s}$ using sample mean $\\mu_s$ and standard deviation $\\sigma_s$.\n* Initialize centroids $c$ randomly, using random data points or using standard K-means.\n* For each iteration to converge the centroids:\n  * Calculate $d_c = w\\|z-c\\|_2^2$.\n  * Select minimum distance for each data points as cluster label.\n  * Update centroids as the average of all assigned data points.\n  * **This is where regular K-means ends**.\n  * Calculate the cluster sum of squares per dimension $B = \\frac{1}{N} \\sum_c \\big(|z \\in c| \\cdot c^2\\big)$.\n  * Calculate soft-threshold per dimension as $\\tau_d = max(0, B_d - \\lambda)$.\n  * Calculate new weights per dimension as $w_d = \\frac{\\tau_d}{\\sum_d \\tau_d}$.\n\nSo what happened here?\n$B$ is a sum of the squared centroids, weighted by the number of points belonging to that centroid.\nSince we are working with standardized data and if a dimension is uncorrelated noise, then $\\mathbb{E}(c_d) = 0$.\nIf this is experienced across all centroids then there is no signal in how the clusters are treating $d$.\nSo one iterpretation is that $B_d$ is the cluster signal-to-noise ratio (SNR) for dimension $d$.\n$\\lambda$ is our penalty which encourages sparsity by pruning low SNR.\nFor the same example with above, $\\lambda = 0.5$ yields:\n\n![...](/images/anomaly/anomaly_17.png)\n\nWith $w_0 \\approx 0.5$ and $w_1 \\approx 0.5$ while the rest $w_i = 0$.\n\n### Angular K-means to capture the cluster boundaries\n\nUsing sparse K-means one could reasonably track the density of data distribution.\nOne could be interested in knowing the cluster boundaries.\nTo do so one takes the established centroids $c$ and their corresponding data points $x_c$.\nFor each centroid:\n* Calculate weighted directions $u_c = \\frac{(x_c - c)}{\\|(x_c - c)\\|} \\frac{w}{\\|w\\|}$.\n* Cluster $u_c$ using K-means to get angular centroids $c_{ang}$.\n* For each angular centroid $c_{ang}$:\n  * Take directions $u_c^a$ belonging to $c_{ang}$.\n  * Calculate projections $p_c^a = (\\frac{w}{\\|w\\|} (u_c^a - c)) \\cdot \\frac{c_{ang}}{\\|c_{ang}\\|}$.\n  * Take $x_c$ with the highest projection to be this directions furthest representative.\n\nHere are a few examples of what it can look like:\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/anomaly/anomaly_18.png\",\n      alt: \"...\",\n      caption: \"n(clusters) = 2 with n(directions) = 12. Calibration size: 200\"\n    },\n    {\n      src: \"/images/anomaly/anomaly_19.png\",\n      alt: \"...\",\n      caption: \"n(clusters) = 6 with n(directions) = 12. Calibration size: 200\"\n    },\n    {\n      src: \"/images/anomaly/anomaly_20.png\",\n      alt: \"...\",\n      caption: \"n(clusters) = 12 with n(directions) = 12. Calibration size: 200\"\n    },\n    {\n      src: \"/images/anomaly/anomaly_21.png\",\n      alt: \"...\",\n      caption: \"n(clusters) = 12 with n(directions) = 6. Calibration size: 200\"\n    },\n  ]}\n  layout=\"carousel\"\n/>\n\nIn theory, if the boundaries are good then one could figure out which clusters border each other:\n\n![...](/images/anomaly/anomaly_22.png)\n\nFiguring out how to combine the boundary points is not a trivial task.\nThe simplest approach is to rerun the angular clustering to calculate the points from the joined clusters.\nAlso note, because this is a 2D problem we get these nice lines between the border points.\nIn higher dimensions the lines becomes surfaces for 3D, volumes for 4D e.t.c.\nLong story short, we use the border points and not the lines.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/anomaly/anomaly_23.png\",\n      alt: \"...\",\n      caption: \"n(clusters) = 5 with n(directions) = 12. Calibration size: 200\"\n    },\n    {\n      src: \"/images/anomaly/anomaly_24.png\",\n      alt: \"...\",\n      caption: \"n(clusters) = 10 with n(directions) = 8. Calibration size: 200\"\n    },\n  ]}\n  layout=\"carousel\"\n/>\n\nHere is an example of anomalies found approximately 20% outside of the boundary:\n\n![...](/images/anomaly/anomaly_25.png)","src/content/blog/anomaly.mdx","51d8f0296f851343","anomaly.mdx","incremental-median-estimation",{"id":37,"data":39,"body":45,"filePath":46,"digest":47,"legacyId":48,"deferredRender":24},{"title":40,"description":41,"pubDate":42,"tags":43},"Incremental median estimation","A frequentist ping-pong estimator for the median",["Date","2025-07-15T00:00:00.000Z"],[32,44],"streaming-data","import ImageGallery from '../../components/ImageGallery.astro';\n\nA couple of years ago I thought to myself:\n\n*\"I know how to estimate a cumulative average, but how does one estimate the median in the same way?\"*\n\nThis was an interesting question because while averages are great, they still suffer greatly from outliers. \nThis led me down a track that ended on a stack overflow thread of cumulative median esimation, and by extension, any quantile.\nThe algorithm is simple and uses a frequency approach to the problem.\n\nIn a nutshell, if the distribution is stationary then we expect a 50/50 split of higher and lower observations around our median.\nThe way we translate this into an algorithm is that higher/lower nudges our median estimate, and if the higher/lower observations are in balance then the nudges cancel out.\nIt can be coded up as this:\n\n```python\ndef update_median_estimate(\n        median_estimate: float, \n        observation: float, \n        nudge: float\n    ) -> float:\n    diff = median_estimate - observation\n    sign = diff / abs(diff)\n    new_median_estimate = median_estimate - nudge * sign\n    return new_median_estimate\n```\n\nThe median estimate can just be initialized to the first observation. This leave the nudge factor as a parameter. Here is a graph of the median estimations applied to Gaussian noise with two different nudges:\n\n![Two incremental median estimations](/images/incremental_median/inc_1.png)\n\nNote that nudge = 0.1 makes the estimate converge quicker than nudge = 0.01, but, the trade-off is that it is also more sensitive to noise. So, what is a good nudge factor?\n\nOne way is to approach is heuristically; say that we expect the estimator to approximately reach the median in M steps. \nSince the median is the 50th percentile; a worst case scenario where we start at the min or max means that we have to step 50 percentage units. \nThis means that we could choose $\\frac{max(max - median,\\: median - min)}{M}$ as the nudge factor. \nSince we do not know the median we can use the range $R = max - min$ as an upper bound $\\frac{R}{M}$. This is an extreme scenario where the median is close to either the min or max.\nFor the uniform scenario where the median lies dead in the middle between min and max it is twice as big, so then we will get to our estimate in $\\frac{M}{2}$ steps.\n\n![Incrementally estimating max and min](/images/incremental_median/inc_2.png)\n\nWith this method we have a bit more control, as can be seen in the following graph:\n\n![Comparison of different R/M nudges](/images/incremental_median/inc_3.png)\n\nEyeballing this particular example gives the M = 30, 16 steps and M = 120, 91 steps to get to the sample median. \nNow, there additional steps we can take to ensure a fair(er) nudging toward the median and it relates to $R$.\nWe do not expect our estimate of R to be close to the truth after only 3 or 10 observations, so what do we expect it to be and can we correct for that?\n\nFor $n$ samples selected uniformly on the interval [0,1) the expectation of the maximum is $\\frac{n}{n+1}$ and the minimum is $\\frac{1}{n+1}$.\nThis means the expectation of the range is\n$$\nE(R|n) = \\frac{n}{n+1} - \\frac{1}{n+1} = \\frac{n-1}{n+1}.\n$$\n\nThe intuition behind this is that $n$ samples split the line into $n+1$ sections and ordering them by size gives $1$ and $n$ as the first and final sections.\nOf course, our data is not likely uniformly distributed, but the quantiles are by definition.\nThis means that after, for example $n = 9$, we would have expected to seen 10% and 90% percentile with an 80 percentile range.\nBy $n = 99$ we will have seen from the 1st to the 99th percentile and subsequently the $n = 999$ sample gives us 0.001 to 0.999 quantile.\nThere are two take-aways from this observation.\n\nThe first is that we can invert the formula $R = \\frac{n-1}{n+1} \\rightarrow n = -\\frac{R+1}{R-1}$.\nThis is handy if we are looking for a specific quantile $q$, it corresponds to the range $R_q = 1-2q$ and by our formula $n = \\frac{1-q}{q}$. \nThis only makes sense for $q \u003C 0.5$ so for $q > 0.5$ we find its mirrored buddy $\\tilde{q} = 1 - q$. \nNow, specifically for the median this collapses to $n = 1$ but if we are going to look at any quantile then this will come in handy.\nIn essence, this estimate of $n$ will tell you when we it is likely to have seen the quantile we are estimating.\n\nThe second take away is that we can use the formula to create a better estimate $R_n$ out of our naive incremental $R$ as $R_n = \\frac{R}{\\frac{n-1}{n+1}} = \\frac{R(n+1)}{n-1}$. \nThis means that the range will scaled up to conpensate for likely unobserved range. This means that the full nudge heuristic is now $\\frac{R(n+1)}{M(n-1)}$. \nSee two examples below of the difference, one cherry-picked (note that the correction is only relevant when we need the nudges to be large) and one regular:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_median/inc_4.png\",\n      alt: \"Improved convergence example 1\",\n      caption: \"Cherry-picked example\"\n    },\n    {\n      src: \"/images/incremental_median/inc_5.png\",\n      alt: \"Improved convergence example 2\",\n      caption: \"Regular convergence\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nIt is not much but it is something.\n\nThe elephant in the room are the variations in the estimate. \nSince we are observing random noise the estimates will follow the random sequences of nudges like a random walk.\nWhat we know about random walks of $n$ steps is that the expectation $E(\\mathcal{Z}_n) = 0$ with the variance $E(\\mathcal{Z}_n^2) = n$.\nSo if we have a random walk then we would expect the standard deviation to grow like $\\sqrt{n}$, meaning that we should shrink the nudges to compensate for this effect.\nOf course, we would only apply this after $M$ steps, when we have actually expected to converge to the median.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_median/inc_6.png\",\n      alt: \"Test of random walk penalty of the nudge amplitude\",\n      caption: \"Example 1: Gaussian\"\n    },\n    {\n      src: \"/images/incremental_median/inc_7.png\",\n      alt: \"Test of random walk penalty of the nudge amplitude on an exponential distribution\",\n      caption: \"Example 2: Exponential\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nBut, our system is not a true random walk because is it still an approximation of the median, so it will have some drift towards the true distribution median. \nSay that we are targeting the $q_t$ quantile (0.5 in case of the median) with $dq$ nudges (0.1 for our example) of our estimate $q$. \nIf $q = q_t$ then we have a 50/50 of being nudged in either direction, say + happens, then we are at $q + dq$. \nThis has now shifted the probabilities as we now have a 40/60 chance in favor of going back towards $q_t$.\nIf + happens again then we are at $q + 2dq$, meaning now a 30/70 chance next time. \nThe most extreme scenario has $dq = q_t$ when $q_t = 0.5$ as it always reverses its previous step, meaning that the distribution is 0: 25%, $q_t$: 50%, 1: 25%.\n\nOf course, we want to be able to assume any $dq$. \nTo get an intution behind this we simulated using $dq = \\frac{1}{50}$ and noted that is approximated a Gaussian distribution.\nIn the limit where $dq \\rightarrow 0$ it makes sense because then the drift back towards the median is less dominant. \n![Histogram of quantile estimations](/images/incremental_median/inc_8.png)\n\nWe run more experiments with different fractions of $\\frac{dq}{q_t}$, calculate the standard deviation, resulting in:\n![Experiments logging standard deviation of quantile estimations as a function of dq/qt](/images/incremental_median/inc_9.png)\n\nThe curve fitting is easy as it is a square root of the fraction and $x = 0.5$ neatly intersects $y = 0.25$. \nThis means that empirically we have arrived at the rubber band random walk standard deviation $\\sigma = \\frac{1}{\\sqrt{8}}\\sqrt{\\frac{dq}{q_t}} = \\sqrt{\\frac{dq}{8q_t}}$. \nNote that this does not depend on $n$, which we conjectured above. \nSimiarly to what we have done before, we can shuffle expressions around to find the fraction given a target standard deviation $\\sigma$ as $\\frac{dq}{q_t} = 8\\sigma^2$.\nIf we were to calibrate a 95% confidence interval ($\\pm 1.96\\sigma$) over $\\pm 1$ percentiles ($\\rightarrow \\sigma = \\frac{0.01}{1.96}$) then $\\frac{dq}{q_t} \\approx 0.00021 \\rightarrow dq = 1/9604$ ($q_t = 0.5$).\nFor a more liberal $\\pm 2.5$ percentiles $\\frac{dq}{q_t} \\approx 0.0013 \\rightarrow dq = 1/1536.64$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_median/inc_10.png\",\n      alt: \"Example 1 of random walk confidence interval nudges\",\n      caption: \"Example 1\"\n    },\n    {\n      src: \"/images/incremental_median/inc_11.png\",\n      alt: \"Example 2 of random walk confidence interval nudges\",\n      caption: \"Example 2\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nThat has (kind-of) solved it! Now there are two possibilities to take this further, the first is to generalize to any quantile and the second is to adress non-stationarity.\nBut we save those for later posts :)","src/content/blog/incremental-median-estimation.mdx","ac4572655c70f837","incremental-median-estimation.mdx","embeddings",{"id":49,"data":51,"body":57,"filePath":58,"digest":59,"legacyId":60,"deferredRender":24},{"title":52,"description":53,"pubDate":54,"tags":55},"Embedding texts into latent spaces [WIP]","",["Date","2025-09-29T00:00:00.000Z"],[56],"natural-language-processing","For my [masters thesis](https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1323994&dswid=3705) I worked on [Word2Vec](https://arxiv.org/pdf/1301.3781) style models. \nIt was in 2019 and [transformers](https://arxiv.org/pdf/1706.03762) were just starting to gain traction.\nGPT-2 was revealed and [OpenAI famously said they would not release the model](https://www.youtube.com/watch?v=AJxLtdur5fc) because they were afraid of the potential applications.\nIt was a different time.\n\nSince then natural language processing (NLP) tools has become more accessible than ever.\nNLP has basically become the poster boy for AI. \nThere are countless pay-per-token APIs and open source models to download from places like [Hugging Face](https://huggingface.co/models).\n\nLatent spaces have always fascinated me, where complex but regular shapes can be expressed with a few parameters.\nMy mind was blown then I first learned about autoencoders.\nThe concept made so much sense to me and it was amazing that one could train a system such a system end-to-end.\nAnd naturally I was intrigued to find out that this can be done for text as well.\n\nEmbeddings are normalized vectors $v \\in \\mathbb{R}^d, \\|v\\| = 1$.\nWord2Vec trains a vocabulary where a word $w$ is translated to a vector $v_w$.\nFor the latest generation of large language models (LLMs) one typically converts an entire text $s$ into a vector $w_s$.\nDocument embedding was actually the topic of my thesis, \nI was comparing directly training document embeddings \n([Doc2Vec](https://arxiv.org/pdf/1405.4053), [Doc2VecC](https://arxiv.org/pdf/1707.02377))\nversus creating them from, for example, word embeddings as $w_s = \\frac{1}{|s|} \\sum_{w \\in s} v_w$.\nIt worked alright for most tasks but most of those methods are redudant now.\n\nWhen training a model to produce embeddings the goal is to make sure that similar texts point in the same direction.\nThis is the same as saying that the scalar product / cosine distance should be close to 1 for two vectors $v_{s_1} \\cdot v_{s_2} = cos(\\alpha)$.\nFor Word2Vec these were also additative so that $v_{king} - v_{man} + v_{woman} \\approx v_{queen}$, allowing one to query contextually.\nThe additative nature comes from how the model is trained. \nLLM embeddings to not allow for this but instead modern embedding models allow queries, which can warp the embedding space.\n\nIf I were to work with text analysis today I would download a good, small open source model from Hugging Face.\nIt is September 2025 as I am writing this and one of the top trending embedding models is a model from Alibaba called [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B).\nIt scores well on the [MTEB multilingual leaderboard](https://huggingface.co/spaces/mteb/leaderboard), which is good because as a European you are likely to work with more languages than English.\n\n600 million weights is lightweight enough to run on your personal computer, which I prefer when prototyping.\nUsing the library `sentence_transformers` one can create embeddings in a jiffy:\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n\nsentences = [\n    \"The weather is lovely today.\",\n    \"I am a cow. Moo!\",\n    \"Fint väder idag.\"\n]\nembeddings = model.encode(sentences, prompt = \"Treat Swedish and English equally\")\nsimilarities = embeddings @ embeddings.T\nprint(similarities)\n```\n```\n[[1.    0.677 0.898]\n [0.677 1.    0.668]\n [0.898 0.668 1.   ]]\n```\nIf you do not have the model downloaded then model call will do it for you.\nOne can also download the model artifacts separately and put them in that folder, which is what I did.\nThis allow you create embeddings offline, which is neat.\nI did use the OpenAI embeddings API for a while when it came out (the cost for creating embeddings is low, storing them and quering however...) but not sending potentially sensitive data is preferable.\n\nBy the way, removing that prompt \"Treat Swedish and English equally\" results in the following similarities:\n```\n[[1.    0.442 0.685]\n [0.442 1.    0.391]\n [0.685 0.391 1.   ]]\n```\nSo warping the embedding space will impact downstream use like clustering or visualization.","src/content/blog/embeddings.mdx","6649b7cc91c663fe","embeddings.mdx","my-leadership-playbook",{"id":61,"data":63,"body":68,"filePath":69,"digest":70,"legacyId":71,"deferredRender":24},{"title":64,"description":53,"pubDate":65,"tags":66},"My leadership playbook",["Date","2025-09-28T00:00:00.000Z"],[67],"leadership","## Sections\n\n- Aspects of being a leader\n    1. Orientation\n    2. Why should people come to us?\n    3. Execution matters and feedback on execution matters\n    4. Reasonable restraint\n    6. Your team(s)\n    7. Showing character and good values\n- My values\n- Playbook\n    1. Having and tracking concrete targets, goals and progress\n    2. Data strategy\n    3. \"We have too much bread and too little butter\"\n    4. Sourdough management\n- Bite-sized takeaways\n\n\n# Aspects of being a leader\n\n## 1. Orientation\n\nAll positions of leadership are unique.\nIt is therefore fitting that orientation is fundamental.\nIf something is unclear after orientation then it is probably going to be something you and your team will struggle with.\nI think the Drexler / Sibbet Team Performance Model (TPM) captures this in an excelent way.\n\n![Team Performance Model](https://i0.wp.com/www.robertmcneil.com/wp-content/uploads/2016/01/Drexler_Sibbet_Model.jpg?resize=1024%767&ssl=1)\n[Source](https://davidsibbet.com/process-models/)\n\nThe interpretation of TPM is; if anything fails like \"Why can't we set meaningful goals?\" then the root cause it probably\na lack of trust (step 2) or confusion about why we are here in the first place (step 1).\n\nWhen leading a group one must be able to answer \"why are we here?\" in a way that balances \nthe polar-opposite question \"why should we not be here?\".\nThese questions are be layered like an onion of course because there are layers that are more and less paletable.\n\nYou as a leader have a big impact on the group culture, so your answers to the questions are key.\nIt is also an important signal if you can not motivate the *why* of something,\nespecially if it is key to your \"survival\".\nTake this next section as a great example of what you should be able to superbly motivate.\n\n## 2. Why should people come to us?\n\nPeople and resources cost. What are you providing in return that motivates those costs?\nIt is okay if the returns are high risk / high reward, abstract or even idealistic but they require more trust than, you know, iron ore.\nThis is why almost all leaders have to do some amount of business-thinking.\nYou are after all the person responsible for finding work for your team\nand you are also responsible for making sure that the outcome is satisfactory for all.\n\nYou often have to orient yourself, evaluate the streams of work that you have and continually consider potentially new streams.\nOf course, successful businesses (one of the lessons from Jim C. Collins *Good to Great*) employ what is called Hedgehog concept,\nillustrated by this diagram:\n\n![Hedgehoc concept](https://i0.wp.com/www.monkhouseandcompany.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-29-at-14.09.59.png?resize=578%2C463&ssl=1)\n[Source](https://www.monkhouseandcompany.com/resources/insight/7-important-lessons-from-jim-collins-good-to-great/)\n\nThis is what you fall back on rainy days (or years).\nYou have already set yourself up for trouble if you think *execution* follows \"don't put all your eggs in one basket\".\nYour core business can itself be diverse, like marketing or research, but you are unlikely to require multiple, potentially competing schemes.\n\n## 3. Execution matters and feedback on execution matters\n\nAs a leader, whether it be through management or seniority, you have to define what is a job well done.\nThis matters for feedback, your team should know what they are doing well (and should be celebrated for, and do more of) \nand where they could improve (which raises the question if they need training e.t.c.).\nAnd to be crass, a job well done has less to do with the technical details and more to do with the alignment of the core business.\n\nIn John Doerr's \"Measure What Matters\", the book on [Objectives & Key Results (OKRs)](https://www.whatmatters.com/get-examples#engineering),\nhe describes that the effect of properly implemented OKRs is that everyone is aware of how their results are contributing to the objectives.\nIn such an environment you are getting feedback regularly and the need for a \"check-in\" at the yearly performance review becomes superflous.\n\nOf course it can be hard to have a job whose function can be captured in 3-5 quarterly key results, but trying is important.\nPeople also need their hedgehog concepts. \nIf someone is doing too many tasks or saddling too many roles then it is probably a sign of uncertainty or disorganisation.\n\n## 4. Reasonable restraint\n\nMicromanagement can be a difficult itch but restraint is almost always the solution.\nYou are taking time away from yourself and the person you are overriding is probably not learning that much either.\nThe book [\"the making of a manager\" by Julie Zhuo](https://www.juliezhuo.com/book/manager.html) expanded my perspective on this by showing me more situations to be restrained.\n\nOne paraphrased example would be \"you are probably assisting those falling behind way more than you are boosting those with good pace\".\nAnother one is \"focus on your time where you can be a force multiplier instead of an adding force (you going in an doing the job yourself)\".\nRestraint and reflection is the answer. Use your manager or your mentors for discussing strategies.\nAnother thing is that you can also ask your team for feedback, but expect them to package it differently then if you were a fellow colleague.\n\n## 5. Your team(s)\n\nBeing a team leader and in particular a manager means *getting better outcomes from a group of people working together* (Julie Zhuo).\nEverything we have covered on being oriented in your environment, having clear goals, giving feedback, showing restraint and building trust are components to make a team function well.\n\nJulie also compiles a few summaries of what managers should focus on during a day:\n* Her own days can be categorized with: Purpose, people & process. Purpose is the outcome your team is trying to accomplish - the why. Why do you wake up and choose to do *this* instead of the thousands of other things you could be doing? Why pour your time and energy into this particular goal with this particular group of people? Then there is people - the who. Are the team members of your team set up to succeed? Do they have the right skills? Are they motivated? Process - the how. The word conjures nightmares but the point of having & knowing processes is so *simple tasks do not get enormously complicated*.\n* Her summary of Chris Cox: \"Half of what he looked at was my team's results - did we achieve our aspirations? The other half was based on the strength and satisfaction of my team - did I do a good job hiring and developing individuals, and was my team happy and working well together?\"\n* Her summary of J. Richard Hackmans five conditions: Having a real team (one with clear boundaries and stable membership), a complelling direction, an enabling structure, a supportive organizational context and expert coaching.\n\n### Team composition, role balance and skill distribution\n\nSometimes you get to choose your team, sometimes you inherit it.\nIt goes without saying but balancing the work, both internally and in interface with other teams, is a complex environment.\nIf you think back to the Hedgehog concept, we need to match **passion** of the team with what they as a group can **excel** at to meet the external **demand**.\n\n* Passion - Changing the passion of team members is nigh impossible.\nThat is trying to make someone love you.\nOnly drastic gambles, like letting someone change their career path could potentially reignite fading passion.\nIf the passion is not there then people are unlikely to stick around for a long time.\nThe best you can do is hire passionate people and shift the culture toward that desired norm.\n\n* Excellence - What the group can excel at is more flexible.\nYou can shift tasks and responsiblities around.\nYou can rework processes meaning that tasks take a new form.\nYou can outsource some work aspects to other teams or bring in aspects previously handled by other groups.\nYou can give people the time to develop new skills or learn a new tool.\n\n* Demand - The external demand is also subject to some flexibility.\nGiven that changing a groups composition is a slow process without extreme measures it might be possible to shift the sources of work.\nOf course, this only work if what governs you allow it.\nA start-up is free to pivot as it wants but a function within a large organization probably have some hardcoded responsiblities.\n\nYour team relies on you for shifting the culture in a positive direction, assisting their individual development\nand bringing in a steady source of tasks that suit their profile. \nThe external environment relies on you to effectively lead and/or manage your team to make it worth the cost.\n\nWhen I was reading Tony Fadell's [\"Build\"](https://www.buildc.com/the-book) I was captured by chapter 2.2 titled \"Facts vs Opinions\".\nHe argues that the recent leader obsession with being \"fact-oriented\" or \"data-driven\" has some downsides.\nOne of the downside is analysis-paralysis, being stuck in prestudy-prugatory.\nAnother downside is that it downplays the importance of showing character and good values.\nThis is important enough to require a section of its own.\n\n## 6. Showing character and good values\n\nShowing character is an aspect of leadership that is double-edged.\nIf it turns out that you have good values then people will remember you for it.\nIf not then they will probably still remember you but for the wrong reasons.\n\nWhat are good values?\\\n*It means challenging the status quo when you are clearly stuck in a local minima.*\\\n*It means that you take ownership, not only on sunny days but also on rainy days.*\\\n*It means that your team can rely on you to make high-level decisions and care about their operalization.*\\\n*It means that you adress whispers before they become rumors.*\\\n*It means speaking up when you see something unfair.*\\\n*It means handing over responsilibity when you are not suited for it.*\n\n### The tensions of leadership\n\nThe hardest part of leading is that you can not do it perfectly.\n* You will have to make decisions under uncertainty.\n* You will have to settle disputes, even when it might rustle some feathers.\n* You will have to shoot down and ignore great proposals.\n* You will have power over people, which changes how they view you.\n* Some discussions will be a bit awkward.\n* In some situations you can not be completely transparent.\n* You will be judged for action and inaction alike.\n\nThat is why we emphasise trust.\nGiven that you have the skills to actually maneuver the complex business environment, having people trust you is key.\nIf you earn people's trust then they will forgive your shortcomings, because they know that you have got their back.\nThis is a property that management share with politicians.\nIt is only fair because with the role comes power, which must be wielded resposibly.\n\nThis post is not meant to rant about pathologies of leadership and power, \nbut one humorous, dark take on it is [David Groeber's \"Bullshit jobs\"](https://en.wikipedia.org/wiki/Bullshit_Jobs#:~:text=More%20than%20half,%5B3%5D).\nHe captures what happens in late-stage capitalist societies where looking busy and sounding important trumphs productivity.\n\n# My values\n\n* Physics\n    - Approach problems from first principles.\n    - Even simple systems can be studied almost indefinitely.\n    - \"Somebody probably though of this over a 100 years ago\".\n* Engineering\n    - Product longevity comes in at the design phase. A well-engineered thing last longer and keeps its value.\n    - \"Any idiot can build a bridge but only an engineer can build a bridge that barely holds.\"\n    - I am suspicious if something (read: my code) works on the first try.\n* Data science \n    - \"Somebody probably thought of this in the 60s\".\n    - Technical superiority is no replacement for data and domain knowledge.\n    - Creativity is usually the difference between mediocre and stellar solutions.\n* Customer-facing\n    - Have a customizable but standard plan for how you roll out a solution.\n    - Pick up on what actually matters for the customer and find the root problem to adress.\n    - Have a realistic dialogue about the future of things, no zombie products or projects!\n* In general\n    - I keep my ears open to quickly form myself an understanding of what is going on.\n    - I like to make people happy and engaged. I would like for those meaning well to be successful.\n    - I like to explain ideas. I also love to brainstorm and spar ideas. It is a team sport.\n    - I would like to believe that I stay honest and humble in most situations.\n\n# Playbook\n\n**My view on data science** - It is the intersection of data, its interpretation, its transformation, statistics,\noptimization, machine learning, signal processing. Programming is a tool but code is not the main goal, \ninsights and decision support are.\nData is typically tabular but graphs, images, video, audio, GIS are workable.\n\nGiven my background I would likely lead in a data science context but that itself is not given.\nOther domains of interest like finance, supply chain, logistics, sales and product all benefit from someone\nwith data science thinking and the potential for leading department process transformation.\n\n## 1. Having and tracking concrete targets, goals and progress\n\nRecognition is important.\nPeople appreciate you being aware of what they are working on.\nThis can be tied together with what you present outward as group performance.\nThere should be some targets that are key to the groups continued existence.\nBasically metrics of your Hedgehog concept.\nThis can be number of completed projects or number of sales.\nIf you are a function in a supply chain, say logistics of finished product to market, \nthen the progress (perhaps measured in %) of finished product left to the market is one of many metrics.\n\nBasically your budgeted numbers.\n\nHowever, this is mainly for you and it can even demotivating for your team if the numbers are looking gloom.\nInstead it is also important to set goals, objectives where you as a team can excel and grow.\nGoals should be set on a group level but also on a personal level.\nTheir concreteness can vary.\nIf someone is supposed to be a researcher but is always involved in other projects then stating \"at least 60% research time\" is a goal.\nIf you commit to such a goal then defending against those external pulls will require support and backing.\n\nSpeaking of being a researcher, goals like \"publish 2 findings this year\" is not a motivating goal.\nResearch, design or even engineering rarely benefit from commands like \"innovate at least 2 things this year\".\nInstead they require people to follow their passions until one day, something magical happens... or at least, \nuntil a point of maturity where other, more structured processes can join in or take over.\nInstead of goals I would like to call this progress.\n\nKeeping a list of progress is better than setting goals. \nThis list should be continually updated and shared within the teams.\nProgress should not only be strictly positive. \nIf someone is stuck in a dead-end project then helping them get out of it and writing a lessons learned note, is progress.\nIf someone has not progressed in a while, then it is a signal to check-in and see if there are some interesting *negative* results that can be shared.\nIt is important to also say \"we tried this and it did not work\" or \"this was a waste of time\".\nIt is healthy to review these events you can figure out how to do less of it in the future.\n\nNow, there are a lot of things here. Targets. Goals. Progress. So here is a shorthand:\n\n* Targets are my problem. This is what we sell outwards (goals are bonuses). Progress can be fed into targets.\n* Goals can have varying length, but 1 quarter to 1 year are good spans. They should capture team ambitions.\n* Progress is simply taking the pulse of the team and maturity of their work.\n\n\n## 2. Data strategy\n\nNo matter which side of the table I am on, I can not help myself to create a data strategy if one does not exist.\n\nWe start by asking questions like:\n* What data to we need to operate? \n* Where does it exist?\n* Is that the ground truth data?\n* Who owns the data?\n* How can we access it?\n* How often do we need it?\n* Who else needs it?\n* Which processes are in place today?\n* Can we prioritize the data? Which ones are must-haves and which ones are nice-to-have?\n\nThen one needs can start sketching when can we have data X, Y, Z and at what level.\nIf the data maturity of the organization is high then the rest is trivial.\nAn information architect and a pair of data engineers can easily transfer data to where it need to be, in the right shape.\nUnfortunately it is rarely that easy.\n\nCreating a data strategy often requires pragmatism. If there is no ground truth data then we need to define it.\nIf there is no owner, then one must be assigned. If it can not be accessed then we need to review access policys.\nUnfortunately one often find this out **after** an AI-level initative has been budgeted.\n*That is not how we do things around here*.\nYou as the person spearheading something like that should do your due diligence.\n\nThe hierarchy of data needs is roughly:\n* Can you even scrape the data together into one pile? How long does that take?\n* Can you gather the data into one place or at least have one hub?\n* Is the data reliable enough to report as official numbers?\n* Have you built business reporting on top of these numbers?\n\nIf data exists and it is good enough for official reporting, then we have a chance.\nIf not then one must start with these two steps. \nOf course, one typically want **all** the data for all possible use cases.\nThe best one can do is to pick a use case or a family of use cases, enable those with data, business intelligence and then do the *fun* stuff.\nAlthough, in my experience, going from no source of truth to one source of truth itself often has great effects.\n\n## 3. \"We have too much bread and too little butter\"\n\nThe quote is of a colleague of mine describing our department's approach to initatives.\n\nAvoiding to spread yourself thin is easier said than done.\nIt requires both discipline and insights into how your team operates.\nWhat we would like to avoid is a situation where multiple sources of work are attached to every single person.\n\nFrom experience an individual can at max juggle 7-9 (2-3 medium/big, 5-6 small) things in their head\nbut that is the limit where congestion hits. \nThey will be so bogged down so that the pace of the work that gets done matches the rate of incoming work\nor the growth / prolonging of existing work (interest on your work debt).\nThis is not a state to be in for longer periods of time.\nIt leaves little-to-no time for reflection, documentation, robustification, skill development and other non-urgent things.\n\nBelow is a graph showcasing this effect on a toy example.\nActive tasks accrue interest.\nThis can be interpreted as scope drift, decreasing motivation over time or the fact that formalities (read: meetings, workshops) have a fixed time budget whileas time spent on progress is flexible (and there is a problem if the formalities take up more time than you have to spend on progress).\nIn the parallel burndown example we start the three tasks at the same time.\nThe pace is $\\frac{1}{3}$ for each task.\nThe initially estimated effort is $8 + 5 + 3 = 16$ weeks but it ends up taking 25 weeks!\nIn the sequential example the tasks are knocked out one at a time, only taking 18 weeks in total.\n\n![...](/images/leadership/leadership_1.png)\n\nThis is not a model for real time management as in reality there is probably a goldilock\nzone for each individual.\nBut the point still stands.\nFocus beats context switching and formality overhead for getting things done **fast**.\nOf course, it is our responsiblity as leaders to sacrifie some sacred focus time to support others.\nAfter all, helping the team time manage can unlock $\\frac{25}{18} \\approx 1.39 \\rightarrow 39\\%$ efficiency! \n(source: the graph above ... joking!)\n\nWhat are some tools for handling this?\n* Prevent it from becoming a problem in the first place.\n    * One must dare to say \"no\" or \"later\".\n    * One must clear out old things stealing time. Do you have any old sourdoughs?\n    * **Changing what work and deliverables the team takes on.**\n* An individual's tasks can be load-balanced onto colleagues but this is a risky maneuver.\nIn another colleague start to need offloading then it quickly bogs down the group until they are back up and running.\nInstead it is wiser to either pause their work or pause the work of the colleague taking over.\n* Task size and interest can be guesstimated. \n    * In an agile context people are usually comfortable with T-shirt sizes (S, M, L, XL) for task size.\n    * Interest is harder. It probably correlates with things that makes an S-task feel like a L-task:\n        * Are there a lot of formalities?\n        * Is there percieved hostility or apathy in the working group?\n        * Have the ground work been laid? (like, do you have a data strategy?)\n        * Is there a clear path on how to achieve the goal?\n\nIn the end this will always be a moving target because external demand and the people in your group fluctuates.\n**Changing what work and deliverables the team takes on** is an issue that has to be adressed on a deeper level.\nIt probably means changing definitions of your targets and team progress.\nIt will also be a negotation with your customers and whoever is deciding your budget.\nTherefore it is one of those things that require character, strong convictions, an actionable alternative plan and the trust of the team.\n\n## 4. Sourdough management\n\nSourdoughs, new and old, is something you are changed with clearing up.\nIn my mind, letting them grow is *negative* progress.\nWork has to be undone before it can progress smoothly again.\nThe most common sources of sourdoughs are:\n* Half-finished projects where almost all resources have been moved on to other tasks.\n* Data or software decay due to not having been maintained or even used for a long time.\n* Prestudy-purgatory, where people have gotten fed up with the idea before it even reached any level of maturity.\nYet its name keeps showing up on the list of things that are \"in the working\".\n* Technical debt accrued by taking shortcuts when building something. Typically POCs in production.\n* Keeping to an old way of doing things or a tool despite the cost outweighing the benefits.\n* A project that has developed organically (without a plan) for way too long.\n* Something that does not match the roles in your team, but they do anyway.\n\nBeing aware of sourdoughs and keeping track of them is key.\nIt is uncomfortable for some not to think \"done is done, what is next?\"\nand face the fact that what used to be a ✅ is now a ❌.\nTaking care of them starts with one question \"if we remove this, will it be missed or will everyone involved be *relieved*?\"\n\nIf it will be missed, then one needs to figure out the plan for adressing it.\nWhile it is tempting to stop all work until the sourdoughs are fixed, \none often have to solve them simulateneosly as handling new work.\nClearly communicating both inward and outward is important to manage expectations.\nThe most uncomfortable aspects surrounding sourdough management is the sunk cost fallacy (for the people who invested in it)\nand the destruction of capital that working on it has implied.\nIt is therefore important to educate stakeholders on how to move forward in the future.\n\n# Bite-sized takeaways\n* As a starting point, Keep It Simple Stupid (KISS) should be applied to product- and project management.\n* Formalities should serve the product or project and not the other way around.\n* I recommend [\"The Culture Map\" by Erin Meyer](https://erinmeyer.com/books/the-culture-map/) for insights into typical culture clashes.\n* Estimated (financial) impact rarely captures whether you have have passionate people and an opportunity.\n* Top-down rarely works in a data context. Always start bottom-up.","src/content/blog/my-leadership-playbook.mdx","7c598b32380da36d","my-leadership-playbook.mdx","causal-modeling",{"id":72,"data":74,"body":80,"filePath":81,"digest":82,"legacyId":83,"deferredRender":24},{"title":75,"description":76,"pubDate":77,"tags":78},"Learning about causal modeling","Notes on my random walk of causal modeling and the PyWhy ecosystem",["Date","2025-08-29T00:00:00.000Z"],[79,72],"learning","The only thing I knew about causal modeling before I wrote this post is the double ML check. \nIt is the method for checking observed confounders.\nI have had to use it in my work because a manager approached us with the question \"Does net promoter score (eNPS) correlate with accident statistics?\"\n\nFor those of you who do not know, net promoter score is a question \"Would you recommend this company to an acquaintance?\" on a scale of 1-10.\nIf you pick 6 or below you are a detractor, if you pick 9 or 10 then you are a promoter, 7 and 8s do not count.\nThe score is calculated as $eNPS = \\%promoters - \\%detractors$.\n\neNPS is a touchy subject that managers are usually encouraged to increase, often without clear methods or expected results.\nTherefore, the question itself becomes quite convoluted when you think about what this manager is asking for,\n\"Is this looking to strengthen the eNPS-higher-equals-better narrative?\" or is it simply that understanding accident statistics is important,\nbut then why focus on such a metric?\n\nRegardless, because of the nature of the question we went beyond just quantifying the correlation. \nIt exists, it is like 0.1 or 0.2.\nBut the question is of course if that is the cause or if there are confounders.\nAnd actually... logically the opposite would be a more logical formulation: \"more accident-prone workplace $\\rightarrow$ less likely to recommend\".\n\nThe simplest confounding model is:\n![...](/images/causal/causal_1.png)\n\nWhere we can label $C$ as our effect (increased frequency in workplace accidents). \n$B$ is the eNPS and $A$ are potential confounders.\nAs someone who had already studied eNPS from before I knew that you get pretty good $R^2$ of eNPS\nusing the employee survey average (also called engagement) and more specifically to questions like\n\"I know, understand and relate to our company-level strategy\". \nThat question alone can explain like 90% of the variance and is also *highly correlated* with how many managers are between you and the CEO.\n\nIn a nutshell, to estimate the isolated effect of $B \\rightarrow C$ we first need to account for $A \\rightarrow B$.\nWe do this by training a model $f(A) \\rightarrow B'$ and then we calculate the residual $r_B = B - B'$. \nIf we then fit another model $g(A) \\rightarrow C'$ and then we calculate the residual $r_C = C - C'$.\nNow we have removed all we know from $A$ from both $B$ and $C$. \nThe final step is then to see if $B$ have any effect on $C$ by training a final model $h(r_B) \\rightarrow r_C$.\nIf the model $h$ has no explainatory power then it points to $A$ carrying all the information.\n\nOne can do this and say that $B \\not\\rightarrow C$.\n\n# A random walk along the PyWhy libraries\n\nResources: https://www.pywhy.org/\n\n## EconML - Basics\n\nResources: https://www.pywhy.org/EconML/spec/causal_intro.html \n\nEconML is the PyWhy library for *automated learning and intelligence for causation and economics.*\n\nThis resource was a great start for me. \nI like the contrast between forecasting (like, predicting how many sales a video game will have next month)\nand causal modeling (like, understanding how many people will buy the game if we show an ad).\nMuch like Bayesian modeling; causal modeling requires you to write down and encode modeling assumptions.\nThe basic terminology is outcome (Y), treatment (T) and confounders (W). Confounders can be observed or unobserved.\n\nThe main thing that we are after is the conditional average treatment effect or CATE.\n\n### Method 1 - Randomized experiments\nThe gold standard for doing causal modeling is randomized experiments.\nIn the video game example they explain that gamers are more likely to get targeted by ads and more likely to buy games in the first place.\nBy randomly assigning ads, or in this experiment withholding ads, from gamers one can study the outcome. \nNote that this might not always be practical or possible to achieve hence why the field of research into causal models exists.\n\n### Method 2 - Measure confounders\n\nThis links to their page for *estimation methods under unconfoundness*: https://www.pywhy.org/EconML/spec/estimation.html\n\nWhen perfectly randomized experiments can not be performed then one can use this suite to estimate hetrogeneous treatment effects.\nThe theoretical guarantees only hold when all confounders are observed.\nThe suite contains four chapters:\n* **Orthogonal / Double machine learning (DML)**\n\u003Cbr>This is the methodology we described in the beginning of this post.\nFor this package the classes are `DML, LinearDML, SparseLinearDML, KernelDML, NonParamDML, CausalForestDML`.\nThe difference between these methods are how they create the residual models $h$. \nDML is generic while Linear, Sparse, Kernel use specific implementations.\nNonParamDML is technically a meta learner (later section) and CausalForestDML is a forest based estimator (also later section).\nSome example code:\u003C/br>\n```python\nfrom econml.dml import DML, SparseLinearDML, NonParamDML\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nest = DML(model_y=GradientBoostingRegressor(),\n          model_t=GradientBoostingRegressor(),\n          model_final=LassoCV(fit_intercept=False))\n\nest = SparseLinearDML()\nest.fit(y, T, X=X, W=W)\npoint = est.effect(X, T0=T0, T1=T1)\nlb, ub = est.effect_interval(X, T0=T0, T1=T1, alpha=0.05)\n\nest = NonParamDML(model_y=GradientBoostingRegressor(),\n                  model_t=GradientBoostingRegressor(),\n                  model_final=GradientBoostingRegressor())\nest.fit(y, t, X=X, W=W)\n```\n* **Doubly robust learning**\n\u003Cbr>What if we are interested in creating a fully contextualized model instead of just understanding treatments?\nThat is where doubly robust learning comes in. \nSimilarly to DML it estimates hetreogenous treatment effects but then also predicts the outcome using data $X$.\nFor this package the classes are `DRLearner, LinearDRLearner, SparseLinearDRLearner, ForestDRLearner`.\u003C/br>\n```python\nfrom econml.dr import DRLearner, LinearDRLearner\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n\nest = DRLearner(model_regression=GradientBoostingRegressor(),\n                model_propensity=GradientBoostingClassifier(),\n                model_final=GradientBoostingRegressor())\nest.fit(y, T, X=X, W=W)\npoint = est.effect(X, T0=T0, T1=T1)\n\nest = LinearDRLearner()\nest.fit(y, T, X=X, W=W)\nest.effect(X, T0=t0, T1=t1)\n```\n* **Forest based estimators**\n\u003Cbr>When reading this section I am struck by the fact that it is simply DML or DR learners but these tree based models have specific desireable properties.\nThe estimators are `DMLOrthoForest`, `DROrthoForest`, `CausalForestDML`, `ForestDRLearner`. \nGenerally speaking this class of estimators are good when you have a lot of features, no good idea of how the effect heterogeneity looks like and they support confidence intervals. \u003C/br>\n```python\nfrom econml.orf import DMLOrthoForest, DROrthoForest\nfrom econml.sklearn_extensions.linear_model import LinearRegression, WeightedLasso\n\nest = DMLOrthoForest(n_trees=1, max_depth=1, subsample_ratio=1,\n                     model_T=LinearRegression(),\n                     model_Y=LinearRegression())\nest.fit(Y, T, X=W, W=W)\n\nest = DROrthoForest(n_trees=1, max_depth=1, subsample_ratio=1,\n                    propensity_model=sklearn.linear_model.LogisticRegression(),\n                    model_Y=LinearRegression())\nest.fit(Y, T, X=W, W=W)\n\nest = DMLOrthoForest(n_trees=100,\n                     max_depth=5,\n                     model_Y=WeightedLasso(alpha=0.01),\n                     model_T=WeightedLasso(alpha=0.01))\nest.fit(Y, T, X=X, W=W)\n```\n* **Meta-learners**\n\u003Cbr>This class of estimators generally do not give confidence intervals but offer full flexibility in model selection.\nOther than `NonParamDML` and `DRLearner` from before; this section convers T-, S- and X-learners as well as Domain Adaption Learner.\nFor a binary treatment $T$ (0 or 1) the T-learner trains one model $M_i(Y_i \\sim X_i)$ per treatment and estimates the CATE as $M_1(X_1) - M_0(X_0)$.\nThe S-learner trains one model with the treatment $T$ as one-hot encoded, the CATE estimate is therefore $M(X,T=1) - M(X,T=0)$.\nThe X-learner trains two models like the T-learner, \ncalculates the residuals $D_1 = Y_1 - M_1(X_1)$ and $D_0 = M_0(X_0) - Y_0$, \nfits the residuals with another pais of models $W_i(D_i \\sim X_i)$ and finally calculates the CATE as\n$\\tau = g(x)W_0(x) + (1-g(x))W_1(x)$, where $g(x)$ is an estimation of $p(T = 1\\|X)$.\nThe Domain Adaption Learner is a variation of the X-learner that weights \n$M_0(Y_0 \\sim X_0, w = \\frac{g(X_0)}{1-g(X_0)})$ and $M_1(Y_1 \\sim X_1, w = \\frac{1-g(X_1)}{g(X_1)})$,\nthen it similarly calculates residuals but fits a combined model $D$.\u003C/br>\n\n### Method 3 - Instrumental variables\n\nAn instrument is a variable that has an effect on the treatment but not directly with the outcome.\nThe IV methods allows you estimate CATE without all confounders if you have a valid instrument.\nHowever, if the effect on the treament is weak then the methods tend to be biased than measured confounders.\nThe best instruments are typically randomized or arbitrary assignments. \nThis is why IV methods typically are used for A/B testing or when a person from a pool as been assign to decide treatment.\n\nThe IV estimators in EconML are `OrthoIV`, `DMLIV`, `NonParamDMLIV`, `LinearDRIV`, \n`SparseLinearDRIV`, `ForestDRIV`, `IntentToTreatDRIV`, `LinearIntentToTreatDRIV`\nwhere the DML, DR are the methods from before. \nIntent to treat simply means that there is a randomized assignment of treatment groups which we use to analyse our results against.\n\n```python\nfrom econml.iv.dr import LinearIntentToTreatDRIV\n\nest = LinearIntentToTreatDRIV()\nest.fit(y, T, Z=Z, X=X, W=X)\nest.effect(X)\n```\n\n### Bonus method - Dynamic double machine learning\n\nEconML also has a section on dynamic estimation methods.\nThese are cases where multiple treatments have been offered over time and some final outcome is of interest.\n[This article from 2021](https://arxiv.org/pdf/2002.07285) described the methodology.\n\n```python\nfrom econml.panel.dml import DynamicDML\n\nest = DynamicDML()\nest.fit(y_dyn, T_dyn, X=X_dyn, W=W_dyn, groups=groups)\nest.effect(X)\n```\n\n### Side note on EconML - Federated learning\n\nFederated learning is definitely a topic for another post but it seems that\nCATE estimation is something that can be nicely distributed across multiple models.\nThis is of course interesting in case one is prohibited from aggregating data from multiple sources.\n\n## DoWhy\n\nDoWhy is a tool that builds on tools like EconML that is made for tasks like:\n* Estimating causal effects\n* Quantify causal influence\n* Root-cause analysis and explaination\n* Asking and answering what-if questions\n* Predicting outcome for out-of-distribution samples\n\nDoWhy has four steps to causal inference:\n1. Modeling\n2. Identification\n3. Estimation\n4. Refuting\n\nTo run DoWhy one requires a causal graph of their problem (part of modeling).\nThese are best constructed using domain knowledge but can be assisted by data-driven methods like those from `causal-learn`.\nDoWhy expects the causal graphs to be encoded in a `networkx` graph.\n\n### Graphical causal models\n\nA GCM is a combination of a causal direct acyclic graph (DAG) of variables and a causal mechanism for each of the variables.\nThe mechanisms can be purely stochastic, conditional stochastic or functional.\nA GCM with stochastic parents and conditional stochastic children is called a probabalistic causal model (PCM).\nA GCM with stochastic parents and functional children is called a structural causal model (SCM).\nAn invertible SCM (ISCM) has invertible function, the most common being additive noise models $X_i = f(PA_{X_i}, N_i) = f'(PA_{X_i}) + N_i$,\nwhere $X_i$ is the variable at node $i$, $PA_{X_i}$ are its parents, $N_i$ is noise.\n\nWith a GCM one can hope to adress Judea Pearl's three levels of causality:\n1. Association - *What do we observe when X = x?* - PCM, SCM\n2. Intervention - *What will happen if we change X?* - PCM, SCM\n3. Counterfactuals - *What would we have observed if X had taken a different value?* - ISCM\n\nHere is how to set up a SCM for three variables $X, Y, Z$ in a chain.\n\n```python\nfrom dowhy import gcm\nimport networkx as nx\ncausal_model = gcm.StructuralCausalModel(nx.DiGraph([(\"X\", \"Y\"), (\"Y\", \"Z\")]))\ngcm.auto.assign_causal_mechanisms(causal_model, data)\n# alternatively (manually)\n# causal_model.set_causal_mechanism('X', gcm.EmpiricalDistribution())\n# causal_model.set_causal_mechanism('Y', gcm.AdditiveNoiseModel(gcm.ml.create_linear_regressor()))\n# causal_model.set_causal_mechanism('Z', gcm.AdditiveNoiseModel(gcm.ml.create_linear_regressor()))\ngcm.fit(causal_model, data)\n```\n\nWhen we have a GCM we can draw samples from it like:\n```python\ngenerated_data = gcm.draw_samples(causal_model, num_samples=1000)\n```\n\nAnd to evaluate it:\n```python\nsummary_evaluation = gcm.evaluate_causal_model(\n    causal_model, \n    data, \n    compare_mechanism_baselines = True\n)\nprint(summary_evaluation)\n```\n\n#### Confidence intervals\nDue to the random nature of the data and model fitting it is recommended by the authors of PyWhy to bootstrap confidence intervals.\nThis can be done to multiple rounds of training or just to the inference (when training is expensive).\n\n```python\nZ = np.random.normal(loc=0, scale=1, size=1000)\nX = 2*Z + np.random.normal(loc=0, scale=1, size=1000)\nY = 3*X + 4*Z + np.random.normal(loc=0, scale=1, size=1000)\ndata = pd.DataFrame(dict(X=X, Y=Y, Z=Z))\n\ncausal_model = gcm.StructuralCausalModel(\n    nx.DiGraph([('Z', 'Y'), ('Z', 'X'), ('X', 'Y')])\n)\ngcm.auto.assign_causal_mechanisms(causal_model, data)\n# fit model multiple times to get CI\nstrength_median, strength_intervals = gcm.confidence_intervals(\n    gcm.fit_and_compute(\n        gcm.arrow_strength,\n        causal_model,\n        bootstrap_training_data=data,\n        target_node='Y'\n    )\n)\n\n# alternatively: fit once and only calculate CI at inference\ngcm.fit(causal_model, data)\nstrength_median, strength_intervals = gcm.confidence_intervals(\n    gcm.bootstrap_sampling(\n        gcm.arrow_strength,\n        causal_model,\n        target_node='Y'\n    )\n)\n```\n\n### Identification\n\nThe main class of PyWhy called `CausalModel` expects data, treatment and outcome with an optional graph argument.\nIf one supplies the graph then one can use the `.identify_effect()` method to get a report of methods for estimating the outcome.\nThe methods include backdoor, frontdoor and instrumental variables.\nEssentially a methodology selection.\n\n### Refute\n\n#### Refuting a causal graph\n\nOne concept of DoWhy is graph refutation, \nmeaning that we can test the assumptions that we have encoded through our domain knowledge or found using data-driven methods.\nThis is done by testing all conditional independence constraints, also known as local Markov conditions (LMCs).\n\n```python\nfrom dowhy.gcm.falsify import falsify_graph\n# causal_graph is a networkx digraph\nresult = falsify_graph(causal_graph, data, show_progress_bar=False)\nprint(result)\n```\n\nThe core idea behind LMCs are that there are three types of structures:\n* Colliders: $A \\rightarrow B$, $C \\rightarrow B$.\n* Forks: $B \\rightarrow A$, $B \\rightarrow C$.\n* Chains: $A \\rightarrow B$, $B \\rightarrow C$.\n\nBy definition, $A$ and $C$ are statistically independent in a collider.\nIn a fork they are dependent and in a chain $A$ and $C$ are *conditionally* independent on $B$.\nBy testing these parts of the graph using observed data one can potentially refute invalid assumptions.\n\n#### Refuting effect estimates\n\nAn effect refutation can be done either through negative control or sensitivity analysis.\nNegative control means that we consciously change something about the problem. \nIt is a family of sanity checks with two types:\n* Invariant transformations - These transformations do not change the premise of the problem. Methods include subsampling the data and adding an independent random common cause.\n* Nullifying transformations - Placebo treatment where one replaces the treatment with an independent random variable and dummy outcomes where one replaces the outcome should both nullify the effect.\n\nSensivity analysis adds an artificial confounder to the problem and studies the impact on the effect.\n\n```python\nres_placebo = model.refute_estimate(\n    identified_estimand, \n    estimate,\n    method_name=\"placebo_treatment_refuter\", \n    show_progress_bar=True, \n    placebo_type=\"permute\"\n)\nprint(res_placebo)\n\nres_random = model.refute_estimate(\n    identified_estimand, \n    estimate, \n    method_name=\"random_common_cause\", \n    show_progress_bar=True\n)\nprint(res_random)\n```\n\n# Examples of applying causal modeling\n\n## Exploring causes of hotel booking cancelations\n\n[Link](https://www.pywhy.org/dowhy/v0.13/example_notebooks/DoWhy-The%20Causal%20Story%20Behind%20Hotel%20Booking%20Cancellations.html#)\n\n[Data source](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md)\n\nThis example has you study the effect of assigning a different hotel room on cancelations.\nFollowing the example one gets an effect of -0.26 meaning a negative effect on cancelations.\nThe causal graph they set up looks like this, which is quite a bowl of spaghetti:\n\n![...](/images/causal/causal_2.png)\n\nTo decompose what they did, they say that *days in waiting list*, *hotel type*, *number of booking changes*,\n*is a repeated guest* and unknown confounders (U) influence whether or not a person gets assigned a different room.\nTo then determine cancelation they say that *lead time* (number of days from booking to arrival), *days in waiting list*,\n*hotel type*, *total number of special requests*, *number of booking changes*, *different room assignment*, \n*is a repeated guest*, *previous bookings canceled*, *total stay in days*, *number of guests* \nand *required car spaces* impacts it.\n\nThey reason that the negative effect is probably due to the unobserved confounder *\"showed up to the hotel\"* \nbecause a different room assignment is likely to happen on the day of your check-in.\nAnd that was about it for this example, but lets dig a little deeper.\n\nFirst of all, what are we dealing with here?\n\nHere is a table of the *reservation status*. Note that both canceled and no-shows are counted as cancelations.\n\n| reservation_status | size  | mean of is_canceled |\n|--------------------|-------|------|\n| Canceled           | 42954 | 1.0  |\n| Check-Out          | 74745 | 0.0  |\n| No-Show            | 1203  | 1.0  |\n\nAnd the definition of *different room assignment* is *reserved room type* $\\neq$ *assigned room type*.\nHere are the statistics for how often a room type was reserved / assigned and their corresponding mean of cancelations.\n\n| **Room type** | size of reserved | mean of reserved | size of assigned | mean of assigned |\n|---------------|------------------|------------------|------------------|------------------|\n| **A**         | 85601            | 0.392273         | 73863            | 0.445487         |\n| **B**         | 1118             | 0.329159         | 2163             | 0.236708         |\n| **C**         | 931              | 0.330827         | 2354             | 0.188615         |\n| **D**         | 19173            | 0.318208         | 25166            | 0.252603         |\n| **E**         | 6497             | 0.293828         | 7738             | 0.253554         |\n| **F**         | 2890             | 0.304498         | 3732             | 0.248124         |\n| **G**         | 2083             | 0.366299         | 2539             | 0.307208         |\n| **H**         | 601              | 0.407654         | 708              | 0.354520         |\n| **L**         | 6                | 0.333333         | 1                | 1.000000         |\n| **P**         | 2                | 1.000000         | 2                | 1.000000         |\n| **I**         |                  |                  | 357              | 0.014006         |\n| **K**         |                  |                  | 279              | 0.043011         |\n\nNote that only room type A is where we have fewer assignments than resvations and that it is the only type\nwhere the rate of cancelation goes up (we except L and P as they are so few observations).\nAnother observation is I and K, which are not available during reservation but can be assigned\nand they have a very low rate of cancelation.\n\n| reserved_room_type | assigned_room_type | size | mean of is_canceled |\n|--------------------|--------------------|------|---------------------|\n| A                  | I                  | 211  | 0.009479            |\n| C                  | I                  | 9    | 0.111111            |\n| D                  | I                  | 67   | 0.014925            |\n| E                  | I                  | 39   | 0.000000            |\n| F                  | I                  | 10   | 0.000000            |\n| G                  | I                  | 15   | 0.066667            |\n| H                  | I                  | 6    | 0.000000            |\n| A                  | K                  | 210  | 0.033333            |\n| B                  | K                  | 2    | 0.000000            |\n| D                  | K                  | 44   | 0.113636            |\n| E                  | K                  | 16   | 0.000000            |\n| F                  | K                  | 3    | 0.000000            |\n| G                  | K                  | 4    | 0.000000            |\n\nIt also seems that they have a roughly equal assignment to the room reservation frequency (A then D, E and B, C, F, G).\nNow, it could just be that room types I and K are *really nice* or this is the frequency of cancelation when a customer\nturns up to the hotel. After all it is not that likely that you are going to cancel in the hotel lobby.\nActually, there is a way to somewhat gauge the quality of the rooms, which it by the average rate *adr* column.\n\n| **assigned_room_type** | count | mean | std | min | 25% | 50% | 75% | max |\n|------------------------|-------|------|-----|-----|-----|-----|-----|-----|\n| **A**                  | 40958 | 93   | 38  | 0   | 68  | 90  | 114 | 337 |\n| **B**                  | 1651  | 94   | 39  | 0   | 75  | 92  | 113 | 276 |\n| **C**                  | 1910  | 107  | 65  | 0   | 61  | 88  | 144 | 508 |\n| **D**                  | 18809 | 101  | 49  | 0   | 64  | 96  | 134 | 376 |\n| **E**                  | 5776  | 112  | 56  | 0   | 69  | 100 | 149 | 452 |\n| **F**                  | 2806  | 142  | 68  | 0   | 86  | 149 | 191 | 368 |\n| **G**                  | 1759  | 158  | 80  | 0   | 99  | 153 | 216 | 510 |\n| **H**                  | 457   | 159  | 84  | -6  | 93  | 150 | 211 | 402 |\n| **I**                  | 352   | 41   | 65  | 0   | 0   | 0   | 63  | 310 |\n| **K**                  | 267   | 53   | 61  | 0   | 0   | 8   | 100 | 283 |\n\nIt does seem like A through H is an increasing price ladder, meaning that H is more exclusive than A.\nNote that I and K have a low average and even medians close to or are zero. So these room assignments tend to be free(!).\nSo if we consider a higher letter to be an upgrade, how often does one get upgraded when a different room is assigned?\n\n|                | **Stayed** | **Canceled** |\n|----------------|------------|--------------|\n| **Downgraded** | 617        | 21           |\n| **Upgraded**   | 13289      | 772          |\n\nFor cancelations the upgrade rate is 97.4% and for regular stays it was 95.6%.\nIf one flips the question the cancelation rates for upgrades is 5.5% and for downgrades at 3.3%.\nThis again is another counter-intuitive insight, how does downgrades make people more likely to not cancel?\n\nBut another observation is the inbalance between stayed and canceled!\nNote from the first table that it is like 44k canceled to 75k stays.\nThe imbalance is also telling of the nature of this problem.\nMost room type changes occur to people who stay:\n\n|                | **Stayed** | **Canceled** |\n|----------------|------------|--------------|\n| **Got the same room type**    | 60839        | 43364           |\n| **New room type assigned**    | 13906      | 793          |\n\nSo what even correlates with *assigned a different room type*?\n\n|                                | correlation |\n|--------------------------------|-------------|\n| is_canceled                    | -0.25       |\n| lead_time                      | -0.14       |\n| adr                            | -0.13       |\n| total_stay                     | -0.10       |\n| guests                         | -0.05       |\n| previous_cancellations         | -0.03       |\n| days_in_waiting_list           | -0.01       |\n| arrival_date_day_of_month      | -0.00       |\n| has_children                   | 0.01        |\n| arrival_date_week_number       | 0.01        |\n| has_babies                     | 0.02        |\n| total_of_special_requests      | 0.02        |\n| previous_bookings_not_canceled | 0.04        |\n| is_alone                       | 0.07        |\n| required_car_parking_spaces    | 0.08        |\n| is_repeated_guest              | 0.09        |\n| booking_changes                | 0.09        |\n\n*Not canceling* (in other words, showing up), booking far in advance, paying a lot, having a long stay,\nbeing a repeated guest, requiring car parking and being alone.\nAnd what correlates with canceling?\n\n|                                | correlation |\n|--------------------------------|-------------|\n| different_room_assigned        | -0.25       |\n| total_of_special_requests      | -0.24       |\n| required_car_parking_spaces    | -0.19       |\n| booking_changes                | -0.14       |\n| is_repeated_guest              | -0.09       |\n| is_alone                       | -0.08       |\n| previous_bookings_not_canceled | -0.06       |\n| has_babies                     | -0.03       |\n| arrival_date_day_of_month      | -0.01       |\n| has_children                   | -0.00       |\n| arrival_date_week_number       | 0.01        |\n| total_stay                     | 0.02        |\n| guests                         | 0.05        |\n| adr                            | 0.05        |\n| days_in_waiting_list           | 0.05        |\n| previous_cancellations         | 0.11        |\n| lead_time                      | 0.29        |\n\nAfter turning this problem up and down it seems like 1. most room type changes are actually upgrades and \n2. that it when the treatment is applied (before or in the hotel lobby) matters for studying this phenomena further.\nProposing a different (simpler) model does not move the remove this issue.\n\n![...](/images/causal/causal_3.png)\n\nThe effect estimate for the above example is -0.35 using propensity score weighting.\nThere are also no strong instrument variables for different room assignment.\nNonetheless the dataset is interesting to study but probably not the best example for causal modeling.\n\n\n## Root-cause of elevated latencies in a microservice architecture\n\n[Link](https://www.pywhy.org/dowhy/v0.13/example_notebooks/gcm_rca_microservice_architecture.html)\n\nThe data is simulated using truncated exponentials, half normals and a Bernoulli hit-miss of the cache.\n\nThe causal graph reflects the system architecture.\n\n![...](/images/causal/causal_4.png)\n\nUsing this causal graph and creating the model as:\n```python\nfrom dowhy import gcm\nfrom scipy.stats import halfnorm\n\ncausal_model = gcm.StructuralCausalModel(causal_graph)\nfor node in causal_graph.nodes:\n    if len(list(causal_graph.predecessors(node))) > 0:\n        causal_model.set_causal_mechanism(node, gcm.AdditiveNoiseModel(gcm.ml.create_linear_regressor()))\n    else:\n        causal_model.set_causal_mechanism(node, gcm.ScipyDistribution(halfnorm))\n```\n\nOne can fit the model to the data and evaluate the fit.\nFor empirical distributions one looks at the Kullback-Leibler (KL) divergence.\nAll of the KL divergence is low so nothing to note there. \nFor the dependent nodes the documentation recomments the normalized continuous ranked probablility (nCRPS).\nCRPS generalizes the mean absolute percentage error (MAPE) to probabalistic predictions.\nIt gives a good idea of the model accuracy and calibration to the causal mechanisms.\n\n\n|                           | **KL** | **nCRPS** |\n|---------------------------|--------|-----------|\n| **Customer DB**           | 0.042  |           |\n| **Shipping Cost Service** | 0.010  |           |\n| **Product DB**            | 0.065  |           |\n| **Order DB**              | 0.049  |           |\n| **Auth Service**          |        | 0.29      |\n| **Caching Service**       |        | 0.45      |\n| **Order Service**         |        | 0.30      |\n| **Product Service**       |        | 0.36      |\n| **API**                   |        | 0.11      |\n| **www**                   |        | 0.07      |\n| **Website**               |        | 0.09      |\n\nNote that product service and caching service have relatively high nCRPS.\nThe evaluator logic picks out that this is only \"fair\" model perfromance and not \"good\" like the others\n(presumably the threshold is $\\approx 0.3$).\nThis is expected as the data generation process for these two variables are non-additive noise models.\nThe cache has a multiplicative aspect due to cache misses and the product service is a max function of its upstream components, caching service included.\n\nNonetheless, the model can still be used to produce insights.\n\nNext step is to use the [attribute_anomalies](https://proceedings.mlr.press/v162/budhathoki22a/budhathoki22a.pdf) function to\nscore anomalous observations. In this example they have created an anomalous version of the data where the website is slower.\nBy bootstrapping one can get confidence intervals for this estimation as:\n```python\nmedian_attribs, uncertainty_attribs = gcm.confidence_intervals(\n    gcm.fit_and_compute(\n        gcm.attribute_anomalies,\n            causal_model,\n            normal_data,\n            target_node='Website',\n            anomaly_samples=outlier_data.iloc[[0]]\n    ),\n    num_bootstrap_resamples=10\n)\n```\n\n![...](/images/causal/causal_5.png)\n\nHere the attribution states that the cache is the main driver of high latency.\nCustomer DB has a negative contribution, meaning that it was fast.\nProduct service also shows positive contribution but this could be a leaking effect from the caching due to the nonlinear relationships.\nIn order to get more clarity than this anecdotal example we can analyze 1000 anomalous samples using the [distribution_change](https://assets.amazon.science/b6/c0/604565d24d049a1b83355921cc6c/why-did-the-distribution-change.pdf) function.\n\n```python\nmedian_attribs, uncertainty_attribs = gcm.confidence_intervals(\n    lambda : gcm.distribution_change(\n        causal_model,\n        normal_data.sample(frac=0.6),\n        outlier_data.sample(frac=0.6),\n        'Website',\n        difference_estimation_func = lambda x, y: np.mean(y) - np.mean(x)),\n    num_bootstrap_resamples = 10\n)\n```\n\n![...](/images/causal/causal_6.png)\n\nClear as day, it is the caching service that has experienced a distribution shift.\nUsing our causal model we can also simulate an intervention where we allocate resources from shupping cost service to caching service.\nWe trade 1 second faster of caching service for 2 seconds slower shipping cost. The code for this:\n```python\nmedian_mean_latencies, uncertainty_mean_latencies = gcm.confidence_intervals(\n    lambda : gcm.fit_and_compute(\n        gcm.interventional_samples,\n        causal_model,\n        outlier_data,\n        interventions = {\n            \"Caching Service\": lambda x: x-1,\n            \"Shipping Cost Service\": lambda x: x+2\n        },\n        observed_data = outlier_data)().mean().to_dict(),\n    num_bootstrap_resamples=10\n)\n```\nThe median mean latency of the website changes from 5.5 to 4.5 seconds.\n\n\n## Impact of 401(k) eligibility on net financial assets\n\n[Link](https://www.pywhy.org/dowhy/v0.13/example_notebooks/gcm_401k_analysis.html)\n\n[Data source](https://martinspindler.r-universe.dev/datasets)\n\nIn this example the authors propose to study the effect of being eligible for a 401(k) plan on the net financial assets.\nThis is basically saying \"does this plan make people wealthier?\".\nRunning the example one is struck with the fact that connecting a binary variable to a number of $s becomes tricky.\nRetirement plans like 401(k) growths with age and higher income. It is a compounded process.\n\nTheir proposed causal graph is one where all covariates impact the treatment and the outcome.\n\n![...](/images/causal/causal_7.png)\n\nBy pruning edges using causal minimality tests one gets the following causal graph instead:\n\n![...](/images/causal/causal_7_2.png)\n\nThe code for the causal minimality test: \n```python\ndef test_causal_minimality(graph, target, data, method='kernel', significance_level=0.10, fdr_control_method='fdr_bh'):\n    p_vals = []\n    all_parents = list(graph.predecessors(target))\n    for node in all_parents:\n        tmp_conditioning_set = list(all_parents)\n        tmp_conditioning_set.remove(node)\n        p_vals.append(\n            gcm.independence_test(\n                data[target].to_numpy(), \n                data[node].to_numpy(), \n                data[tmp_conditioning_set].to_numpy(), \n                method=method\n            )\n        )\n\n    if fdr_control_method is not None:\n        p_vals = multipletests(p_vals, significance_level, method=fdr_control_method)[1]\n\n    nodes_above_threshold = []\n    nodes_below_threshold = []\n    for i, node in enumerate(all_parents):\n        if p_vals[i] \u003C significance_level:\n            nodes_above_threshold.append(node)\n        else:\n            nodes_below_threshold.append(node)\n\n    print(\"Significant connection:\", [(n, target) for n in sorted(nodes_above_threshold)])\n    print(\"Insignificant connection:\", [(n, target) for n in sorted(nodes_below_threshold)])\n\n    return sorted(nodes_below_threshold)\n```\n\nUpon seeing this I also thought that it would be interesting to try out some causality identification methods from causal-learn.\nUsing Peter-Clark algorithm for causal discovery one gets the following graph:\n\n![...](/images/causal/causal_7_3.png)\n\nAnd using Fast Causal Inference (FCI) one gets this one:\n\n![...](/images/causal/causal_7_4.png)\n\nAnd for Greedy Equivalence Search (GES):\n\n![...](/images/causal/causal_7_5.png)\n\nAnd finally, Linear, Non-Gaussian Acyclic Model (LiNGAM):\n\n![...](/images/causal/causal_7_6.png)\n\nOut of all the methods only LiNGAM put e401 $\\rightarrow$ net financial assets.\n\nTo continue with the LiNGAM suggestion after pruning some edges:\n\n![...](/images/causal/causal_7_7.png)\n\nUsing the auto assignment of causal mechanisms `gcm.auto.assign_causal_mechanisms` yields:\n\n```\nage Discrete AdditiveNoiseModel using HistGradientBoostingRegressor\ninc Discrete AdditiveNoiseModel using HistGradientBoostingRegressor\nfsize Discrete AdditiveNoiseModel using Pipeline\neduc Discrete AdditiveNoiseModel using LinearRegression\ndb Empirical Distribution\nmarr Discrete AdditiveNoiseModel using HistGradientBoostingRegressor\ntwoearn Discrete AdditiveNoiseModel using HistGradientBoostingRegressor\npira Discrete AdditiveNoiseModel using Pipeline\nhown Discrete AdditiveNoiseModel using HistGradientBoostingRegressor\nhequity Empirical Distribution\nhmort Discrete AdditiveNoiseModel using HistGradientBoostingRegressor\nnohs Discrete AdditiveNoiseModel using Pipeline\nhs Discrete AdditiveNoiseModel using Pipeline\ne401 Classifier FCM based on HistGradientBoostingClassifier()\nnet_tfa Discrete AdditiveNoiseModel using HistGradientBoostingRegressor\n```\n\nThe model evaluation suite claims that the fit is only subpar, \nwhich makes sense that there is a ton of noise and the causality not being that strong.\nWhen we perform the same CATE per income bracket estimation as in the example we see some difference:\n\n![...](/images/causal/causal_13.png)\n\nNote that the effect has a lower bound of approximately 0 for the lower 50% and that the higher earners have a higher lower bound.\nThis basically means that the effect is more inequal than the example made it out to be.\n\n\n## Intrinsic influence on river flow\n\nIn this example the authors extract flow (but personally I prefer water level) data for 5 measuring stations.\nThe [Department for Environment Food & Rural Affairs](https://environment.data.gov.uk/hydrology/explore) provides it for free.\nThe five stations map out three tributaries to River Ribble, which is measure upsteam at New Jumbles Rock and downstream at Samlesbury\n\n![...](/images/causal/river-map.jpg)\n\nThe following causal graph can be constructed:\n\n![...](/images/causal/causal_8.png)\n\nAnd fitted to the time series of the level measurements, taken every 15 mins.\nBelow is a line chart of the water levels for about a year.\nAs one can expect; floodings correlate.\nBut we want to figure out which are the more significant contributors.\n\n![...](/images/causal/causal_10.png)\n\nUsing simple statistics from the table below we see that Henthorn both has the highest water level and correlation to Samlesbury.\nSo it could be that it contibutes the most. \nAlthough, consider that New Jumbles Rock add no new tributary, so its contribution should be negible.\n\n| ****                 | **Correlation to Samlesbury** | **Average river level** |\n|----------------------|-------------------------------|-------------------------|\n| **Hodder-Place**     | 0.883594                      | 0.489060                |\n| **Whalley-Weir**     | 0.898620                      | 0.489290                |\n| **New-Jumbles-Rock** | 0.958220                      | 0.486724                |\n| **Henthorn**         | 0.962957                      | 0.576654                |\n| **Samlesbury**       | 1.000000                      | 1.227162                |\n\nSetting up the structual causal model as\n```python\nscm_river = gcm.StructuralCausalModel(river_graph)\ngcm.auto.assign_causal_mechanisms(scm_river, df)\ngcm.fit(scm_river, df)\n```\n\nOne can then calculate the [intrincic_causal_influence](https://proceedings.mlr.press/v238/janzing24a/janzing24a.pdf) for this model and data.\n```python\niccs_river = gcm.intrinsic_causal_influence(scm_river, target_node='Samlesbury')\n```\n\nBy normalizing the intrincic causal influence one gets the following attribution:\n![...](/images/causal/causal_9.png)\n\nNote that New Jumbles Rock is close to zero, as expected.\nSamlesbury has some unexplained noise, probably due to water seeping in between the measurement stations.\nThe highest contribution comes from Henthorn, then Hoddler-Place and lastly Whalley-Weir.\n\n\n## Counterfactual analysis in medical care\n\nIn this example the authors generate synthetic data.\nThere are 3 variables:\n* Condition, binary - Does the patient have a rare condition, for example an allergy.\n* Treatment, 3 options - 0 = Do nothing, 1 = Treatment 1, 2 = Treatment 2.\n* Vision, real value - The patients vision after treatment.\n\nWe get no information of the vision before treatment, which is basically the noise in this problem.\nThe causal model is can assume is simple.\n\n![...](/images/causal/causal_11.png)\n\nThe `gcm.auto.assign_causal_mechanisms` fits an additive noise model using a histogram gradient boosting regressor.\nThe distributions for treatment and condition are empirical.\n\nNow, given a specific patient *Alice* we are interested in a counterfactual question.\nShe has a rare condition (= 1), took treatment 2 and she got a good vision after the treatment.\nUsing our model one can pose the counterfactual question of what would have happened if she would not have done the treatment or treatment 1.\n\n```python\ncounterfactual_data1 = gcm.counterfactual_samples(\n    causal_model,\n    {'Treatment': lambda x: 1},\n    observed_data = specific_patient_data\n)\n\ncounterfactual_data2 = gcm.counterfactual_samples(\n    causal_model,\n    {'Treatment': lambda x: 0},\n    observed_data = specific_patient_data\n)\n```\n\n![...](/images/causal/causal_12.png)","src/content/blog/causal-modeling.mdx","f0ffd5fdcb41d95d","causal-modeling.mdx","incremental-quantile-estimation-2",{"id":84,"data":86,"body":91,"filePath":92,"digest":93,"legacyId":94,"deferredRender":24},{"title":87,"description":88,"pubDate":89,"tags":90},"An alternative method for incremental quantile estimation","Focusing on Bayesian validation for alternative incremental quantile estimation",["Date","2025-07-28T00:00:00.000Z"],[32,44],"import ImageGallery from '../../components/ImageGallery.astro';\n\nWhile finishing up the last post on incremental quantile estimation got intrigued by the Bayesian estimator.\nIt can basically verify any method, so the question is if one can make a simpler (and more robust) approach than the nudger?\nTo strengthen my intuition I ran an experiment where one takes the first 10 observations and estimate their quantiles.\nThe quantile we measure our error towards is $q = 0.9$.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_1.png)\n\nNotice that the 0 error line is between the yellow observation 5 (=1.64) and blue observation 0 (=1.01).\nThis is also where we find the red sample quantile.\nNow, what to do with this information?\nEither one can start throwing away the lousy estimates for better ones (like, in between the blue and yellow), or one can be smarter about their initial selection.\n\nTo address the latter.\nRemember from the previous posts that one expects to see the range $R$ after $n$ observations $R = (n-1)/(n+1)$.\nThis also meant that one had to wait until $n = \\frac{1-\\tilde{q}}{\\tilde{q}}$ where $\\tilde{q} = min(q, 1-q)$\nto match the expected range of observed quantiles. \nSo for our example $q = 0.9 \\rightarrow n = 9$ we basically already did this.\nHowever, with this reasoning we should pick our first observation as our median estimate and that does not always work out.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_2.png\",\n      alt: \"Using the first observation to approximate the median\",\n      caption: \"Using the first observation to approximate the median\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_3.png\",\n      alt: \"Using the first observation to approximate the median\",\n      caption: \"Using the first observation to approximate the median\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_4.png\",\n      alt: \"Using the first observation to approximate the median\",\n      caption: \"Using the first observation to approximate the median\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nSo there is clearly some value in having more than 1 observation in order to learn something more about the spread of quantiles.\nProbably the more the merrier but we are interested in small solutions, so $n = $ 2, 3, 5 or even 10 would be nice.\nTo make our task easier to manage we can start by giving ourselves a calibration window $w$ where we get to pick our $n$ observations.\n\nOne thing to keep track of if $w \u003C \\frac{1-\\tilde{q}}{\\tilde{q}}$. \nIn that case we know that our estimate is the maximum or minimum observation times a factor $\\delta$ for the potentially missing range $\\delta = \\frac{R_q}{R_w} = (1-2\\tilde{q})\\frac{w+1}{w-1}$.\nFor the case where we have seen the observed the range we can sort the observations and pick out the quantile estimate from there.\nOf course, that would require us to buffer the observations, in which case we might as well call $n = w$.\nThen we pick the ith indexed observations in a sorted list which matches our quantile (for $q = 0.9$ we pick the 9th out of 10), then we pick $\\pm 1$ indices around it and cram out estimates into that range.\nThis does not work out that nice as can be seen below:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_5.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Using the first n observations to naively estimate a quantile range\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_6.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Using the first n observations to naively estimate a quantile range\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_7.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Using the first n observations to naively estimate a quantile range\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nThis is a good example where trying to be clever can be severly punishing, as in example 2.\nIf we put that idea on pause and instead consider another dimension of our estimate, *how long does it take until we can trust it?*\nFor both the median and $q = 0.9$ it seems to take at least 100 observations before the Bayesian estimate stabilizes.\nThis is also a limitation to be way of because it means that we can not shuffle our observations around and reseting the Bayesian estimates too often.\n\nAnother fact that to note is the **correlation** between the quantile estimation errors. \nThis is because they are estimated using the same random sequence of observations.\nFor two observations $y_1, y_2$ that are close to each other $\\Delta y = |y_2-y_1| \u003C\u003C 1$ this means that differentiating between the two would require an observation that lies between them.\nThis means that one probably want to randomly drop observations to avoid correlation in the estimates and that one wants to estimate relatively distinct observations.\n\nFor a dropout probability of 50%:\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_8.png)\n\nSo, back to the algorithm crafting. \nIf we start with the first $M$ observations that we did before (better not try to be clever).\nThe next step is after $W$ observations to reset the observations based on what we have learned.\nIf we take a look at the quantile estimation errors vs the observation values at $n = 100$ we have the following scatter plot.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_9.png)\n\nFor this example we do not even have an observation on either side of the 0.\nThis means that the method we use need to extrapolate.\nA linear regression will do a decent job for this example.\nHowever, even this demo distribution (Gaussian) is not linear as it is only linear-ish.\nTherefore multiple shooting is required to narrow down the interval of shooting.\n\nOne important choice will be the how to take the linear regression $y = kx + m$ and use that information.\nThe most likely position of the quantile is the intercept $m$ but only one estimate can have that position.\nWe also know that we do not want to agressively cluster the estimates either.\nA fair initial assumption is to assume some $\\pm \\Delta q$ interval, like $\\pm 0.1$ quantiles or $\\pm 0.05$ quantiles.\nThis means that for $W = 100$, $M = 10$, $\\Delta q = 0.1$ every 100 observations we will reset the estimates to $(-0.1, -0.0\\bar{7}, -0.0\\bar{5}, ..., 0.1)$ quantile errors according to a linear fit of error and observations.\nThe result for a few different $\\Delta q$ can be seen below.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_10.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Interval +/-0.1\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_11.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Interval +/-0.05\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_12.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Interval +/-0.01\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNote that only $\\pm 0.1$ quantiles does not collapse to a single value unlike $\\pm 0.05$ and $\\pm 0.01$ quantiles.\nAnother observation is that $\\Delta q = 0.1$ collapses to a single value for the median, so that property is dependent on $q$.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_13.png)\n\nHere is what its corresponding linear fit looks like. \nIt basically collapses into a ball where quantiles and estimates no longer correlate.\nThis is a symptom of what we disussed before, longer periods are needed to differentiate between the points.\nIt is also information if we do not get a positive slope, because by definition it should always be positive.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_14.png)\n\nBy doubling the $W$ we give the observations more time to converge, avoiding collapse for the median.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_15.png\",\n      alt: \"...\",\n      caption: \"Doubling the reset interval W after every reset.\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_16.png\",\n      alt: \"...\",\n      caption: \"It corresponding linear fit at 1000 observations\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nAfter experimenting with $\\Delta q$ I thought of an even simpler way of using the result of the linear model.\nIf all we care about is the intercept $m$, then lets just make our new batch of estimates $y_{i,new} = (1-\\gamma) y_{i,old} + \\gamma m$ where $y_i$ is the $i$th observation.\nThis keeps the behavior of the median the same and also makes $q = 0.9$ more sensible. See the difference below for the same data.\nIn this example $\\gamma = 0.5$, which seems to be a good compromise between converge and keeping the spread of estimates.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_17.png\",\n      alt: \"...\",\n      caption: \"Linear model\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_18.png\",\n      alt: \"...\",\n      caption: \"Smoothing model\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nOf course, this method runs the risk of getting stuck below or over the estimate. \nThis tends to happen if all the $M$ initializing observations are on one side of the quantile $q$ we are trying to estimate. \nIf our estimations are biased (one-sided), we should correct for it as $y_{i,new} = (1-\\gamma) y_{i,old} + \\gamma m - \\gamma bias$,\nwhere the $bias = \\frac{1}{M}\\sum_{i=1}^{M} (q - y_i)$ and $y_i$ are estimates. See the difference below.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_19.png\",\n      alt: \"...\",\n      caption: \"Without bias\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_20.png\",\n      alt: \"...\",\n      caption: \"With bias\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNow the final step is to actually put a number of what we think the quantile is.\nThere are a few simple options like the intercept $m$ that is used to update the estimates.\nAnother option is to take the estimate with the lowest quantile estimation error.\nYet another option is to do the linear regression after every observation and mix it with the intercept as $m = (1 - \\frac{c_W}{W}) m_{reset} + \\frac{c_W}{W} m_{online}$, \nwhere $c_W$ is the counter towards the reset window $W, m_{reset}$ is the intercept that determines the estimates and $m_{online}$ is always calculated.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_21.png\",\n      alt: \"...\",\n      caption: \"q = 0.9 intercept as quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_22.png\",\n      alt: \"...\",\n      caption: \"q = 0.5 intercept as quantile estimate\"\n    },\n      {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_23.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 intercept as quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_24.png\",\n      alt: \"...\",\n      caption: \"q = 0.9 lowest quantile estimation error estimate as quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_25.png\",\n      alt: \"...\",\n      caption: \"q = 0.5 lowest quantile estimation error estimate as quantile estimate\"\n    },\n      {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_26.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 lowest quantile estimation error estimate as quantile estimate\"\n    },\n      {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_27.png\",\n      alt: \"...\",\n      caption: \"q = 0.9 lowest mix method as quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_28.png\",\n      alt: \"...\",\n      caption: \"q = 0.5 lowest mix method as quantile estimate\"\n    },\n      {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_29.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 lowest mix method as quantile estimate\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nOut of the three the mix method provides a seemingly stable output.\nNote that all methods have troubles converging when $q \\geq 0.9$.\nThis is due to the design of our algorithm.\nWe are not making sure to include our maximum and minimum in our estimation ranges. Instead we are relying on our initial $M$.\nWe are also using pretty aggressive reset windows, we would not expect to see a $q = 0.999$ observation within $W = 100, 200, ...$.\nIt is also possible to play around with the bias correction and convert it to PI-regulator, meaning that we accumulate biases over time.\nAnother way would also be to amplify the bias factor when $q$ is close to 0 or 1, effectively adding a P-factor. \n\nA few combined steps include:\n* Track the min $a$ and max $b$.\n* Make sure the slope $k \\geq 0$.\n* Scale the bias by $k$ as $y_{i,new} = (1-\\gamma) y_{i,old} + \\gamma (m -  \\cdot k \\cdot bias)$.\n* Calculate the $\\Delta y_{overstep} = max(0, max_i(y_{i,new}) - b)$ and $\\Delta y_{understep} = min(0, min_i(y_{i,new}) - a)$.\n* Adjust the estimates by any overstep or understep $y_{i,adj} = y_{i,new} - \\Delta y_{overstep} - \\Delta y_{understep}$.\n\nThis is basically a sanity check (are we outside of bounds) and using the slope which should be the correct P-factor.\nIt is not a perfect solve as can be observed by the third example in the graphs below.\nThe weirdness seems to appear if we are unfortunate enough to put all of our estimates above the true quantile.\nThis can potentially be mitigated but similarly to the previous posts, lets reserve extreme quantiles for later.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_30.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 with proposed k * bias and over/understep correction\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_31.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 with proposed k * bias and over/understep correction\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_32.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 with proposed k * bias and over/understep correction\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nLet's write down the algorithm so far:\n\n```python\ndef linear_regression(x, y):\n    n = len(x)\n    xm = sum(x) / n\n    ym = sum(y) / n\n    variance = sum((x - xm)**2)\n    covariance = sum((x - xm)*(y - ym))\n    slope = covariance / variance\n    intercept = ym - slope * xm\n    return slope, intercept\n\ndef estimate_quantiles(\n        data: np.array, \n        quantile: float,\n        number_of_estimators: int = 10,\n        dropout_probability: float = 0.5,\n        reset_window: int = 100,\n        reset_window_scaling: float = 2,\n        update_factor: float = 0.5\n    ) -> np.array:\n    '''\n    This function takes an array of observations, \n    treats them as streaming data and incrementally \n    estimates the q quantile. The estimate converges\n    over time as the estimate approaches the true\n    quantile.\n\n    :param data: Input data as a numpy array.\n    :param quantile: The desired quantile to estimate.\n    :param m_initial_steps: The desired number of steps until an initial guess.\n    :param quantile_sigma: A parameter specifies the confidence interval.\n    '''\n\n    q_tilde = min(quantile, 1-quantile)\n    a = min(data[:number_of_estimators]) # Signal min\n    b = max(data[:number_of_estimators]) # Signal max\n    estimates = data[:number_of_estimators]\n\n    n = np.zeros(number_of_estimators)\n    s = np.zeros(number_of_estimators)\n    alpha = quantile * 1/q_tilde * np.ones(number_of_estimators)\n    beta = (1-quantile) * 1/q_tilde * np.ones(number_of_estimators)\n\n    quantile_estimates = []\n    quantile_estimate = 0\n    counter = 0\n    reset_window_scale = 1\n    reset_intercept = 0\n    reset_slope = 0\n\n    for y in data[number_of_estimators:]:\n        a = min(a, y)\n        b = max(b, y)\n        dropout = 1*(np.random.rand(number_of_estimators) \u003C dropout_probability)\n        n += dropout*1\n        s += dropout*1*((estimates - y) >= 0)\n        quantile_posterior_expected_value = (alpha + s)  / (alpha + beta + n)\n        posterior_expected_value_diff = (quantile_posterior_expected_value - quantile)\n\n        slope, intercept = linear_regression(posterior_expected_value_diff, estimates)\n        slope = max(0, slope)\n        bias = sum(posterior_expected_value_diff)/number_of_estimators\n\n        if counter >= reset_window_scale * reset_window:\n            reset_intercept = intercept\n            reset_slope = slope\n            estimates = (\n              (1 - update_factor) * estimates \n              + update_factor * reset_intercept \n              - update_factor * bias * reset_slope\n            )\n            overstep = max(0, max(estimates) - b)\n            understep = min(0, min(estimates) - a)\n            estimates = estimates - overstep - understep\n            n = np.zeros(number_of_estimators)\n            s = np.zeros(number_of_estimators)\n            reset_window_scale = reset_window_scaling*reset_window_scale\n            counter = 0\n\n        if reset_window_scaling == 1:\n            quantile_estimate = intercept\n        else:\n            reset_alpha = counter/(reset_window*reset_window_scale)\n            quantile_estimate = (1-reset_alpha) * reset_intercept + reset_alpha * intercept\n\n        counter += 1\n        quantile_estimates.append(quantile_estimate)\n      \n    return quantile_estimates\n```\n\nHere are a few examples of running it for multiple quantiles:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_33.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_34.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_35.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_36.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_37.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for uniform noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_38.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for uniform noise\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNow, if one designing this algorithm for monitoring a distribution like this then one might can probably share estimators.\nThat can be the topic of another post together with other methods for estimating distributions.\nOne question we looked into in the previous post about incremental quantile estimation was how the estimator behaves when the distribution shifts.\nThe short answer is that it can not follow it. \nThe estimator breaks and even if we add in a forgetting factor we have the problem that this approach does not fare well when the observations are one sided.\nThis is what happens when our distribution shifts far, like in the second shift in the example below. \nAll estimates are far beyond the new maximum and thus the linear fit will be uninformative.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_39.png)\n\nI will continue to meditate on this approach and how it could be robustified.","src/content/blog/incremental-quantile-estimation-2.mdx","d746cf1236343efb","incremental-quantile-estimation-2.mdx","incremental-quantile-estimation",{"id":95,"data":97,"body":102,"filePath":103,"digest":104,"legacyId":105,"deferredRender":24},{"title":98,"description":99,"pubDate":100,"tags":101},"Extending the median estimator to any quantile","A frequentist ping-pong estimator extended to any quantile",["Date","2025-07-18T00:00:00.000Z"],[32,44],"import ImageGallery from '../../components/ImageGallery.astro';\n\nThis post continues from the incremental median estimation.\n\nTo estimate any quantile instead of the median one adds a bias to the sign of the nudge.\nWhat does this represent? \nConsider the case of the 90th percentile, if our estimate is accurate then we expect 90% of the incoming observations to be below the estimate.\nThis means that the 90 / 10 split must be the equilibrium between the 90% downward nudges and 10% upward nudges.\nThe bias $b = 1 - 2q$ for the 90% percentile is $b = -0.8$, this means that a positive sign $\\rightarrow 0.2$ and a negative $\\rightarrow -1.8$.\nIf we multiply these nudges by their frequency we see that they are in balance: $0.9 \\cdot 0.2 + 0.1 \\cdot -1.8 = 0.18 - 0.18 = 0$.\n\nOne thing to note is that the average absolute nudge is $0.18 + 0.18 = 0.36$, which is lower than $\\|0.5 \\cdot 1\\| + \\|0.5 \\cdot -1\\| = 1.0$ for the median.\nThis means that if we use a bias we also want to scale the size of the nudge by the inverse of $s_b = 4\\tilde{q}(1-\\tilde{q})$ where $\\tilde{q} = min(q, 1-q)$. \nThis means that an updated algorithm can be written as:\n\n```python\ndef update_quantile_estimate(\n        quantile_to_estimate: float,\n        quantile_estimate: float, \n        observation: float, \n        nudge: float\n    ) -> float:\n    bias = 1 - 2 * quantile_to_estimate\n    q_tilde = min(quantile_to_estimate, 1-quantile_to_estimate)\n    scale_bias = 4*q_tilde*(1-q_tilde)\n    diff = quantile_estimate - observation\n    sign = diff / abs(diff)\n    new_quantile_estimate = quantile_estimate - nudge * (sign + bias) / scale_bias\n    return new_quantile_estimate\n```\n\nThe nudges can utilize the same schema as for the median $\\frac{R(n+1)}{M(n-1)}$ for quick converge.\nBut the rubber band random walk confidence interval $\\frac{dq}{\\tilde{q_t}q_t} = 8\\sigma^2$ that we empirically derived is no longer applicable.\nOn top of the rubber band (pulling us toward the quantile estimate when we drift to far) and the random walk (because the observations are random in nature) aspects, we also now have a drift (bias) to account for.\n\nAfter repeating the experiment for different values of $q$ and refit the function we find that actually $\\frac{dq}{\\tilde{q_t}} = \\frac{4\\sigma^2}{\\tilde{q_t}}$.\nThis means that $dq$ is actually $q$-invariant and that the simplified formula gives $dq = 4\\sigma^2$ and that the $\\pm 2.5$ percentile CI $\\rightarrow M = 1536.4$ is universal.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_1.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_2.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_3.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_4.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_5.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNote that the $q = 0.01$ quantile looks a bit jumpy and $q = 0.999$ even more so. \nThat is because we have so few observations, meaning that the nudges become highly unbalanced.\nAnother factor is the fact that we used $\\pm 2.5$ percentile confidence interval for something like the 1st percentile.\nSo we accept quite a lot of noise in our estimate.\nHere is how the estimation of $q = 0.999$ looks for 10000 observations:\n\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_6.png)\n\nIf one counts the upward jumps one gets 11, which is close to the expected value of $(1-0.999) \\cdot 10000 = 10$.\nThere are some potential remedies for this, the first one is to adjust the confidence interval to match the quantile.\nThis would mean that $dq = 4\\sigma^2 = 4(\\frac{0.001}{1.96})^2 = \\frac{1}{960400}$.\nThis first approach will do the trick but it will take forever to converge.\n\nThe second potential remedy is just a sanity check. *Why are we allowing the quantile estimate to go beyond our maximum or minimum observed value?*\nThis was not a problem for the median estimation but here it is clearly an issue.\nBy bounding the estimate one gets something that looks like this.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_7.png)\n\nHowever, this leaves an issue on how to deal with the excess as it is required to balance the downward nudges.\nIf one allows the excess to carry over and nullify the opposite nudges then one ends up with flat segments like this.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_8.png)\n\nNeither of these are that satisfactory because they do not solve the jumpiness at its core.\nWe should also state that the extreme quantiles are something we will rarely need.\nLet us focus on another aspect of quantile estimation and we might be able to resove the issue in another way.\n\nThe jumpiness fundamentally comes from our frequentist approach to estimating the quantiles.\nTo approach the problem from a Bayesian point of view we can attempt correct our frequentist estimator $\\hat{q}$.\nThe $\\pm$ signs $x_i$ around our estimator $\\hat{q}$ follow a Bernoulli distribution $x_1, ..., x_n \\sim$ Bernoulli($p$). \nThe beta distribution $\\beta(a,b)$ is a conjugate prior distribution to Bernoulli($p$).\nThe posterior distibution becomes $p\\:|\\:x_1, ..., x_n \\sim \\beta(a + s,b + t)$, where $s$ are the number of successes and $t$ the total number of observations.\nThis means that $E(p\\:|\\:x_1, ..., x_n) = \\frac{a + s}{a + b + t}$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_9.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_10.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_11.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nOne way to use this is to track $|E(p\\:|\\:x_1, ..., x_n) - q|$ and use that to slow down the nudges.\nThe rescale nudges $u$ could be $u_{new} = u\\frac{|E(p\\:|\\:x_1, ..., x_n) - q|}{min(\\tilde{q},\\:1.96\\sigma)}$. \nThis means that is $E(p) = q$ then we stop nudging altogether and we have normal nudges when we are outside the confidence interval determined by our $\\sigma$ hyperparameter.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_12.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_13.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_14.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nAnd to make sure that we have not broken anything, this is what it looks for the median.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_15.png)\n\nA complete function for doing this estimation can be written as:\n\n```python\ndef estimate_quantiles(\n        data: np.array, \n        quantile: float, \n        m_initial_steps: int = 60, \n        quantile_sigma: float = 0.025\n    ) -> np.array:\n    '''\n    This function takes an array of observations, \n    treats them as streaming data and incrementally \n    estimates the q quantile. The estimate converges\n    over time as the estimate approaches the true\n    quantile.\n\n    :param data: Input data as a numpy array.\n    :param quantile: The desired quantile to estimate.\n    :param m_initial_steps: The desired number of steps until an initial guess.\n    :param quantile_sigma: A parameter specifies the confidence interval.\n    '''\n    # Derived quantities\n    q_tilde = min(quantile, 1-quantile)\n    bias = 1 - 2 * quantile\n    scale_bias = 4*q_tilde*(1-q_tilde)\n    dq = 4*(quantile_sigma/1.96)**2\n    m_initial_steps_adjusted = max(m_initial_steps, (1-(q_tilde))/(q_tilde))\n    posterior_diff_scale = min(quantile_sigma/1.96, q_tilde)\n\n    a = data[0] # Signal min\n    b = data[0] # Signal max\n    quantile_estimates = []\n    quantile_estimate = data[0] # Initialize to the first observation\n\n    total_number_of_observations = 0\n    number_of_negative_side_observations = 0\n    total_number_of_observations_prior = 1/q_tilde\n    number_of_negative_side_observations_prior = quantile*total_number_of_observations_prior\n    excess = 0\n    n = 1 # We have initialized on the first data point\n\n    for y in data[1:]:\n        n += 1\n        a = min(a, y)\n        b = max(b, y)\n        r = b - a\n        diff = quantile_estimate - y\n        sign = diff / abs(diff)\n        nudge = r * (n+1) / (m_initial_steps_adjusted * (n-1))\n    \n        if n \u003C= m_initial_steps_adjusted:\n            quantile_update = nudge * (sign + bias) / scale_bias\n            new_quantile_estimate = quantile_estimate - quantile_update\n            quantile_estimate = min(b, max(a, new_quantile_estimate))\n        else:\n            total_number_of_observations += 1\n            if sign > 0:\n                number_of_negative_side_observations += 1\n            quantile_posterior = (\n                (\n                  number_of_negative_side_observations_prior \n                  + number_of_negative_side_observations\n                ) / (\n                  number_of_negative_side_observations_prior \n                  + total_number_of_observations_prior \n                  + total_number_of_observations\n                )\n            )\n            posterior_diff = (quantile_posterior - quantile)\n            quantile_update = (\n              r \n              * dq \n              * (sign + bias) / scale_bias \n              * abs(posterior_diff) / posterior_diff_scale\n            )\n            if excess and (excess * quantile_update) > 0:\n                excess_sign = excess / abs(excess)\n                excess -= quantile_update\n                if excess_sign != excess / abs(excess):\n                    excess = 0\n                quantile_update = 0\n            new_quantile_estimate = quantile_estimate - quantile_update\n            quantile_estimate = min(b, max(a, new_quantile_estimate))\n            excess += new_quantile_estimate - quantile_estimate\n        quantile_estimates.append(quantile_estimate)\n\n    return quantile_estimates\n```\n\nAnd here is what it looks like when you fire of an array of them ($q \\in (0.1, 0.2, ..., 0.9)$):\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_16.png\",\n      alt: \"Quantile estimates for Gaussian noise\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_17.png\",\n      alt: \"Quantile estimates for Gaussian noise\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_18.png\",\n      alt: \"Quantile estimates for exponential noise\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n      {\n      src: \"/images/incremental_quantile/incremental_quantile_19.png\",\n      alt: \"Quantile estimates for exponential noise\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_20.png\",\n      alt: \"Quantile estimates for uniform noise\",\n      caption: \"Quantile estimates for uniform noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_21.png\",\n      alt: \"Quantile estimates for uniform noise\",\n      caption: \"Quantile estimates for uniform noise\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nBut what happens for non-stationary data, say that the distribution shifts?\nIn the example below we shift from $x \\sim N(0, 1)$ to $x \\sim N(2,1)$ after 1000 steps.\nThe estimates do follow along but the jaggedness is back.\nThis is because our Bayesian estimator is cumulative and it will therefore take a long time before those initial 1000 steps become neglible.\n\n![Shifted distribution](/images/incremental_quantile/incremental_quantile_22.png)\n\nA way to speed things up is to add a \"forgetting factor\" $\\lambda$ to the cumulative $s$ and $t$.\nBecause our quantile rate of change is $dq$ we define the forgetting factor $\\lambda = (1-c \\cdot dq)$.\nThis means that the forgetting matches the inertia induced by our desired confidence interval.\nSay that we want to forget 90% within one $\\frac{1}{dq}$ observations, \nsolving the expression $0.1 = (1 - c \\cdot dq)^\\frac{1}{dq} \\rightarrow c = \\frac{1 - e^{dq \\cdot log(0.1)}}{dq}$,\nwhich for our typical confidence interval means $c \\approx 2.3$.\nPutting this back into our forgetting factor simplifies the solution $\\lambda = (1- \\frac{1 - e^{dq \\cdot log(0.1)}}{dq} \\cdot dq) = e^{dq \\cdot log(0.1)}$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_23.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_24.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_25.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_26.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nThe code additions are as follows.\n```python\n...\nforgetting_factor = np.exp(dq * np.log(0.1))\n...\nfor y in data[1:]:\n  ...\n  if n \u003C= m_initial_steps_adjusted:\n    ...\n  else:\n    total_number_of_observations = forgetting_factor*total_number_of_observations\n    number_of_negative_side_observations = forgetting_factor*number_of_negative_side_observations\n    total_number_of_observations += 1\n    if sign > 0:\n      number_of_negative_side_observations += 1\n    ...\n```\n\nOne added bonus to this framework is that one can extract the $\\delta_q = E(p\\:|\\:x_1, ..., x_n) - q$ and sum $\\sum_{q} \\delta_q$ as a measure of distribution deviation.\nThis aggregated signal can be used to indicate that the estimator is converging.\nIt can also be treated as an indicator for change but there are better ways to do that, plus\nas one can see in the example in the example below, it does not capture widening or narrowing of a distribution.\n\n![Shifted distribution](/images/incremental_quantile/incremental_quantile_27.png)\n\nThe topic of change detection warrants its own set of posts.\nIt is for example possible to build one on top of a quantile estimator.\nAnything that can be used as a reference for what the distribution used to look like.","src/content/blog/incremental-quantile-estimation.mdx","f39e792f71c8b33f","incremental-quantile-estimation.mdx","particle-filters",{"id":106,"data":108,"body":114,"filePath":115,"digest":116,"legacyId":117,"deferredRender":24},{"title":109,"description":110,"pubDate":111,"tags":112},"Particle filter as Bayesian filtering via sampling","Signal processing vibes",["Date","2025-09-08T00:00:00.000Z"],[79,113],"signal-processing","Particle filtering got *way* more intuitive to me when I found out it is Bayesian filtering via sampling... and how it is coded.\nFiltering allows us to estimate the true state of a system given noisy observations and a model of how the system operates.\nThis is the type of signal processing vibes that appreciate.\n\nParticle filters are a sibling to Kalman filters, which work with Gaussian distributions and linear models.\nKalman filters are incredibly powerful in their own right and it feels like an internal joke that no data scientists knows how to implement one.\nTo be fair, closed-form solutions are neat (and efficient) but it takes time to meditate on their inteterpretation in code.\nParticle filters are what you bring out when distributions are non-Gaussian and the models non-linear.\nThey trade computation for flexibility, something we can usually afford in modern systems.\n\n## Store supply example\n\nImage that you are have stock $S_t$ at time $t$.\n\nFirst of all, taking inventory takes time so you eyeball the quantity $X_t = S_t + Z_t$, where $Z_t$ is a noise distribution.\nFor this example we can say that $Z_t \\sim N(0, 5)$, meaning that you have an $\\approx \\pm 10$ uncertainty band (95% confidence interval) when taking inventory.\nThis is the **only** measurement you can make in this example.\n\nNow, we are operating a store and stock leaves the store every single day $\\Delta S_t$.\nFor this example we can assume that we are working with whole number stock and the sales follow a Poisson distribution $\\Delta S_t \\sim \\text{Poisson}(\\lambda)$.\nAs $\\mathcal{E}(\\Delta S_t) = \\lambda$ we can set $\\lambda = 5$ to make the expected number of sales to be 5 a day.\n\nAlright, we would eventually run out of stock if we were not restocking.\nRestocking is rare enough to be drawn from a Bernoulli distribution where $p_t = g(X_t) = max(0, 1 - \\frac{X_t}{kr})$,\nwhere $r$ is the restock order size. To make it simple we know this, we always order one truck, say $r = 35$.\n$k$ is a hidden parameter which we for this example set as $k = 2$, meaning that daily restock probability $p_t = 50\\%$ \nwhen we have observed one weeks worth of stock.\n\n### Filtering goals\n\nGiven that we can only observe $X_t$, what is $S_t$ likely to be?\n\nGiven that we do not know $\\lambda$ or how $p_t$ works, can we estimate them?\n\n### Particle filter - Step 0 - Initialize\nThe $n_u$ particles $u_i$ will have three states $u_i = \\{s_i, \\lambda_i, p_i\\}$.\n\nWe initialize these with prior distributions $s_i \\sim U(0, 100)$, $\\lambda_i \\sim U(3, 10)$ and $p_i \\sim U(0.02, 0.2)$.\nFor our model of the system we will assume that $\\Delta S_t \\sim \\text{Poisson}(\\lambda)$ but we do not know the dynamics of $p_t = g(X_t)$,\ninstead we will assume that it can be fairly approximated using a static probability $\\hat{p}$.\n\nThe estimates for $\\lambda$ and $\\hat{p}$ are treated as latent variables because they are not observed directly.\n\n### Particle filter - Step 1 - Predict\n\nGiven that we observe $X_t$ we start by predicting how each particle $u_i$ evolves.\nThis is done by sampling demand $\\Delta s_i \\sim \\text{Poisson}(p_i)$ and a restock event as $v_i \\sim U(0,1)$.\nThe complete update is then\n$$\ns_i^{new} = \\begin{cases}\ns_i - \\Delta S_i + r,\\quad\\;\\text{if}\\: v_i > p_i,\\\\\ns_i - \\Delta S_i,\\quad\\quad\\quad\\text{if}\\: v_i \\leq p_i.\\\\\n\\end{cases}\n$$\n\n### Particle filter - Step 2 - Weights from likelihood\n\nThe next step of the particle filter is to calculate weight based on the likelihood of the particles' stock $s_i$.\nFor Gaussian noise the likelihood is proportional $\\mathcal{L}_i \\sim L_i = e^{-\\frac{1}{2}(\\frac{X_t - s_i}{\\sigma_{noise}})^2}$.\nThe weight showing relative likelihood then becomes $w_i = \\frac{L_i}{\\sum_i L_i}$.\nNote that these calculations requires us to say something about the error distribution, similar to a generalized linear model (GLM).\nThe error distribution could for example be exponential, Bernoulli, Poisson, Binomial depending on the nature of your problem.\n\n### Particle filter - Step 3 - Resample\n\nTo focus the particles on the more likely region we draw them with replacement with probability $w_i$.\n\n### Particle filter - Step 4 - Estimation\n\nFrom this cloud of particles $u_i$ we can estimate the mean, median or best weighted particle to get point estimates for $S_t$, $\\lambda$ and $\\hat{p}$.\nWe can also use the population statistics of the particles to calculate upper and lower quantile bands.\n\n## Code\n\n```python\nnoise_mu = 0\nnoise_sigma = 5\nsales_lam = 5\nweeks_of_stock = 2\nrestock_size = sales_lam * 7\n\nn_days = 40\ndt = 1\nN = int(np.ceil(n_days / dt))\nt = dt * np.arange(N)\nstarting_stock = weeks_of_stock * restock_size\n\nn_particles = 1000\n\nstock_prior = np.random.randint(0, 100, n_particles)\nsales_lam_prior = np.random.uniform(3, 10, n_particles)\nrestock_p_prior = np.random.uniform(0.02, 0.2, n_particles)\n\nparticles = np.zeros((n_particles, 3))\nSTOCK_COL = 0\nSALES_LAM_COL = 1\nRESTOCK_P_COL = 2\nparticles[:,STOCK_COL] = stock_prior\nparticles[:,SALES_LAM_COL] = sales_lam_prior\nparticles[:,RESTOCK_P_COL] = restock_p_prior\n\nstock = 1.0*starting_stock\nground_truth = np.zeros(N)\nground_truth[0] = stock\nobservations = np.zeros(N)\n\nestimated_stocks = np.zeros(N)\nestimated_stocks_ci = np.zeros((N,2))\nestimated_sales_lams = np.zeros(N)\nestimated_sales_lams_ci = np.zeros((N,2))\nestimated_restock_ps = np.zeros(N)\nestimated_restock_ps_ci = np.zeros((N,2))\n\nnp.random.seed(5)\ntrue_restocks = 0\nfor i in range(N):\n    t_sim = i*dt # not used\n\n    # System evolution\n    demand = np.random.poisson(sales_lam)\n    stock = max(stock - demand, 0)\n    internally_observed_stock = stock + (noise_mu + noise_sigma*np.random.randn())\n    restock_p = max(0, 1 - internally_observed_stock / (weeks_of_stock * restock_size))\n    if restock_p > np.random.rand():\n        true_restocks += 1\n        stock += restock_size\n    observed_stock = stock + (noise_mu + noise_sigma*np.random.randn())\n    ground_truth[i] = stock\n    observations[i] = observed_stock\n\n    # Particle\n    # 1. Prediction\n    demand = np.random.poisson(particles[:,SALES_LAM_COL])\n    particles[:, STOCK_COL] = np.maximum(particles[:, STOCK_COL] - demand, 0)\n    predicted_restock = particles[:,RESTOCK_P_COL] > np.random.rand(n_particles)\n    particles[:, STOCK_COL] += restock_size*predicted_restock\n\n    # 2. Update likelihood\n    likelihood = np.exp(-1/2 * ((observed_stock - particles[:, STOCK_COL]) / noise_sigma)**2)\n    weights = likelihood / np.sum(likelihood)\n\n    # 3. Resample\n    resample_indices = np.random.choice(range(n_particles), size=n_particles, p=weights)\n    particles = particles[resample_indices]\n\n    # 4. Estimates\n    alpha = 0.05\n    estimated_stocks[i] = np.mean(particles[:,STOCK_COL])\n    estimated_stocks_ci[i,0] = np.percentile(particles[:,STOCK_COL], alpha/2)\n    estimated_stocks_ci[i,1] = np.percentile(particles[:,STOCK_COL], 100-alpha/2)\n    estimated_sales_lams[i] = np.mean(particles[:,SALES_LAM_COL])\n    estimated_sales_lams_ci[i,0] = np.percentile(particles[:,SALES_LAM_COL], alpha/2)\n    estimated_sales_lams_ci[i,1] = np.percentile(particles[:,SALES_LAM_COL], 100-alpha/2)\n    estimated_restock_ps[i] = np.mean(particles[:,RESTOCK_P_COL])\n    estimated_restock_ps_ci[i,0] = np.percentile(particles[:,RESTOCK_P_COL], alpha/2)\n    estimated_restock_ps_ci[i,1] = np.percentile(particles[:,RESTOCK_P_COL], 100-alpha/2)\n```\n\nRunning this code yields the following example:\n\n![...](/images/particles/particles_1.png)\n\nThe average of $s_i$ tracks the ground truth pretty well.\n\nThe parameter estimations are a bit stiffer but their confidence bands are converging over time:\n\n![...](/images/particles/particles_2.png)\n\n![...](/images/particles/particles_3.png)\n\nOne thing to note that our code does not allow the particle latent variables to take on new values.\nThis is entirely possible by adding some random noise, interpolations between multiple particle or throwing in a genetic element.\nWith real data there is never going to be stable parameters, there we usually settle for long-term averages (which $\\hat{p}$ is) or some seasonality.\nWe did not add it here but it would make sense that there is a weekly, monthly and/or yearly seasonality to demand.\n\nUsing particle filters one can fit, for example, stochastic differential equations.","src/content/blog/particle-filters.mdx","86d91d3c666052d3","particle-filters.mdx","nonlinear-linear-regression",{"id":118,"data":120,"body":126,"filePath":127,"digest":128,"legacyId":129,"deferredRender":24},{"title":121,"description":122,"pubDate":123,"tags":124},"Nonlinear linear regression","Extending whitebox modeling",["Date","2025-07-31T00:00:00.000Z"],[18,125],"machine-learning","import ImageGallery from '../../components/ImageGallery.astro';\n\nConsider linear regression $y(\\textbf{x}) = \\sum_{i = 0}^{m} (w_i x_i) + b$ \nwhere $y \\in \\rm I\\!R$ is the output, \n$\\textbf{x} \\in \\rm I\\!R^m$ the input with $m$ features,\n$w_i$ is the weight corresponding to the $i$th feature and $b$ is the intercept.\nThe neat thing about this model is that it has a closed form maximum likelihood solution.\n\nThere is also the extention to the generalized linear model where $E(y|\\textbf{x}) = \\mu = g^{-1}(\\textbf{x}\\beta)$,\nthe way to read this is that $y$ is sampled from some distribution $y \\sim Y$ (does not have to be Gaussian) and it's\nexpected value $\\mu$ depends on some link function $g$ of a linear combinations of the inputs $\\textbf{x}\\beta$.\nThese types of models generally don't have closed forms and require iterative maximum likelihood or monte carlo methods.\n\nBut where we are going we need yet another extension to generalized additive models where $g(E(y|\\textbf{x})) = \\sum_{i = 0}^{m} f_i(x_i) + b$.\nNow instead of using linear weights we allow for any smooth transformation $f_i$ of the input signals $x_i$.\nIf one has $g$ as the identity function then this is simply called additative models $y = \\sum_{i = 0}^{m} f_i(x_i) + b$.\n\nA few years ago I implemented additative models myself, only to discover that someone already proposed it in 1981.\nThat's machine learning for you. While researching this topic I also found this [article of comparative setups of\nlinear models, generalized linear models, additative models and generalized additative models](https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf).\nBasically, these models will never beat your average black box model (the article compares to Random Forests TM).\nBut they do offer some unique interpretability.\n\nMy implementation was in Pytorch and used 1 layer multilayer perceptrons instead of splines or trees.\nThe model fitting was not done using backfitting but rather gradient descent.\n\n```python\ndef create_weights(n_in, n_out):\n    scale = ((6 / (n_in + n_out))**0.5)\n    return scale*2*(torch.rand(n_in, n_out)-0.5)\n\nclass univariate_nllr_model(nn.Module):\n    def __init__(\n            self, \n            n_input, \n            n_hidden = 10, \n            activation_function = nn.Tanh(),\n            loss_function = nn.MSELoss()\n        ):\n        super().__init__()\n        self.hidden_nodes_weights = nn.Parameter(create_weights(n_input, n_hidden))\n        self.hidden_nodes_bias = nn.Parameter(create_weights(n_input, n_hidden))\n        self.aggregration_weights = nn.Parameter(create_weights(n_input, n_hidden))\n        self.linear = nn.Linear(n_input, 1)\n        self.activation_function = activation_function\n        self.loss_function = loss_function\n        self.double()\n        \n    def f(self, x):\n        hidden = self.activation_function(\n            x.unsqueeze(2) * self.hidden_nodes_weights + self.hidden_nodes_bias\n        )\n        return (hidden * self.aggregration_weights).sum(-1)\n\n    def forward(self, x):\n        return self.linear(self.f(x)).squeeze(1)\n    \n    def evaluate(self, x, y, l2_last_layer = 1e-3):\n        fx = self.f(x)\n        yp = self.linear(fx).squeeze(1)\n        loss = self.loss_function(y, yp)\n        if l2_last_layer:\n            loss += l2_last_layer * ((self.linear.weight**2).sum() + (self.linear.bias**2).sum())\n        return loss\n```\n\nFor a toy problem where $x_{1,2,3,4,5} \\sim N(0,1)$ and $y = \\frac{5}{1 + e^{-2x_1}} + 3e^{-x_2^2}$ one gets the following $w_i f_i(x_i)$:\n![...](/images/nonlinear_linear_regression/nllr_1.png)\n\nOf course these are nice smooth functions that fits the tanh activation function well. \n\nThis is what ReLU looks like ($n_{hidden} = 10$):\n![...](/images/nonlinear_linear_regression/nllr_2.png)\n\nRegardless, playing around with the MLP parameters (layer size, depth, activation functions, regularization) will determine the ability of the $f_i(x_i)$ transformation.\nOne can also regularize $f_i(x_i)$ to make sure that uninteresting features (2, 3, 4 in this example) becomes close to zero.\n\nIf we take a real dataset, like the concrete compressive strength dataset [available here](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength).\nThe goal is to predict the strength as measured in MPa using different blends of concrete. \nUsing 32 hidden neurons in 1 layer to learn the nonlinearities yields for different activation functions:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_3.png\",\n      alt: \"...\",\n      caption: \"Tanh activation function, training MSE\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_4.png\",\n      alt: \"...\",\n      caption: \"Tanh activation function, learned nonlinearities\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_5.png\",\n      alt: \"...\",\n      caption: \"ReLU activation function, training MSE\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_6.png\",\n      alt: \"...\",\n      caption: \"ReLU activation function, learned nonlinearities\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_7.png\",\n      alt: \"...\",\n      caption: \"Sin activation function, training MSE\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_8.png\",\n      alt: \"...\",\n      caption: \"Sin activation function, learned nonlinearities\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_9.png\",\n      alt: \"...\",\n      caption: \"Binary step activation function, training MSE\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_10.png\",\n      alt: \"...\",\n      caption: \"Binary step activation function, learned nonlinearities\"\n    },\n  ]}\n  layout=\"carousel\"\n/>\n\nFirst of all, one would typically not use sine or binary step as activation functions in neural nets.\nThey do yield some interesting observations. The sine seems to overfit and come up with some interleaving effects. \nThe binary finds almost nothing but two effects.\n\n| Activation | MSE | No effect fields | Comment |\n| -------- | ------- |  ------- | ------- |\n| tanh | 53.3 | Coarse aggregate, fine aggregate, water, fly ash | Increasing cement generally makes it stronger. Superplasticizer and age both have a tapering effect. Slag has a minor effect. |\n| ReLU | 48.6 | - | Same cement, age and slag observations from tanh. Excessive water has a negative effect. Superplasticizer appears to even have a negative effect after its initial usefulness. |\n| sin | 31.2 | - | Age, superplasticizer and water behave the same as ReLU. Cement is a shared observation but aggregates are slight down in excessive amounts. Slag also seems to have no effect on the whole.|\n| Binary step | 159.2 | All except age and superplasticizer | When we remove nuanced expressions from the model and study on-off effects one only captures effects from superplasticizer and age. |\n\nThe linear(ish) observations seem to be:\n* Higher cement content means a stronger concrete.\n* Avoid excessive superplasticizer and water.\n* 30 to 90 days to cure.\n\nSurely there are cross-effects and trade-offs that one does when balancing a recipe for concrete.\nFor example more cement does require more water and superplasticizer is make the concrete workable for higher cement content in the absence of water.\nThis type of model does not answer that unless we add those factors explicitly.\nBut if where are truly interested in learning more about concrete then there are better models.\n\nComing back to the topic of nonlinear linear models.\nThe downside of my approach is that there are no confidence bounds...\nThat said, as I was writing this, then I looked closer at the article from before and found that they do theirs using bootstrapping.\nSo then it does not really count.\n\nHowever, when working with generalized linear models one can work with proper distributions in a Bayesian fashion.\nWe do lose that feature nonlinearity but instead we gain how the linear coefficients distribute.\n\nOf course, another way to approach these problem is to take a truly black-box model such as a neural network or boosted trees,\nand try to figure out which patterns in the data it reacts to.\nThat is where one would turn to sensitivity analysis.","src/content/blog/nonlinear-linear-regression.mdx","54b5deed64dfe8fa","nonlinear-linear-regression.mdx","supply-chain-modeling",{"id":130,"data":132,"body":140,"filePath":141,"digest":142,"legacyId":143,"deferredRender":24},{"title":133,"description":134,"pubDate":135,"tags":136},"Building a supply chain model","Using Python MIP and Polars",["Date","2025-09-02T00:00:00.000Z"],[137,138,139],"linear-programming","supply-chain","polars","In my current role I have worked with models of supply chains.\nThese models tend to be built using mathematical programming, in particular linear programming (LP) or mixed integer LP (MILP).\nOne typically starts off with a graph and later translate it into a program to solve.\nHere is an example of how to do it:\n\n* Raw materials, *R* - Gets produced or procured in source nodes.\n* Transportation edge, *g* - Moves raw materials into inventories or into industries, at a cost.\n* Inventory, *I* - Nodes that intermediately store raw materials or finished goods.\n* Industry, *P* - Nodes that transforms raw material into finished goods using recipes.\n* Customer or market, *M* - Sink nodes that absorbs finished goods.\n\nThere are many practical planning questions that stem from these five concepts.\n* Which raw materials should one procure and when? \n* How does transport capacity does one need?\n* What is the optimal transporation setup?\n* How should one utilize inventories to balance raw material or product flows?\n* Which recipes for production best utilize the raw material and logistics conditions?\n* How should customer or market demand be met cost-optimally?\n\nWhen creating a model to understand these questions one must understand the answer is only ever going to be as good as your data.\nOften one wants to not only understand how the business has been performing but how one should act in the future.\nThat requires one to have qualified guesses for that the cost of raw materials, transportation, inventory and production will be.\nNot only that, the hardest thing is usually to tell what the market wants even just a few months in advance.\n\nAnother thing to understand is that these models get **big** quick. \nIf you have 10 products to produce from 10 different variations of raw materials, transported in 10 different routes then there are 1000 combinations.\nOne usually have to be smart about model building in order to reduce the number of options and at the same time, keeping model output within predicable bounds.\nAn example of this is if you have a raw material plan. \nShould you let the model pick how much to produce itself with the plan as the ceiling?\nOr should you maybe just force the model to stick to the plan and work with imports and inventories to manage what is missing?\n\nYou pick the resolution to fit the problem.\nDo you need all the variants of raw materials or do you just need the big family of materials?\nThis of course depends who you are building the model for in your organisation.\n\n## Toy example\n\nI created a toy example here using [Polars](https://docs.pola.rs/api/python/dev/reference/index.html) for data management and [Python-MIP](https://docs.python-mip.com/en/latest/quickstart.html) for optimization.\n\n```python\nimport polars as pl\nimport mip\n```\n\nWe then hardcode some data for 3 months, 3 types of raw materials, 2 products using 3 different recipes:\n\n\u003Cdetails>\n  \u003Csummary>Expand to see dummy data\u003C/summary>\n\n\u003Cdiv style={{ fontSize: \"0.7rem\" }}>\n```python\ndf_raw_materials = pl.DataFrame([\n    {\"name\":\"R1\", \"product\":\"raw_material_1\", \"cost_per_unit\":20.4, \"t\":1, \"max_production\":1000},\n    {\"name\":\"R1\", \"product\":\"raw_material_1\", \"cost_per_unit\":20.4, \"t\":2, \"max_production\":900},\n    {\"name\":\"R1\", \"product\":\"raw_material_1\", \"cost_per_unit\":20.4, \"t\":3, \"max_production\":1100},\n    {\"name\":\"R1\", \"product\":\"raw_material_2\", \"cost_per_unit\":16.9, \"t\":1, \"max_production\":400},\n    {\"name\":\"R1\", \"product\":\"raw_material_2\", \"cost_per_unit\":16.9, \"t\":2, \"max_production\":400},\n    {\"name\":\"R1\", \"product\":\"raw_material_2\", \"cost_per_unit\":16.9, \"t\":3, \"max_production\":400},\n    {\"name\":\"R2\", \"product\":\"raw_material_1\", \"cost_per_unit\":22.1, \"t\":1, \"max_production\":1000},\n    {\"name\":\"R2\", \"product\":\"raw_material_1\", \"cost_per_unit\":22.1, \"t\":2, \"max_production\":900},\n    {\"name\":\"R2\", \"product\":\"raw_material_1\", \"cost_per_unit\":22.1, \"t\":3, \"max_production\":1100},\n    {\"name\":\"R2\", \"product\":\"raw_material_2\", \"cost_per_unit\":17.2, \"t\":1, \"max_production\":400},\n    {\"name\":\"R2\", \"product\":\"raw_material_2\", \"cost_per_unit\":17.2, \"t\":2, \"max_production\":400},\n    {\"name\":\"R2\", \"product\":\"raw_material_2\", \"cost_per_unit\":17.2, \"t\":3, \"max_production\":400},\n    {\"name\":\"R2\", \"product\":\"raw_material_3\", \"cost_per_unit\":38.2, \"t\":1, \"max_production\":220},\n    {\"name\":\"R2\", \"product\":\"raw_material_3\", \"cost_per_unit\":38.2, \"t\":2, \"max_production\":220},\n    {\"name\":\"R2\", \"product\":\"raw_material_3\", \"cost_per_unit\":38.2, \"t\":3, \"max_production\":220},\n])\n\ndf_inventories = pl.DataFrame([\n    {\"name\":\"I1\", \"product\":\"raw_material_1\", \"cost_per_unit\":2.5, \"max_inventory\":20000, \"min_inventory\":0, \"initial_inventory\":0},\n    {\"name\":\"I1\", \"product\":\"raw_material_2\", \"cost_per_unit\":2.5, \"max_inventory\":20000, \"min_inventory\":0, \"initial_inventory\":0},\n    {\"name\":\"I2\", \"product\":\"raw_material_1\", \"cost_per_unit\":3.1, \"max_inventory\":20000, \"min_inventory\":0, \"initial_inventory\":0},\n    {\"name\":\"I2\", \"product\":\"raw_material_2\", \"cost_per_unit\":3.1, \"max_inventory\":20000, \"min_inventory\":0, \"initial_inventory\":0},\n    {\"name\":\"I2\", \"product\":\"raw_material_3\", \"cost_per_unit\":3.1, \"max_inventory\":20000, \"min_inventory\":0, \"initial_inventory\":0},\n])\n\ndf_industry_recipe = pl.DataFrame([\n    {\"name\":\"P1\", \"recipe\":1, \"product\": \"product_1\", \"consumed\":\"raw_material_1\", \"quantity\": 1},\n    {\"name\":\"P1\", \"recipe\":1, \"product\": \"product_1\", \"consumed\":\"raw_material_2\", \"quantity\": 0.8},\n    {\"name\":\"P1\", \"recipe\":2, \"product\": \"product_1\", \"consumed\":\"raw_material_1\", \"quantity\": 0.9},\n    {\"name\":\"P1\", \"recipe\":2, \"product\": \"product_1\", \"consumed\":\"raw_material_2\", \"quantity\": 0.7},\n    {\"name\":\"P1\", \"recipe\":2, \"product\": \"product_1\", \"consumed\":\"raw_material_3\", \"quantity\": 0.2},\n    {\"name\":\"P1\", \"recipe\":3, \"product\": \"product_2\", \"consumed\":\"raw_material_1\", \"quantity\": 0.9},\n    {\"name\":\"P1\", \"recipe\":3, \"product\": \"product_2\", \"consumed\":\"raw_material_2\", \"quantity\": 0.5},\n    {\"name\":\"P1\", \"recipe\":3, \"product\": \"product_2\", \"consumed\":\"raw_material_3\", \"quantity\": 0.4},\n])\n\ndf_industry_production = pl.DataFrame([\n    {\"name\":\"P1\", \"t\":1, \"production_capacity\":10000},\n    {\"name\":\"P1\", \"t\":2, \"production_capacity\":10000},\n    {\"name\":\"P1\", \"t\":3, \"production_capacity\":10000},\n])\n\ndf_market = pl.DataFrame([\n    {\"name\":\"M1\", \"product\": \"product_1\", \"t\":1, \"price\":100, \"demand\":1e9},\n    {\"name\":\"M1\", \"product\": \"product_1\", \"t\":2, \"price\":100, \"demand\":1e9},\n    {\"name\":\"M1\", \"product\": \"product_1\", \"t\":3, \"price\":100, \"demand\":1e9},\n    {\"name\":\"M1\", \"product\": \"product_2\", \"t\":1, \"price\":85, \"demand\":1e9},\n    {\"name\":\"M1\", \"product\": \"product_2\", \"t\":2, \"price\":85, \"demand\":1e9},\n    {\"name\":\"M1\", \"product\": \"product_2\", \"t\":3, \"price\":85, \"demand\":1e9},\n])\n\ndf_transport = pl.DataFrame([\n    {\"name\":\"gR1I1\", \"from_node\":\"R1\", \"to_node\":\"I1\", \"cost_per_unit\":10.0, \"max_capacity\": 10000, \"min_capacity\":0},\n    {\"name\":\"gR1I2\", \"from_node\":\"R1\", \"to_node\":\"I2\", \"cost_per_unit\":13.0, \"max_capacity\": 10000, \"min_capacity\":0},\n    {\"name\":\"gR2I1\", \"from_node\":\"R2\", \"to_node\":\"I1\", \"cost_per_unit\": 7.0, \"max_capacity\": 10000, \"min_capacity\":0},\n    {\"name\":\"gR2I2\", \"from_node\":\"R2\", \"to_node\":\"I2\", \"cost_per_unit\":12.0, \"max_capacity\": 10000, \"min_capacity\":0},\n    {\"name\":\"gI1P1\", \"from_node\":\"I1\", \"to_node\":\"P1\", \"cost_per_unit\": 9.0, \"max_capacity\": 10000, \"min_capacity\":0},\n    {\"name\":\"gI2P1\", \"from_node\":\"I2\", \"to_node\":\"P1\", \"cost_per_unit\": 6.0, \"max_capacity\": 10000, \"min_capacity\":0},\n    {\"name\":\"gR1P1\", \"from_node\":\"R1\", \"to_node\":\"P1\", \"cost_per_unit\":15.0, \"max_capacity\": 10000, \"min_capacity\":0},\n    {\"name\":\"gR2P1\", \"from_node\":\"R2\", \"to_node\":\"P1\", \"cost_per_unit\":17.0, \"max_capacity\": 10000, \"min_capacity\":0},\n    {\"name\":\"gP1M1\", \"from_node\":\"P1\", \"to_node\":\"M1\", \"cost_per_unit\": 0.0, \"max_capacity\":  1e12, \"min_capacity\":0},\n])\n```\n\u003C/div>\n\u003C/details>\n\nAfter that we can start with model building.\nWe need to create variables, constraints and an objective.\nBecause we want to maximize profits in this example we want a maximization model.\nWe will add the constraints as either $=$ (equal) or $\\leq$ (less than equal) between a left and right hand side.\nThe objective is initialized to 0.\n\n```python\nmodel = mip.Model(sense = mip.MAXIMIZE)\n\nobjective = 0\nconstraint_leq_lhs = []\nconstraint_leq_rhs = []\nconstraint_eq_lhs = []\nconstraint_eq_rhs = []\n```\n\nTo create variables we define a helper function for Polars.\nThis is mainly because Polars is a little icky with arbitrary types, so they have to be treated with tongs.\nBasically Polars will treat them as Python objects in a non-vectorized way (which it warns you is slow).\nThat is no real issue as we are mainly interested in Polars API, which I prefer over Pandas.\n\nThe first part of `create_variables` parses if the lower and upper bound are columns,\notherwise they are assumed to be numbers or something like `mip.INF`.\nThen on the dataframe of interest we join a variable dataframe.\nThe variable dataframe is a dataframe of unique combinations of the dimension columns.\nThe variables are created using `model.add_var(...)`, which requires a UDF-friendly Polars function `.map_elements(...)` to run.\nSince we require multiple columns in the naming process we must define a struct (the default is that Polars operate on columns not on rows).\nIn order to not make Polars freak out at this new column we specify the data types to be `pl.Object` which is the catch-all.\n\n```python\ndef create_variables(model, df, dim_columns, lb, ub, name_prefix = \"\"):\n    lb_is_str = False\n    ub_is_str = False\n    all_input_columns = dim_columns.copy()\n    if isinstance(lb, str):\n        all_input_columns += [lb]\n        lb_is_str = True\n    if isinstance(ub, str):\n        all_input_columns += [ub]\n        ub_is_str = True\n    return df.join(\n            df[all_input_columns]\n            .unique()\n            .with_columns(\n                pl.struct(all_input_columns)\n                .map_elements(\n                    lambda row: \n                        model.add_var(\n                            name=name_prefix+\"-\".join([str(row[col]) for col in dim_columns]),\n                            lb = lb if not lb_is_str else row[lb],\n                            ub = ub if not ub_is_str else row[ub],\n                        ),\n                    return_dtype=pl.Object\n                )\n                .alias(VAR)\n            )[dim_columns + [VAR]],\n            on = dim_columns,\n            how = \"left\"\n        )\n```\n\n\u003Cdetails>\n  \u003Csummary>Expand to see how the variables are created\u003C/summary>\n\n```python\nNAME = \"name\"\nPRODUCT = \"product\"\nTIMESTEP = \"t\"\nFROM_NODE = \"from_node\"\nTO_NODE = \"to_node\"\nVAR = \"variable\"\nCONSUMED = \"consumed\"\nRECIPE = \"recipe\"\n\ntimesteps = pl.DataFrame({TIMESTEP:[1,2,3]})\n\nv_raw_material_production = create_variables(\n    model,\n    df_raw_materials, \n    [TIMESTEP, NAME, PRODUCT], \n    lb = 0, \n    ub = \"max_production\"\n)\n\nv_raw_material_inventory = create_variables(\n    model,\n    df_inventories\n    .join(timesteps, how=\"cross\"), \n    [TIMESTEP, NAME, PRODUCT], \n    lb = \"min_inventory\", \n    ub = \"max_inventory\"\n)\n\nv_production_per_recipe = create_variables(\n    model,\n    df_industry_recipe\n    .join(timesteps, how=\"cross\"),\n    [TIMESTEP, RECIPE], \n    name_prefix=\"recipe_\",\n    lb = 0, \n    ub = mip.INF\n)\n\nproducts_transport_to = pl.concat([\n    df_inventories[[NAME, PRODUCT]],\n    df_industry_recipe[[NAME, CONSUMED]].rename({CONSUMED:PRODUCT}),\n    df_market[[NAME, PRODUCT]]\n]).unique().rename({NAME:TO_NODE}).sort([TO_NODE, PRODUCT])\n\nv_transport = create_variables(\n    model,\n    df_transport\n    .join(timesteps, how=\"cross\")\n    .join(products_transport_to, on = TO_NODE, how = \"left\"),\n    [TIMESTEP, FROM_NODE, TO_NODE, PRODUCT], \n    lb = \"min_capacity\", \n    ub = \"max_capacity\"\n)\n```\n\u003C/details>\n\nAfter this we can start defining constraints, which is the bulk of our work.\nIn order to do it more compactly we define two more helper functions `sum_variables` and `custom_elementwize_series`.\nBoth of them are essentially just macros for applying `mip.xsum` is less and more elaborate ways.\n\n\n\u003Cdetails>\n  \u003Csummary>Expand to see how the constraints are created\u003C/summary>\n\n```python\nDEMAND = \"demand\"\nFLOW = \"flow\"\nLHS = \"left_hand_side\"\nRHS = \"right_hand_side\"\nTSHIFT = \"_tshifted\"\nSCALED = \"_scaled\"\n\nc_raw_material_transport = (\n    v_transport\n    .group_by([TIMESTEP, FROM_NODE, PRODUCT])\n    .agg(sum_variables(output_name=LHS))\n    .rename({FROM_NODE:NAME})\n    .join(v_raw_material_production, on = [TIMESTEP, NAME, PRODUCT])\n    .rename({VAR:RHS})\n)\n\nc_raw_material_inventory_io = (\n    v_transport\n    .filter(pl.col(TO_NODE).str.starts_with(\"I\"))\n    .drop(NAME).rename({TO_NODE:NAME})\n    .group_by(TIMESTEP, NAME, PRODUCT)\n    .agg(sum_variables(output_name=\"incoming\"))\n    .join(\n        v_transport\n        .filter(pl.col(FROM_NODE).str.starts_with(\"I\"))\n        .drop(NAME).rename({FROM_NODE:NAME})\n        .group_by(TIMESTEP, NAME, PRODUCT)\n        .agg(sum_variables(output_name=\"outgoing\")),\n        on = [TIMESTEP, NAME, PRODUCT],\n        how = \"left\"\n    )\n    .with_columns(\n        custom_elementwize_series(\n            output_name=LHS, \n            ordered_input=[\"incoming\", \"outgoing\"], \n            ordered_input_function=lambda i, o: mip.xsum([i, -o])\n        )\n    )\n    .join(\n        v_raw_material_inventory.with_columns(pl.col(VAR))\n        .join(\n            v_raw_material_inventory.with_columns(pl.col(TIMESTEP)+1),\n            on = [TIMESTEP, NAME, PRODUCT],\n            how = \"left\",\n            suffix = TSHIFT\n        )\n        .with_columns(\n            custom_elementwize_series(\n                output_name=RHS, \n                ordered_input=[VAR, VAR + TSHIFT], \n                ordered_input_function=lambda t, tn: (\n                    mip.xsum([t, -tn]) if (t is not None and tn is not None) else 0\n                )\n            )\n        ),\n        on = [TIMESTEP, NAME, PRODUCT],\n        how = \"left\"\n    )\n)\n\nc_initial_inventory = (\n    v_raw_material_inventory\n    .filter(pl.col(TIMESTEP) == 1)\n    .rename({VAR:LHS, \"initial_inventory\":RHS})\n)\n\nc_raw_material_consumption = (\n    v_production_per_recipe\n    .with_columns(\n        custom_elementwize_series(\n            output_name=VAR+SCALED,\n            ordered_input=[VAR, \"quantity\"],\n            ordered_input_function=lambda v, w: w * v\n        )\n    )\n    .group_by(TIMESTEP, NAME, CONSUMED)\n    .agg(sum_variables(output_name=LHS, variable_name=VAR+SCALED))\n    .join(\n        v_transport\n        .drop(NAME).rename({TO_NODE:NAME, PRODUCT:CONSUMED})\n        .group_by(TIMESTEP, NAME, CONSUMED)\n        .agg(sum_variables(RHS)),\n        on = [TIMESTEP, NAME, CONSUMED],\n        how = \"left\"\n    )\n)\n\nc_product_flow = (\n    v_production_per_recipe\n    .select([TIMESTEP, NAME, PRODUCT, RECIPE, VAR]).unique()\n    .group_by([TIMESTEP, NAME, PRODUCT])\n    .agg(sum_variables(LHS))\n    .join(\n        v_transport\n        .drop(NAME).rename({FROM_NODE:NAME})\n        .group_by([TIMESTEP, NAME, PRODUCT])\n        .agg(sum_variables(RHS)),\n        on = [TIMESTEP, NAME, PRODUCT],\n        how = \"left\"\n    )\n)\n\nc_production_capacity = (\n    v_production_per_recipe\n    .select([TIMESTEP, NAME, RECIPE, VAR]).unique()\n    .group_by([TIMESTEP, NAME])\n    .agg(sum_variables(LHS))\n    .join(\n        df_industry_production.rename({\"production_capacity\":RHS}),\n        on = [TIMESTEP, NAME],\n        how = \"left\"\n    )\n)\n\nc_market_demand = (\n    df_market\n    .rename({DEMAND:RHS})\n    .join(\n        v_transport\n        .drop(NAME).rename({TO_NODE:NAME})\n        .group_by([TIMESTEP, NAME, PRODUCT])\n        .agg(sum_variables(LHS)),\n        on = [TIMESTEP, NAME, PRODUCT],\n        how = \"left\"\n    )\n)\n```\n\u003C/details>\n\nThe first constraint **what gets produced must be transported away** is probably the simplest while\nthe second **do not invent inventory** is probably the most complex.\nThe first is basically saying that $produced = transported$\nThe second is that $incoming - outgoing = current - previous$, that is, \nif we are building inventory then we must have more incoming than outgoing.\n\nOnce we have all constraint LHS and RHS defined, we can add them to the model:\n```python\nconstraint_eq_lhs += c_raw_material_transport[LHS].to_list()\nconstraint_eq_rhs += c_raw_material_transport[RHS].to_list()\nconstraint_eq_lhs += c_raw_material_inventory_io[LHS].to_list()\nconstraint_eq_rhs += c_raw_material_inventory_io[RHS].to_list()\nconstraint_eq_lhs += c_initial_inventory[LHS].to_list()\nconstraint_eq_rhs += c_initial_inventory[RHS].to_list()\nconstraint_eq_lhs += c_raw_material_consumption[LHS].to_list()\nconstraint_eq_rhs += c_raw_material_consumption[RHS].to_list()\nconstraint_eq_lhs += c_product_flow[LHS].to_list()\nconstraint_eq_rhs += c_product_flow[RHS].to_list()\nconstraint_leq_lhs += c_production_capacity[LHS].to_list()\nconstraint_leq_rhs += c_production_capacity[RHS].to_list()\nif market_demand_must_be_met:\n    constraint_eq_lhs += c_market_demand[LHS].to_list()\n    constraint_eq_rhs += c_market_demand[RHS].to_list()\nelse:\n    constraint_leq_lhs += c_market_demand[LHS].to_list()\n    constraint_leq_rhs += c_market_demand[RHS].to_list()\n\nfor lhs, rhs in zip(constraint_eq_lhs, constraint_eq_rhs):\n    model += lhs == rhs, str(lhs) + \" == \" + str(rhs)\n\nfor lhs, rhs in zip(constraint_leq_lhs, constraint_leq_rhs):\n    model += lhs \u003C= rhs, str(lhs) + \" \u003C= \" + str(rhs)\n\n```\n\nNote that we made `market_demand_must_be_met` conditional.\nThis is an example on how to make the logic easily configurable.\nWith a boolean one can define if demand must be met exactly or if it is an upper bound.\n\nThe last thing is to add an objective.\nThis is done by summarizing all the costs and profits associated with the variables.\nIt is all one great weighted sum:\n\n```python\nprint(objective)\n- 20.41-R1-raw_material_1 - 20.42-R1-raw_material_1 - 20.43-R1-raw_material_1 - 16.91-R1-raw_material_2 - 16.92-R1-raw_material_2 - 16.93-R1-raw_material_2 - 22.11-R2-raw_material_1 - 22.12-R2-raw_material_1 - 22.13-R2-raw_material_1 - 17.21-R2-raw_material_2 - 17.22-R2-raw_material_2 - 17.23-R2-raw_material_2 - 38.21-R2-raw_material_3 - 38.22-R2-raw_material_3 - 38.23-R2-raw_material_3 - 12.51-R1-I1-raw_material_1 - 12.51-R1-I1-raw_material_2 - 12.52-R1-I1-raw_material_1 - 12.52-R1-I1-raw_material_2 - 12.53-R1-I1-raw_material_1 - 12.53-R1-I1-raw_material_2 - 16.11-R1-I2-raw_material_1 - 16.11-R1-I2-raw_material_2 - 16.11-R1-I2-raw_material_3 - 16.12-R1-I2-raw_material_1 - 16.12-R1-I2-raw_material_2 - 16.12-R1-I2-raw_material_3 - 16.13-R1-I2-raw_material_1 - 16.13-R1-I2-raw_material_2 - 16.13-R1-I2-raw_material_3 - 9.51-R2-I1-raw_material_1 - 9.51-R2-I1-raw_material_2 - 9.52-R2-I1-raw_material_1 - 9.52-R2-I1-raw_material_2 - 9.53-R2-I1-raw_material_1 - 9.53-R2-I1-raw_material_2 - 15.11-R2-I2-raw_material_1 - 15.11-R2-I2-raw_material_2 - 15.11-R2-I2-raw_material_3 - 15.12-R2-I2-raw_material_1 - 15.12-R2-I2-raw_material_2 - 15.12-R2-I2-raw_material_3 - 15.13-R2-I2-raw_material_1 - 15.13-R2-I2-raw_material_2 - 15.13-R2-I2-raw_material_3 - 9.01-I1-P1-raw_material_1 - 9.01-I1-P1-raw_material_2 - 9.01-I1-P1-raw_material_3 - 9.02-I1-P1-raw_material_1 - 9.02-I1-P1-raw_material_2 - 9.02-I1-P1-raw_material_3 - 9.03-I1-P1-raw_material_1 - 9.03-I1-P1-raw_material_2 - 9.03-I1-P1-raw_material_3 - 6.01-I2-P1-raw_material_1 - 6.01-I2-P1-raw_material_2 - 6.01-I2-P1-raw_material_3 - 6.02-I2-P1-raw_material_1 - 6.02-I2-P1-raw_material_2 - 6.02-I2-P1-raw_material_3 - 6.03-I2-P1-raw_material_1 - 6.03-I2-P1-raw_material_2 - 6.03-I2-P1-raw_material_3 - 15.01-R1-P1-raw_material_1 - 15.01-R1-P1-raw_material_2 - 15.01-R1-P1-raw_material_3 - 15.02-R1-P1-raw_material_1 - 15.02-R1-P1-raw_material_2 - 15.02-R1-P1-raw_material_3 - 15.03-R1-P1-raw_material_1 - 15.03-R1-P1-raw_material_2 - 15.03-R1-P1-raw_material_3 - 17.01-R2-P1-raw_material_1 - 17.01-R2-P1-raw_material_2 - 17.01-R2-P1-raw_material_3 - 17.02-R2-P1-raw_material_1 - 17.02-R2-P1-raw_material_2 - 17.02-R2-P1-raw_material_3 - 17.03-R2-P1-raw_material_1 - 17.03-R2-P1-raw_material_2 - 17.03-R2-P1-raw_material_3 + 100.01-P1-M1-product_1 + 85.01-P1-M1-product_2 + 100.02-P1-M1-product_1 + 85.02-P1-M1-product_2 + 100.03-P1-M1-product_1 + 85.03-P1-M1-product_2 - 2.51-I1-raw_material_1 - 2.52-I1-raw_material_1 - 2.53-I1-raw_material_1 - 2.51-I1-raw_material_2 - 2.52-I1-raw_material_2 - 2.53-I1-raw_material_2 - 3.11-I2-raw_material_1 - 3.12-I2-raw_material_1 - 3.13-I2-raw_material_1 - 3.11-I2-raw_material_2 - 3.12-I2-raw_material_2 - 3.13-I2-raw_material_2 - 3.11-I2-raw_material_3 - 3.12-I2-raw_material_3 - 3.13-I2-raw_material_3 \n```\n\n\u003Cdetails>\n  \u003Csummary>Expand to see the objective definition\u003C/summary>\n\n```python\nobjective_cost_raw_material = mip.xsum(\n    v_raw_material_production\n     .with_columns(\n        custom_elementwize_series(\n            output_name=VAR+SCALED,\n            ordered_input=[VAR, \"cost_per_unit\"],\n            ordered_input_function=lambda v, w: w * v\n        )\n    )[VAR+SCALED]\n)\n\nobjective_cost_transport = mip.xsum(\n    v_transport\n     .with_columns(\n        custom_elementwize_series(\n            output_name=VAR+SCALED,\n            ordered_input=[VAR, \"cost_per_unit\"],\n            ordered_input_function=lambda v, w: w * v\n        )\n    )[VAR+SCALED]\n)\n\nobjective_cost_incoming_inventory = mip.xsum(\n    v_raw_material_inventory\n    .join(\n        v_transport\n        .drop(NAME).rename({TO_NODE:NAME})\n        .group_by([TIMESTEP, NAME, PRODUCT])\n        .agg(sum_variables(\"incoming\")),\n        on = [TIMESTEP, NAME, PRODUCT],\n        how = \"left\"\n    )\n    .with_columns(\n        custom_elementwize_series(\n            output_name=VAR+SCALED,\n            ordered_input=[\"incoming\", \"cost_per_unit\"],\n            ordered_input_function=lambda v, w: w * v\n        )\n    )[VAR+SCALED]\n)\n\nobjective_cost_old_inventory = mip.xsum(\n    v_raw_material_inventory\n    .with_columns(\n        custom_elementwize_series(\n            output_name=VAR+SCALED,\n            ordered_input=[VAR, \"cost_per_unit\"],\n            ordered_input_function=lambda v, w: w * v\n        )\n    )[VAR+SCALED]\n)\n\n\nobjective_gain_market = mip.xsum(\n    c_market_demand\n    .with_columns(\n        custom_elementwize_series(\n            output_name=VAR+SCALED,\n            ordered_input=[LHS, \"price\"],\n            ordered_input_function=lambda v, w: w * v\n        )\n    )[VAR+SCALED]\n)\n\nobjective += -objective_cost_raw_material\nobjective += -objective_cost_transport\nobjective += -objective_cost_incoming_inventory\nobjective += -objective_cost_old_inventory\nobjective += objective_gain_market\nmodel.objective = objective\n```\n\u003C/details>\n\nWith all this set up we can run the model as:\n\n```python\nmodel.max_gap = 0.05\nstatus = model.optimize(max_seconds=300)\n\nif status == mip.OptimizationStatus.OPTIMAL:\n    print(f'optimal solution cost {model.objective_value} found')\n\nelif status == mip.OptimizationStatus.FEASIBLE:\n    print(f'sol.cost {model.objective_value} found, best possible: {model.objective_bound}')\n\nelif status == mip.OptimizationStatus.NO_SOLUTION_FOUND:\n    print(f'no feasible solution found, lower bound is: {model.objective_bound}')\n\nif status == mip.OptimizationStatus.OPTIMAL or status == mip.OptimizationStatus.FEASIBLE:\n    print('solution:')\n    for v in model.vars:\n       if abs(v.x) > 1e-9: # only printing non-zeros\n          print(f'{v.name} : {v.x}')\n```\n\nIn our example it comes out as 153588 arbitrary currency units.\nThe model focuses all its power on product 2, recipe 3 and ships 1600 units a month.\nBy modifying the data one can easily create scenarios.\nFor example, what happens if we stop the production (`production_capacity = 0`) for $t = 2$?\n\nIn this scenario the model is forced to store the production in inventory I1.\nBecause the production capacity is not a limiting factor it can double the production in $t = 3$ to make up for total production,\nbut, the extra cost from storing and handling 1 month of 1480 units has reduced the objective to 142338 currency units.\n\nSince this is a linear program we can also extract shadow prices using:\n```python\ndf_constraint_shadow_prices = pl.DataFrame({\n    \"name\":[c.name for c in model.constrs], \n    \"shadow_price\":[c.pi for c in model.constrs]\n}).sort(\"shadow_price\")\n```\nThe highest shadow price for this example is the availability of raw material 2. \nIt has a shadow price of 77.42 from R1 and 74.42 from R2.\nIn contrast raw material 1 has 24.1 from R1 and 22.1 from R2. \nRaw material 3 has 0.0 meaning that it is not a limiting resource for additional profits.","src/content/blog/supply-chain-modeling.mdx","4d0378509a47e6dd","supply-chain-modeling.mdx","sensitivity-analysis",{"id":144,"data":146,"body":151,"filePath":152,"digest":153,"legacyId":154,"deferredRender":24},{"title":147,"description":148,"pubDate":149,"tags":150},"Sensitivity analysis & feature importance using gradients","Understanding SHAP and when to use it",["Date","2025-08-03T00:00:00.000Z"],[18,125],"import ImageGallery from '../../components/ImageGallery.astro';\n\nIn the post on nonlinear linear regression we mentioned the topic of sensitivity analysis.\nThis made me want to try a method that I have been thinking about but never tried.\nInstead of using SHAP (more on that later) or black-box probers, why not use forward differentiation?\nOf course, this is maily handy for models implemented in an automatic differentiation framework like Pytorch.\n\nWriters update: \n\u003Cbr>*Turns out that SHAP has a gradient-based explainer, we will compare and reason about it* :)\u003C/br>\n\nIf we implemented a simple, dense univarate neural network in Pytorch like this:\n\n```python\nfrom torch import nn\n\nclass univariate_dense_net(nn.Module):\n    def __init__(\n            self, \n            n_input, \n            n_hidden = [16, 16], \n            activation_function = nn.Tanh()\n        ):\n        super().__init__()\n        self.nodes_per_layer = [n_input] + n_hidden + [1]\n        self.layers = nn.ParameterList()\n        depth = len(n_hidden)\n        for i in range(depth+1):\n            self.layers.append(nn.Linear(self.nodes_per_layer[i], self.nodes_per_layer[i+1]))\n            if i \u003C depth:\n                self.layers.append(activation_function)\n        self.double()\n        \n    def forward(self, x):\n        y = x\n        for layer in self.layers:\n            y = layer(y)\n        return y\n```\n\nAnd throw a problem at it like this\n $x_{1,2,3,4,5,6,7} \\sim N(0,1)$ and $y = \\frac{5}{1 + e^{-2x_1}} + 3e^{-x_2^2} + x_3x_4$.\n\n![...](/images/sensitivity/sens_1.png)\n\nGiven that we store our $x_i$ in a matrix $X$ and targets in a column vector $y$. \nThen one can calculate the gradient of the model output by $\\pm 1$ unit with regards to $x_i$ using`torch.autograd.grad`for Pytorch.\nOne note is that one needs to copy the input data and make it require a gradient for this calculation to work.\n\n```python\nimport torch\nX_grad = X.clone().requires_grad_()\ngrad = torch.autograd.grad(\n    model.forward(X_grad), \n    X_grad, \n    torch.ones_like(y)\n)\n```\n\nWith this we have received an $n \\times m$ matrix where each element is $\\frac{\\partial{y}}{\\partial{x_i}}(k)$,\nwhere $k$ is our row index.\nUsing this derivative of the model at every observation, we can look at things like the average power per feature.\nThis hints at which of the signals the model things are strong contender for nudging the target.\n\n![...](/images/sensitivity/sens_2.png)\n\nIt is a simple feature importance but it has done the trick.\nNote that there was a cross-effect between $x_3x_4$, \nthe question is if that can be found when analyzing the gradients.\nBy doing principal component analysis (PCA) on the gradients we find four components.\nThe rest PC5, PC6, PC7 just capture noise.\n\n![...](/images/sensitivity/sens_3.png)\n\nAnd by looking at the weights of the components we find that PC1 belongs to $x2$, PC4 belongs to $x_1$ but PC2 and PC3 references both $x_3$ and $x_4$.\nThe first time (PC2) they both have positive weight and the second time (PC3) they positive-negative weights. \nPerhaps this hints at the underlying relationship.\nMaybe it does not specifically.\n\n![...](/images/sensitivity/sens_4.png)\n\nHere are the corresponding SHAP values using the official library:\n```python\nimport shap\nshap_values = shap.GradientExplainer(model, data).shap_values(eval_data)\n```\n\n![...](/images/sensitivity/sens_5.png)\n\nNote that the SHAP values seems to think that $x_1$ is more important than $x_2$, \nwhen the size of the gradients say the opposite.\nThe SHAP values are indeed correct when it comes to feature importance.\nThe MSE increases to 70 when $x_1$ is blocked out while it only increases to 30 for $x_2$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/sensitivity/sens_6.png\",\n      alt: \"...\",\n      caption: \"x1 excluded from training\"\n    },\n    {\n      src: \"/images/sensitivity/sens_7.png\",\n      alt: \"...\",\n      caption: \"x2 excluded from training\"\n    },\n  ]}\n  layout=\"carousel\"\n/>\n\nSo feature importance != sensitivity.\nThe gradients tell us something about how to probe the model to change the output.\nIn this case prodding $x_2$ has a better marginal effect than $x_1$.\nHowever, it does not (yet) tell us how removing this parameter would hurt the model.\nFor $x_5, x_6, x_7$ we have learned that lack of sensitivity can be interpreted as feature uninportance.\n\nTo my understanding the SHAP gradient explainer starts by sampling a subset of the data, called the baseline observations $x'$.\nTo calculate the SHAP value $\\phi_i$ for feature $i$ of observation $x_i[k]$ \none calculates the path integral of the gradient from $x_i[k]$ to each baseline $x_i'$\nand then averages over the baselines.\nThe points along the path are parametrized using $\\alpha$ as $x_{\\alpha}'[k] = x' - \\alpha (x[k] - x')$\n$$$\n\\phi_i[k] = (x_i[k] - x'_i)\\int_{\\alpha = 0}^{1} \\frac{\\partial f(x_{\\alpha}'[k])}{\\partial x_i} d\\alpha.\n$$$\n\nTo make life easy for our implementation we can approximate the integral $\\int_0^1 f(x) dx \\approx \\sum_{i=0}^h \\frac{1}{h}f(x+\\frac{i}{h})$.\nThis gives us our own homebrew SHAP values.\nOf course, it is not identical to the real SHAP library. They probably have a much nicer implementation.\n\n![...](/images/sensitivity/sens_8.png)\n\nSo how does this work? Well, basically we are interested in feature importance.\nThe simplest way to answer that question would be just run leave-one-out and document where model performance tanks.\nFor the SHAP values to become large, signaling feature importance, two things must happen:\n1. The path must have some variation in the feature $i$, otherwise $x_i[k] - x'_i = 0$.\n2. The gradient should be consistently positive or negative so that the magnitude of the integral grows.\nThe means that not only must the gradient be stable for changes in feature $i$ but also other features along the path.\nThis might also explain why we see $x_3$ and $x_4$ cluster closer to zero as their effect is dependent.\n\nThe second (messy) observation is related to [this nice post on SHAP titled \"Be careful when intrepreting predictive models in search of causal insights.](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html)\nIt essentially states that SHAP values to not infer causality and they show this with a nice subscriber retention example.\nIn their example there are both measured and unmeasured confounding features, both causing skewed insights regarding discounts and ad spend.\n\nTo verify that the neural net suffers from the same issues we recreate the dataset, train a classifier neural net and calculate its gradients.\nBecause care about the directionality of our gradients we average instead of looking at the power. \nThis gives us the aggregated sensivity in the bar chart below:\n\n![...](/images/sensitivity/sens_9.png)\n\nIndeed, it suffers from the same problem as highlighted by their example.\nSo... feature importance and sensitivity analys can be bogus if the causality is whack.\nWhat can be done about this?\nThe article talks about the double-ML approach for observed cofounders, which is the case for ad spend.\nThe steps are:\n1. Train a model to predict ad spend using possible confounders.\n2. Train a model to predict the target (did renew) using the same set of possible confounders.\n3. Train a model to predict the residual of 2. using the residual from 1.\n\nUsing this methodology one finds that there is no additional information in ad spend after what has been predicted from the confounders.\nThe article also mentions redudancy which is a problem of sales calls as a subset of interactions.\nThis means that the model thinks that sales calls are redudant for the modeling but the simplest way to test this is to simply fit a univariate model.\nFor unobserved confounders however, like product need which influences discount, we are out of luck.\nThe article talks about experimental design to identify the true causal nature.\n\nAll of this has inspired me to dive into causal modeling in my next post.\n\nAnother thing to also reflect upon; now we have only looked inwards at the models but another approach is simulation.\nThis is what sensivitity-analysis-through-probing frameworks do. It is often referred to what-if simulations,\nwhich can act as basis of decision for policy or in a control loop.\n\nHowever, with what we have learned about models missing causality or lacking confidence \none understands that it is never as simple as \"train a model and start using it\".\nA lot of effort can be put towards understanding a problem, models of it and the failure modes of those models.","src/content/blog/sensitivity-analysis.mdx","eef203ce7c09938f","sensitivity-analysis.mdx","stochastic-differential-equations",{"id":155,"data":157,"body":163,"filePath":164,"digest":165,"legacyId":166,"deferredRender":24},{"title":158,"description":159,"pubDate":160,"tags":161},"Stochastic differential equations in practice","I regret skipping this course in university",["Date","2025-09-06T00:00:00.000Z"],[79,32,162],"sde","As anyone with an engineering background I have had my fair share of differential equations.\nHowever, those were only limited to ordinary (ODE) and partial (PDE).\nI have noted that stochatic differential equations (SDE) are typically used in finance \nbut I never got a chance to sit down with them.\nAs someone who has looked at time series through the lens of dynamical systems, \ntime series models, state space models, (not Markov chain models... yet!),\nFourier transform, wavelets, Bayesian modeling, neural networks and so forth\nit only seemed natural to take the next step.\n\nStarting off with the basics, geometric Brownian motion (GBM)\n$$\ndS_t = \\mu S_t dt + \\sigma S_t dW_t,\n$$\nis the classical example of an SDE.\nIt can be used to describe the Brownian motion $W_t$ with volatility $\\sigma$\nof a stock price $S_t$ with a drift $\\mu$.\nThe standard form of SDEs is defined with two generic functions $f$ and $g$:\n$$\nx_t = f(x_t, t) dt + g(x_t, t) dW_t,\n$$\n\nIn order to numerically integrate this one can use the Euler-Maruyama method as:\n$$\nx_{t + \\Delta t} = x_t + f(x_t, t)\\Delta t + g(x_t, t) \\Delta W_t,\n$$\nwhere the order of the error is $O(\\sqrt{\\Delta t})$.\nWhen integrating one can sample $\\Delta W_t$ from a Gaussian distribution like $\\Delta W_t = \\sqrt{\\Delta t} Z, Z \\sim \\mathcal{N}(0,1)$.\nThis is the method that the package `sdeint` uses by default.\nThere are also other methods for integrating called the Milstein method,\nit adds a second-order correction term from Taylor expansion\n$$\nx_{t + \\Delta t} = x_t + f(x_t, t)\\Delta t + g(x_t, t)\\Delta W_t, + \\frac{1}{2} g'(x_t) g(x_t)((\\Delta W_t)^2 - \\Delta t).\n$$\n\nIn order to get positivity preservation one can log-transform the problem and apply Euler-Maruyama (called log-Euler).\nAlternatively one can truncate $x_t = max(x_t, 0)$ either Euler-Maruyama or Milstein to get truncated methods.\nThis prevents one weakness of these methods which is that they can become negative, which for example a stock price can not.\n\nNow, all of this is nice but GBM has a closed form solution\n$$\nS_t = S_0 e^{(\\mu - \\frac{1}{2}\\sigma^2)t + \\sigma W_t},\n$$\nwhich can be simulated using the code:\n```python\nM = 5\nN = 256\ndt = 0.01\n\nX0 = 1\nmu = 1\nsigma = 1\nt = np.arange(dt, N*dt + dt, dt)\n\nY = np.zeros((N, M))\nfor i in range(M):\n    # Create Brownian Motion\n    np.random.seed(i)\n    dB = np.sqrt(dt) * np.random.randn(N)\n    B  = np.cumsum(dB)\n    # Compute exact solution\n    Y[:,i] = X0 * np.exp((mu - 0.5 * sigma**2) * t + sigma * B)\n```\n\n![...](/images/sde/sde_1.png)\n\nAnother SDE is the Vasicek / Ornstein–Uhlenbeck (OU) process which looks like\n$$\ndr_t = a(b - r_t)dt + \\sigma dW_t,\n$$\nwhich is a mean reversion model.\nWhereas GBM models growth the mean reversion type models are suitable for long-term stability like interest rates and volatility.\nThe parameter $a$ is the speed of mean reversion, $b$ is the long term mean and $\\sigma$ is the volatility.\nIt, like GBM, also has an exact solution:\n$$\nr_{t + \\Delta t} = r_t e^{-a \\Delta t} + b(1 - e^{-a \\Delta t}) + \\sigma \\sqrt{\\frac{1 - e^{-2a \\Delta t}}{2a}} Z,\\quad Z \\sim \\mathcal{N}(0,1).\n$$\n\n```python\n# Vasicek parameters\na = 2.0       # mean reversion speed\nb = 0.05      # long-term mean\nsigma = 0.1   # volatility\nr0 = 0.03     # initial rate\nT = 2.56      # horizon\nN = 256       # steps\ndt = T / N\nM = 5         # number of paths\n\nt = np.linspace(0, T, N+1)\n\npaths = np.zeros((N+1 , M))\npaths[0,:] = r0\nnp.random.seed(5)\nfor m in range(M):\n    rt = r0\n    for i in range(N):\n        z = np.random.randn()\n        rt = (\n            rt * np.exp(-a*dt) \n            + b * (1 - np.exp(-a*dt)) \n            + sigma * np.sqrt((1 - np.exp(-2*a*dt))/(2*a)) * z\n        )\n        paths[i+1, m] = rt\n```\n\n![...](/images/sde/sde_2.png)\n\nCIR is also a mean reverting process where the diffusion is different from a Vasicek process\n$$\ndr_t = a(b - r_t)dt + \\sigma \\sqrt{r_t} dW_t.\n$$\nThe diffusion scales with $\\sqrt{r_t}$, which ensures positivity of the process.\nWhile the distribution for Vasicek is normal the CIR distibution is a noncentral chi-squared one.\nIn order to simulate this one can sample $r_t$ from $r_s$ where $\\Delta t = t - s$ as\n$$\n\\begin{align*}\nr_t &\\sim c \\cdot \\chi_d^{\\prime 2}(\\lambda),\\\\\nc &= \\frac{\\sigma^2 (1 - e^{-a(t-s)})}{4a},\\\\\nd &= \\frac{4ab}{\\sigma^2},\\\\\n\\lambda &= \\frac{4a e^{-a(t-s)} r_s}{\\sigma^2 (1 - e^{-a(t-s)})}.\n\\end{align*}\n$$\n\nThe code for this can look like:\n\n```python\nfrom scipy.stats import ncx2\n\n# CIR parameters\na = 2.0       # mean reversion speed\nb = 0.05      # long-term mean\nsigma = 0.1   # volatility\nr0 = 0.03     # initial rate\nT = 2.56      # horizon\nN = 256       # steps\ndt = T / N\nM = 5         # number of paths\n\nt = np.linspace(0, T, N+1)\n\npaths = np.zeros((N+1 , M))\npaths[0,:] = r0\nnp.random.seed(5)\nfor m in range(M):\n    rt = r0\n    for i in range(N):\n        c = (sigma**2 * (1 - np.exp(-a*dt))) / (4*a)\n        d = 4*a*b/sigma**2\n        lam = (4*a*np.exp(-a*dt)*rt) / (sigma**2*(1-np.exp(-a*dt)))\n        # sample from noncentral chi-square\n        rt = c * ncx2.rvs(d, lam)\n        paths[i+1, m] = rt\n```\n\n![...](/images/sde/sde_3.png)\n\n\n\nHeston is an extension for GBM where one assumes that the variance can drift\n$$\n\\begin{align*}\ndS_t &= \\mu S_t dt + \\sqrt{v_t} S_t dW_t^S,\\\\\ndv_t &= \\kappa(\\theta - v_t) dt + \\xi \\sqrt{v_t} dW_t^v,\\\\\ndW_t^S dW_t^v &= \\rho dt.\n\\end{align*}\n$$\n\nNote that the variance $v_t$ is modelled by a CIR process and the noise $dW_t^S, dW_t^v$ is correlated by a factor $\\rho$.\nHeston does not have a closed form solution but neither Euler-Maruyama nor Milstein is optimal for integration.\nInstead there is a specific scheme called the Andersen quadratic exponential (QE) scheme.\nIt starts off by approximating the non-central chi-square distribution for $v_t$ using a QE distribution,\ndepending on the conditional mean $m$ and variance $s^2$ and using them to define the ratio $\\psi$:\n$$\n\\begin{align*}\nm &= \\theta + (v_t - \\theta) e^{-\\kappa \\Delta t},\\\\\ns^2 &= \\frac{v_t \\sigma^2 e^{-\\kappa \\Delta t}}{\\kappa}(1 - e^{-\\kappa \\Delta t}) + \\frac{\\theta \\sigma^2}{2\\kappa}(1 - e^{-\\kappa \\Delta t})^2,\\\\\n\\psi &= \\frac{m^2}{s^2}.\n\\end{align*}\n$$\n\nThen based on a threshold $\\psi_c \\approx 1.5$ one either samples from the quadratic when $\\psi \\leq \\psi_c$:\n$$\n\\begin{align*}\nv_{t + \\Delta t} &= a(b + Z)^2,\\quad Z \\sim \\mathcal{N}(0,1),\\\\\nb &= \\frac{\\psi - 1}{2},\\\\\na &= \\frac{m}{1 + b^2},\n\\end{align*}\n$$\n\nor from the exponential when $\\psi > \\psi_c$:\n$$\n\\begin{align*}\nv_{t + \\Delta t} &= \n\\begin{cases}\n0,\\text{with probability}\\:p\\\\\n\\frac{1}{\\beta} ln(\\frac{1-p}{U}),\\>\\text{with probability}\\:1-p\n\\end{cases}\\\\\np &= \\frac{\\psi-1}{\\psi+1},\\\\\n\\beta &= \\frac{1-p}{m}.\n\\end{align*}\n$$\n\nOnce we have this $v_t$ estimate the estimation of $S_t$ is done using log Euler, given $X_t = ln(S_t)$\n$$\nX_{t + \\Delta t} = X_t + (r - \\frac{1}{2}\\bar{v})\\Delta t + \\sqrt{\\bar{v} \\Delta t}Z_x,\n$$\nwhere $Z_x$ is a standard normal correlated with the variance driver $\\rho$ and $\\bar{v}$ is the average of $v_t$ and $v_{t + \\Delta t}$ (for stability).\nTo create correlated noise, draw two samples $Z_1, Z_2 \\sim \\mathcal{N}(0,1)$ and define $Z_v = Z_1$ and $Z_s = \\rho \\Z_1 + \\sqrt{1 - \\rho^2} Z_2$.\nWe can use $Z_v$ for $v_t$ and $Z_s$ for $S_t$/$X_t$.\nFor the exponential branch of QE the recommendation is to do an independent draw rather than running $Z_s$ through the inverse cumulative density function.\n\n```python\n# Parameters and simulation\nS0 = 100.0\nv0 = 0.04\nr = 0.03\nq = 0.0\nkappa = 1.5\ntheta = 0.04\nsigma = 0.3\nrho = -0.7\nT = 1.0\nN = 252  # daily steps for a year\nM = 5    # requested number of paths\npsi_c = 1.5\n\ndt = T / N\nsqrt_dt = np.sqrt(dt)\ntimes = np.linspace(0, T, N+1)\n\nS = np.zeros((N+1, M))\nv = np.zeros((N+1, M))\nS[0, :] = S0\nv[0, :] = v0\n\nfor n in range(N):\n    # generate independent normals for each path\n    U1 = np.random.normal(size=M)  # will be used as the \"variance normal\" (Zv)\n    U2 = np.random.normal(size=M)  # independent, to build the asset normal (Zs)\n    \n    # correlated normal for asset's Brownian step\n    Z_v = U1\n    Z_s = rho * U1 + np.sqrt(1 - rho**2) * U2\n    \n    v_n = v[n, :]\n    \n    # Precompute exponentials used in moment formulas\n    exp_kdt = np.exp(-kappa * dt)\n    m = theta + (v_n - theta) * exp_kdt\n    # s2 (variance of v_{n+1} conditional on v_n)\n    s2 = (v_n * sigma**2 * exp_kdt * (1 - exp_kdt) / kappa\n            + theta * sigma**2 / (2 * kappa) * (1 - exp_kdt)**2)\n    \n    # Ensure non-negative moments numerically\n    m = np.maximum(m, 0.0)\n    s2 = np.maximum(s2, 0.0)\n    \n    psi = np.zeros_like(m)\n    # avoid division by zero\n    idx_nonzero = m > 0\n    psi[idx_nonzero] = s2[idx_nonzero] / (m[idx_nonzero]**2)\n    psi[~idx_nonzero] = np.inf  # if m==0, force exponential branch\n    \n    v_next = np.zeros_like(v_n)\n    \n    # Branch 1: psi \u003C= psi_c  (central region) -> Non-central chi-square approx via squared normal\n    idx1 = psi \u003C= psi_c\n    if np.any(idx1):\n        psi1 = psi[idx1]\n        m1 = m[idx1]\n        # compute b and a as in Andersen\n        tmp = 2.0 / psi1 - 1.0\n        # safe positive sqrt for the nested expression\n        b = np.sqrt(tmp + np.sqrt(tmp * (tmp + 2.0)))\n        a = m1 / (1.0 + b**2)\n        Zv = Z_v[idx1]\n        v_next[idx1] = a * (b + Zv)**2\n    \n    # Branch 2: psi > psi_c -> mixture with a point mass at zero and exponential tail\n    idx2 = psi > psi_c\n    if np.any(idx2):\n        psi2 = psi[idx2]\n        m2 = m[idx2]\n        p = (psi2 - 1.0) / (psi2 + 1.0)\n        beta = (1.0 - p) / m2\n        # draw uniforms to decide zero vs exp\n        U = np.random.uniform(size=p.size)\n        is_zero = U \u003C= p\n        # where it's zero\n        v_next[idx2][is_zero] = 0.0\n        # else exponential sample\n        not_zero_idx = ~is_zero\n        if np.any(not_zero_idx):\n            U2_exp = np.random.uniform(size=not_zero_idx.sum())\n            v_next[idx2][not_zero_idx] = -np.log(U2_exp) / beta[not_zero_idx]\n    \n    # Numerical floor to avoid tiny negative values\n    v_next = np.maximum(v_next, 0.0)\n    \n    # Asset update using v_bar = 0.5*(v_n + v_next)\n    v_bar = 0.5 * (v_n + v_next)\n    # To avoid negative variance under sqrt due to numerical issues\n    v_bar = np.maximum(v_bar, 0.0)\n    \n    # Log-asset update (Euler-like with correlated normals)\n    # dX = (r - q - 0.5 v_bar) dt + sqrt(v_bar * dt) * Z_s\n    diffusion = np.sqrt(v_bar * dt) * Z_s\n    drift = (r - q - 0.5 * v_bar) * dt\n    S[n+1, :] = S[n, :] * np.exp(drift + diffusion)\n    v[n+1, :] = v_next\n```\n\n![...](/images/sde/sde_4.png)\n\n![...](/images/sde/sde_5.png)\n\n\n\nWhile Heston adds complexity to GBM using variance drift; there are other ways.\nMerton Jump-Diffusion includes a \"jump\" process $J_tdN_t$ where $J_t$ is the jump size and $dN_t$ is a Poisson process.\nThe SDE looks like\n$$\ndS_t = \\mu S_t dt + \\sigma S_t dW_t + J_t S_t dN_t.\n$$ \n\nThe discrete-time update over a step $\\Delta t$ is\n$$\nS_{t + \\Delta t} = S_t e^{(\\mu - \\lambda \\kappa - \\frac{1}{2}\\sigma^2) \\Delta t + \\sigma \\Delta W} \\prod_{i = 1}^{N_{\\Delta t}} e^{Y_i},\n$$\nwhere $\\Delta W \\sim \\mathcal{N}(0,\\Delta t)$, $N_{\\Delta t} \\sim \\text{Poisson}(\\lambda \\Delta t)$, $Y_i \\sim \\mathcal{N}(\\mu_J, \\sigma_J^2)$\nand $\\kappa = e^{\\mu_J + \\frac{1}{2}\\sigma_J^2}-1$.\nThere are multiple ways to simulate this; using a fix $\\Delta t$ using the method above, \nif $\\lambda \\Delta t$ is small one can approximate the Poisson process using Bernoulli($\\lambda \\Delta t$),\nor one can generate all the jumps and integrate between those times. Some example code:\n\n```python\nS0 = 100 # initial price\nmu = 0.05 # drift (real-world or risk-neutral depending on use)\nsigma = 0.2 # diffusion volatility\nlam = 0.3 # jump intensity (lambda)\nmu_j = -0.1 # normal params for jump log-size Y ~ N(mu_j, sigma_j^2)\nsigma_j = 0.25 # normal params for jump log-size Y ~ N(mu_j, sigma_j^2)\nT = 2.52*3\n\nM = 5\nN = 1000\n\nrng = np.random.default_rng(5)\ndt = T / N\nt = np.arange(N)*dt\nkappa = np.exp(mu_j + 0.5 * sigma_j**2) - 1.0\n\nS = np.full((N+1, M), S0, dtype=float)\nsqrt_dt = np.sqrt(dt)\n\nfor i in range(N):\n    # diffusion increments\n    dW = rng.normal(0.0, sqrt_dt, size=M)\n    # Poisson counts for each path in interval\n    Nt = rng.poisson(lam * dt, size=M)\n    # jump multipliers\n    J = np.ones(M, dtype=float)\n    # for paths with at least one jump, sample sum of Y's\n    idx = np.where(Nt > 0)[0]\n    if idx.size:\n        # sample total jump (sum of Nt normals) efficiently:\n        # total Y_sum ~ Normal(Nt*mu_j, Nt*sigma_j^2)\n        Ns = Nt[idx]\n        Y_sums = rng.normal(loc=Ns*mu_j, scale=np.sqrt(Ns)*sigma_j)\n        J[idx] = np.exp(Y_sums)\n\n    # continuous update (compensated drift)\n    cont = (mu - lam * kappa - 0.5 * sigma**2) * dt + sigma * dW\n    S[i+1,:] = S[i,:] * np.exp(cont) * J\n```\n\n![...](/images/sde/sde_6.png)\n\nAnother type of SDE is the SABR (stochastic alpha, beta, rho) volatility model.\nInstead of asset prices $S_t$ or volatility $v_t$ it models a forward $F_t$.\nOne does not typically not integrate it directly, instead there are asymptotic solutions that are used for, e.g., call option pricing.\nThe SDE looks like\n$$\n\\begin{align*}\ndF_t &= \\alpha_t F_t^\\beta dW_t,\\\\\nd\\alpha_t &= \\nu \\alpha_t dZ_t,\\\\\nw_t Z_t &= \\rho dt,\\\\\n\\end{align*}\n$$\nwhere $\\alpha_t$ is the volatility, $\\beta$ is an elasticity parameter, $\\nu$ is the volatility of the volatility\nand $\\rho$ is the correlation between the Browian motions.\nThe SDE is designed to capture the \"smile\", the U-shape of volatility meaning.\nFor forward simulation one can use the following code:\n```python\n# SABR parameters\nF0 = 0.03      # initial forward rate (3%)\nalpha0 = 0.2   # initial volatility\nbeta = 0.5     # elasticity parameter\nnu = 0.4       # vol-of-vol\nrho = -0.3     # correlation\nT = 1.0        # maturity (in years)\nN = 252        # time steps (daily)\ndt = T / N\nn_paths = 5\n\n# Pre-allocate arrays\nF_paths = np.zeros((N+1, n_paths))\nalpha_paths = np.zeros((N+1, n_paths))\nt = dt*np.arange(N)\n\n# Initial conditions\nF_paths[:, 0] = F0\nalpha_paths[:, 0] = alpha0\n\n# Cholesky for correlation\ncov = np.array([[1, rho], [rho, 1]])\nL = np.linalg.cholesky(cov)\n\nnp.random.seed(5)\n# Simulate paths\nfor m in range(n_paths):\n    F, alpha = F0, alpha0\n    for i in range(1, N+1):\n        z = np.random.randn(2)\n        dW, dZ = L @ z * np.sqrt(dt)\n        # Update alpha (lognormal diffusion)\n        alpha = alpha * np.exp(-0.5 * nu**2 * dt + nu * dZ)\n        # Update forward\n        F = F + alpha * (F**beta) * dW\n        # Reflection to prevent negative values\n        F = abs(F)\n        # Store\n        F_paths[i, m] = F\n        alpha_paths[i, m] = alpha\n```\n\nNote that with our reflection or truncation $F_t$ can be come negative, which becomes a problem for $\\beta \\neq 0, 1$.\nTo avoid complex solutions we go for the reflection method in this example, yielding the following curves.\n\n![...](/images/sde/sde_7.png)\n\nHagan's SABR implied volatility asymptotic solution can be implemented in code as:\n```python\ndef sabr_implied_vol(\n        F, # Initial F\n        K, # Strike\n        T, # Time period\n        alpha, \n        beta, \n        rho, \n        nu\n    ):\n    if abs(F - K) \u003C 1e-12:\n        term1 = ((1 - beta)**2 * alpha**2) / (24 * (F**(2 - 2*beta)))\n        term2 = (rho * beta * nu * alpha) / (4 * (F**(1 - beta)))\n        term3 = ((2 - 3*rho**2) * nu**2) / 24\n        vol = (alpha / (F**(1 - beta))) * (1 + (term1 + term2 + term3) * T)\n        return vol\n\n    logFK = np.log(F / K)\n    A = (F * K)**((1 - beta) / 2)\n    D = 1 + ((1 - beta)**2 / 24) * logFK**2 + ((1 - beta)**4 / 1920) * logFK**4\n    z = (nu / alpha) * A * logFK\n    x_z = np.log((np.sqrt(1 - 2*rho*z + z**2) + z - rho) / (1 - rho))\n    term1 = ((1 - beta)**2 * alpha**2) / (24 * A**2)\n    term2 = (rho * beta * nu * alpha) / (4 * A)\n    term3 = ((2 - 3*rho**2) * nu**2) / 24\n    vol = (alpha / (A * D)) * (z / x_z) * (1 + (term1 + term2 + term3) * T)\n    return vol\n```\n\n## Fitting stochastic differential equations to data\n\nWhile generating data using models is fun, using said models to understand data is *funner*.\n\nFor GBM and Vasicek there are closed form maximum likelihood estimators (MLE).\nFor Vasicek it is:\n```python\nn = len(X)\nX_prev, X_next = X[:-1], X[1:]\nX_prev_mean = X_prev.mean()\nX_next_mean = X_next.mean()\n\n# Regression for AR(1) representation\nSxx = np.sum((X_prev - X_prev_mean) ** 2)\nSxy = np.sum((X_prev - X_prev_mean) * (X_next - X_next_mean))\n\na_hat = Sxy / Sxx\nb_hat = X_next_mean - a_hat * X_prev_mean\n\n# Residual variance\nresiduals = X_next - (a_hat * X_prev + b_hat)\nsigma_eps2 = np.sum(residuals ** 2) / (n-1)\n\n# Map back to SDE parameters\nkappa_hat = -np.log(a_hat) / dt\ntheta_hat = b_hat / (1 - a_hat)\nsigma_hat = np.sqrt(2 * kappa_hat * sigma_eps2 / (1 - a_hat**2))\n\nprint(kappa_hat, theta_hat, sigma_hat)\n```\n\nAnd for GBM:\n```python\nn = len(S)\nR = np.diff(np.log(S))  # log-returns\n\nR_bar = np.mean(R)\nsigma2_hat = np.sum((R - R_bar) ** 2) / (n * dt)\nsigma_hat = np.sqrt(sigma2_hat)\nmu_hat = R_bar / dt + 0.5 * sigma2_hat\n\nprint(mu_hat, sigma_hat)\n```\n\nNote that these are just point estimates.\nTo get confidence intervals one can derive them analytically for these examples or we look at another solution scheme.\nA more general way to approach SDE calibration is through a Bayesian framework.\nTake the CIR process, which does not have a closed form MLE because of its noncentral chi-squared distribution\n$$\ndr_t = a(b - r_t)dt + \\sigma \\sqrt{r_t} dW_t,\n$$\nif we throw the Euler-Maruyama method at it we get\n$$\n\\begin{align*}\nr_{t + \\Delta t} &= r_t + a(b - r_t) \\Delta t + \\sigma \\sqrt{r_t \\Delta t} Z, \\quad Z \\sim \\mathcal{N}(0,1),\\\\\nr_{t + \\Delta t} &\\sim \\mathcal{N}(a(b - r_t) \\Delta t, \\sigma^2 r_t \\Delta t).\n\\end{align*}\n$$\n\nThis is something we can construct in, say PyMC, and sample posterior distributions for $a, b, \\sigma$.\n```python\nimport pymc as pm\nimport arviz as az\n\nwith pm.Model() as cir_model:\n    # Priors\n    theta = pm.HalfNormal(\"theta\", sigma=1.0)\n    mu = pm.HalfNormal(\"mu\", sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", sigma=0.1)\n    \n    # Likelihood\n    X_prev = X[:-1]\n    X_next = X[1:]\n    mean = X_prev + theta*(mu - X_prev)*dt\n    sd = sigma*np.sqrt(np.maximum(X_prev, 0)*dt)\n    \n    pm.Normal(\"obs\", mu=mean, sigma=sd, observed=X_next)\n    \n    # Inference\n    trace = pm.sample(2000, tune=1000, target_accept=0.9, return_inferencedata=True)\n\naz.plot_trace(trace)\naz.summary(trace, hdi_prob=0.95)\n```\n\nThe output for one simulated CIR with $a = 0.3, b = 0.05, \\sigma = 0.06$ is:\n|           | **mean** | **sd** | **hdi_2.5%** | **hdi_97.5%** |\n|-----------|----------|--------|--------------|---------------|\n| **a**     | 0.701    | 0.441  | 0.001        | 1.492         |\n| **b**     | 0.040    | 0.017  | 0.010        | 0.066         |\n| **sigma** | 0.057    | 0.001  | 0.054        | 0.059         |\n\nBy sampling from the posterior distributions we can see alternate trajectories for this SDE:\n```python\n# Extract posterior samples\na_samples = trace.posterior['a'].stack(sample=(\"chain\",\"draw\")).values\nb_samples = trace.posterior['b'].stack(sample=(\"chain\",\"draw\")).values\nsigma_samples = trace.posterior['sigma'].stack(sample=(\"chain\",\"draw\")).values\n```\n![...](/images/sde/sde_8.png)\n\nThis method even works for jump processes.\nHere example of a Vasicek + jumps process\n$$\n\\begin{align*}\ndC_t &= \\alpha(\\mu - C_t) dt + \\sigma dW_t + dJ_t,\\\\\n\\xrightarrow[\\text{+ jump-as-mixture}]{\\text{Euler-Maruyama}}\\:&\\approx \\begin{cases}\nC_{t+\\Delta t} = C_t + \\alpha(\\mu - C_t) \\Delta t + \\sigma \\sqrt{\\Delta t} Z, \\text{with probability}\\:1-p_J\\\\\nC_{t+\\Delta t} = C_t + \\alpha(\\mu - C_t) \\Delta t + \\sigma_J Z, \\text{with probability}\\:p_J\n\\end{cases}\\\\\n&= \\begin{cases}\nC_{t+\\Delta t} \\sim \\mathcal{N}(C_t + \\alpha(\\mu - C_t) \\Delta t, \\sigma^2 \\Delta t), \\text{with probability}\\:1-p_J\\\\\nC_{t+\\Delta t} \\sim \\mathcal{N}(C_t + \\alpha(\\mu - C_t) \\Delta t, \\sigma_J^2 \\Delta t), \\text{with probability}\\:p_J\n\\end{cases}\\\\\n\\end{align*}\n$$\n\nThe approximation of Euler-Maruyama has now been baked together with the assumption that diffusion $\\ll$ jump\nand that the jumps are rare enough to be modelled by a Bernoulli distribution rather than a Poisson distribution.\nThese assumptions can be tuned further but the code for this example turns out nice and simple:\n\n```python\nimport pymc as pm\nimport pytensor.tensor as pt\nimport arviz as az\n\nwith pm.Model() as ou_jump_model:\n    # Priors\n    alpha = pm.Exponential('alpha', 1/0.1)\n    mu = pm.Normal('mu', mu=1e5, sigma=5e4)\n    sigma = pm.HalfNormal('sigma', sigma=1e4)\n\n    p_jump = pm.Beta('p_jump', alpha=1, beta=20)\n    mu_jump = pm.Normal('mu_jump', mu=-2e4, sigma=2e4)\n    sigma_jump = pm.HalfNormal('sigma_jump', sigma=2e4)\n\n    # Mixture likelihood\n    C_prev = C[:-1]\n    C_next = C[1:]\n    mean = C_prev + alpha * (mu - C_prev) * dt\n    y = pm.Mixture(\n        \"delta_C_obs\",\n        w=[1 - p_jump, p_jump],\n        comp_dists=[\n            pm.Normal.dist(mu=mean, sigma=sigma*np.sqrt(dt)),\n            pm.Normal.dist(mu=mean + mu_jump, sigma=sigma_jump)\n        ],\n        observed=C_next\n    )\n\n    # Inference\n    trace_jump = pm.sample(draws=2000, tune=1500, target_accept=0.95)\n\n# Summarize\nprint(az.summary(trace_jump, var_names=['alpha','mu','sigma','p_jump','mu_jump','sigma_jump']))\naz.plot_trace(trace_jump, var_names=['alpha','mu','sigma','p_jump'])\nplt.show()\n```\n\nSo if we simulate some data:\n```python\n# --- Parameters ---\nT = 365           # days\nalpha = 0.05      # mean reversion speed\nmu = 100000       # long-run mean\nsigma = 5000      # diffusion volatility\ndt = 0.25         # timestep\nC0 = 80000        # initial\nN = int(T/dt)+1\nt = np.arange(N)*dt\n\n# Jump parameters\njump_prob = 0.02   # daily prob of a jump\njump_mean = -20000 # average jump size (defaults, negative)\njump_sd = 10000    # variability in jump size\n\n# --- Simulation ---\nC = [C0]\nfor t in range(N):\n    prev = C[-1]\n    drift = alpha * (mu - prev) * dt\n    diffusion = sigma * np.sqrt(dt) * np.random.randn()\n    jump = 0\n    if np.random.rand() \u003C jump_prob*dt:\n        jump = np.random.normal(jump_mean, jump_sd)\n    new_val = prev + drift + diffusion + jump\n    C.append(max(new_val, 0))\n```\n\n![...](/images/sde/sde_9.png)\n\nWe can see that the code above estimates good ranges for this SDE, despite the jumps:\n| ****           | **mean**   | **sd**   | **hdi_3%** | **hdi_97%** |\n|----------------|------------|----------|------------|-------------|\n| **alpha**      | 0.050      | 0.014    | 0.023      | 0.077       |\n| **mu**         | 99456.161  | 7529.641 | 86656.148  | 113070.527  |\n| **sigma**      | 5095.827   | 98.195   | 4916.529   | 5284.484    |\n| **p_jump**     | 0.007      | 0.003    | 0.003      | 0.013       |\n| **mu_jump**    | -22737.511 | 4577.910 | -30153.094 | -13448.753  |\n| **sigma_jump** | 10147.951  | 3702.079 | 4136.082   | 16752.985   |","src/content/blog/stochastic-differential-equations.mdx","66be268bb2152e1b","stochastic-differential-equations.mdx"]