[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.11.0","content-config-digest","af38c04c4284c50b","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://yourusername.netlify.app\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,25,26,37,38],"incremental-median-estimation",{"id":11,"data":13,"body":20,"filePath":21,"digest":22,"legacyId":23,"deferredRender":24},{"title":14,"description":15,"pubDate":16,"tags":17},"Incremental median estimation","How to estimate the median of steaming data",["Date","2025-07-15T00:00:00.000Z"],[18,19],"statistics","streaming data","import ImageGallery from '../../components/ImageGallery.astro';\n\nA couple of years ago I thought to myself:\n\n*\"I know how to estimate a cumulative average, but how does one estimate the median in the same way?\"*\n\nThis was an interesting question because while averages are great, they still suffer greatly from outliers. \nThis led me down a track that ended on a stack overflow thread of cumulative median esimation, and by extension, any quantile.\nThe algorithm is simple and uses a frequency approach to the problem.\n\nIn a nutshell, if the distribution is stationary then we expect a 50/50 split of higher and lower observations around our median.\nThe way we translate this into an algorithm is that higher/lower nudges our median estimate, and if the higher/lower observations are in balance then the nudges cancel out.\nIt can be coded up as this:\n\n```python\ndef update_median_estimate(\n        median_estimate: float, \n        observation: float, \n        nudge: float\n    ) -> float:\n    diff = median_estimate - observation\n    sign = diff / abs(diff)\n    new_median_estimate = median_estimate - nudge * sign\n    return new_median_estimate\n```\n\nThe median estimate can just be initialized to the first observation. This leave the nudge factor as a parameter. Here is a graph of the median estimations applied to Gaussian noise with two different nudges:\n\n![Two incremental median estimations](/images/inc_1.png)\n\nNote that nudge = 0.1 makes the estimate converge quicker than nudge = 0.01, but, the trade-off is that it is also more sensitive to noise. So, what is a good nudge factor?\n\nOne way is to approach is heuristically; say that we expect the estimator to approximately reach the median in M steps. \nSince the median is the 50th percentile; a worst case scenario where we start at the min or max means that we have to step 50 percentage units. \nThis means that we could choose $\\frac{max(max - median,\\: median - min)}{M}$ as the nudge factor. \nSince we do not know the median we can use the range $R = max - min$ as an upper bound $\\frac{R}{M}$. This is an extreme scenario where the median is close to either the min or max.\nFor the uniform scenario where the median lies dead in the middle between min and max it is twice as big, so then we will get to our estimate in $\\frac{M}{2}$ steps.\n\n![Incrementally estimating max and min](/images/inc_2.png)\n\nWith this method we have a bit more control, as can be seen in the following graph:\n\n![Comparison of different R/M nudges](/images/inc_3.png)\n\nEyeballing this particular example gives the M = 30, 16 steps and M = 120, 91 steps to get to the sample median. \nNow, there additional steps we can take to ensure a fair(er) nudging toward the median and it relates to $R$.\nWe do not expect our estimate of R to be close to the truth after only 3 or 10 observations, so what do we expect it to be and can we correct for that?\n\nFor $n$ samples selected uniformly on the interval [0,1) the expectation of the maximum is $\\frac{n}{n+1}$ and the minimum is $\\frac{1}{n+1}$.\nThis means the expectation of the range is\n$$\nE(R|n) = \\frac{n}{n+1} - \\frac{1}{n+1} = \\frac{n-1}{n+1}.\n$$\n\nThe intuition behind this is that $n$ samples split the line into $n+1$ sections and ordering them by size gives $1$ and $n$ as the first and final sections.\nOf course, our data is not likely uniformly distributed, but the quantiles are by definition.\nThis means that after, for example $n = 9$, we would have expected to seen 10% and 90% percentile with an 80 percentile range.\nBy $n = 99$ we will have seen from the 1st to the 99th percentile and subsequently the $n = 999$ sample gives us 0.001 to 0.999 quantile.\nThere are two take-aways from this observation.\n\nThe first is that we can invert the formula $R = \\frac{n-1}{n+1} \\rightarrow n = -\\frac{R+1}{R-1}$.\nThis is handy if we are looking for a specific quantile $q$, it corresponds to the range $R_q = 1-2q$ and by our formula $n = \\frac{1-q}{q}$. \nThis only makes sense for $q \u003C 0.5$ so for $q > 0.5$ we find its mirrored buddy $\\tilde{q} = 1 - q$. \nNow, specifically for the median this collapses to $n = 1$ but if we are going to look at any quantile then this will come in handy.\nIn essence, this estimate of $n$ will tell you when we it is likely to have seen the quantile we are estimating.\n\nThe second take away is that we can use the formula to create a better estimate $R_n$ out of our naive incremental $R$ as $R_n = \\frac{R}{\\frac{n-1}{n+1}} = \\frac{R(n+1)}{n-1}$. \nThis means that the range will scaled up to conpensate for likely unobserved range. This means that the full nudge heuristic is now $\\frac{R(n+1)}{M(n-1)}$. \nSee two examples below of the difference, one cherry-picked (note that the correction is only relevant when we need the nudges to be large) and one regular:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/inc_4.png\",\n      alt: \"Improved convergence example 1\",\n      caption: \"Cherry-picked example\"\n    },\n    {\n      src: \"/images/inc_5.png\",\n      alt: \"Improved convergence example 2\",\n      caption: \"Regular convergence\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nIt is not much but it is something.\n\nThe elephant in the room are the variations in the estimate. \nSince we are observing random noise the estimates will follow the random sequences of nudges like a random walk.\nWhat we know about random walks of $n$ steps is that the expectation $E(\\mathcal{Z}_n) = 0$ with the variance $E(\\mathcal{Z}_n^2) = n$.\nSo if we have a random walk then we would expect the standard deviation to grow like $\\sqrt{n}$, meaning that we should shrink the nudges to compensate for this effect.\nOf course, we would only apply this after $M$ steps, when we have actually expected to converge to the median.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/inc_6.png\",\n      alt: \"Test of random walk penalty of the nudge amplitude\",\n      caption: \"Example 1: Gaussian\"\n    },\n    {\n      src: \"/images/inc_7.png\",\n      alt: \"Test of random walk penalty of the nudge amplitude on an exponential distribution\",\n      caption: \"Example 2: Exponential\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nBut, our system is not a true random walk because is it still an approximation of the median, so it will have some drift towards the true distribution median. \nSay that we are targeting the $q_t$ quantile (0.5 in case of the median) with $dq$ nudges (0.1 for our example) of our estimate $q$. \nIf $q = q_t$ then we have a 50/50 of being nudged in either direction, say + happens, then we are at $q + dq$. \nThis has now shifted the probabilities as we now have a 40/60 chance in favor of going back towards $q_t$.\nIf + happens again then we are at $q + 2dq$, meaning now a 30/70 chance next time. \nThe most extreme scenario has $dq = q_t$ when $q_t = 0.5$ as it always reverses its previous step, meaning that the distribution is 0: 25%, $q_t$: 50%, 1: 25%.\n\nOf course, we want to be able to assume any $dq$. \nTo get an intution behind this we simulated using $dq = \\frac{1}{50}$ and noted that is approximated a Gaussian distribution.\nIn the limit where $dq \\rightarrow 0$ it makes sense because then the drift back towards the median is less dominant. \n![Histogram of quantile estimations](/images/inc_8.png)\n\nWe run more experiments with different fractions of $\\frac{dq}{q_t}$, calculate the standard deviation, resulting in:\n![Experiments logging standard deviation of quantile estimations as a function of dq/qt](/images/inc_9.png)\n\nThe curve fitting is easy as it is a square root of the fraction and $x = 0.5$ neatly intersects $y = 0.25$. \nThis means that empirically we have arrived at the rubber band random walk standard deviation $\\sigma = \\frac{1}{\\sqrt{8}}\\sqrt{\\frac{dq}{q_t}} = \\sqrt{\\frac{dq}{8q_t}}$. \nNote that this does not depend on $n$, which we conjectured above. \nSimiarly to what we have done before, we can shuffle expressions around to find the fraction given a target standard deviation $\\sigma$ as $\\frac{dq}{q_t} = 8\\sigma^2$.\nIf we were to calibrate a 95% confidence interval ($\\pm 1.96\\sigma$) over $\\pm 1$ percentiles ($\\rightarrow \\sigma = \\frac{0.01}{1.96}$) then $\\frac{dq}{q_t} \\approx 0.00021 \\rightarrow dq = 1/9604$ ($q_t = 0.5$).\nFor a more liberal $\\pm 2.5$ percentiles $\\frac{dq}{q_t} \\approx 0.0013 \\rightarrow dq = 1/1536.64$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/inc_10.png\",\n      alt: \"Example 1 of random walk confidence interval nudges\",\n      caption: \"Example 1\"\n    },\n    {\n      src: \"/images/inc_11.png\",\n      alt: \"Example 2 of random walk confidence interval nudges\",\n      caption: \"Example 2\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nThat has (kind-of) solved it! Now there are two possibilities to take this further, the first is to generalize to any quantile and the second is to adress non-stationarity.\nBut we save those for later posts :)","src/content/blog/incremental-median-estimation.mdx","53e4d3baee52be3b","incremental-median-estimation.mdx",true,"reading-list",{"id":25,"data":27,"body":33,"filePath":34,"digest":35,"legacyId":36,"deferredRender":24},{"title":28,"description":29,"pubDate":30,"tags":31},"Reading list","Interesting books and articles",["Date","2025-07-10T00:00:00.000Z"],[32],"other","This reading list is updated sporadically.\n\n## Books\n\n- **Good to great** - *Jim Collins* \u003Cbr> \nThis book made me understand just *how* fundamental certain challenges can be to an organisation.\nBefore I used to think that if I, an employee, worked harder and spoke my mind; \nthat it would change the course of the group or the company for the better. \nAfter this book I realize that changing things to get positive development means reworking the DNA of the business.\nAnd that's not something a single contributor can do. \u003C/br>\n\n- **Build** - *Tony Fadell* \u003Cbr> \nLots of nuggets for how to think about products and organisation. \u003C/br>\n\n- **Measure what matters** - *John Doerr* \u003Cbr>\nThis book is the book on Objectives & Key Results, OKR's for short. \nI read this because a department manager wanted to introduce it to our group.\nI was intrigued how goals can progression can be tracked over different hierarchy levels.\nUnfortunately this manager did not do the theory justice nor did he actually follow up the scappy goals we set for the quarter.\u003C/br>\n\n- **Bullshit jobs** - *David Groeber* \u003Cbr> \nThe first half outlines job archetypes that passes the time without long-term productivity. \nIt is a dark and funny characterisation of modern, mostly white-collar, work.\nThe second half attempts to diagnose why we have gotten to this point.\nIt is an interesting perspective and worth the read. \u003C/br>\n\n## Take aways\n* No strategy can save a lack of culture and that no culture can save a lack of leadership.","src/content/blog/reading-list.mdx","e3988be7b3a2d769","reading-list.mdx","incremental-quantile-estimation",{"id":37,"data":39,"body":44,"filePath":45,"digest":46,"legacyId":47,"deferredRender":24},{"title":40,"description":41,"pubDate":42,"tags":43},"Incremental quantile estimation","How to estimate any quantile of steaming data",["Date","2025-07-18T00:00:00.000Z"],[18,19],"import ImageGallery from '../../components/ImageGallery.astro';\n\nThis post continues from the incremental median estimation.\n\nTo estimate any quantile instead of the median one adds a bias to the sign of the nudge.\nWhat does this represent? \nConsider the case of the 90th percentile, if our estimate is accurate then we expect 90% of the incoming observations to be below the estimate.\nThis means that the 90 / 10 split must be the equilibrium between the 90% downward nudges and 10% upward nudges.\nThe bias $b = 1 - 2q$ for the 90% percentile is $b = -0.8$, this means that a positive sign $\\rightarrow 0.2$ and a negative $\\rightarrow -1.8$.\nIf we multiply these nudges by their frequency we see that they are in balance: $0.9 \\cdot 0.2 + 0.1 \\cdot -1.8 = 0.18 - 0.18 = 0$.\n\nOne thing to note is that the average absolute nudge is $0.18 + 0.18 = 0.36$, which is lower than $\\|0.5 \\cdot 1\\| + \\|0.5 \\cdot -1\\| = 1.0$ for the median.\nThis means that if we use a bias we also want to scale the size of the nudge by the inverse of $s_b = 4\\tilde{q}(1-\\tilde{q})$ where $\\tilde{q} = min(q, 1-q)$. \nThis means that an updated algorithm can be written as:\n\n```python\ndef update_quantile_estimate(\n        quantile_to_estimate: float,\n        quantile_estimate: float, \n        observation: float, \n        nudge: float\n    ) -> float:\n    bias = 1 - 2 * quantile_to_estimate\n    q_tilde = min(quantile_to_estimate, 1-quantile_to_estimate)\n    scale_bias = 4*q_tilde*(1-q_tilde)\n    diff = quantile_estimate - observation\n    sign = diff / abs(diff)\n    new_quantile_estimate = quantile_estimate - nudge * (sign + bias) / scale_bias\n    return new_quantile_estimate\n```\n\nThe nudges can utilize the same schema as for the median $\\frac{R(n+1)}{M(n-1)}$ for quick converge.\nBut the rubber band random walk confidence interval $\\frac{dq}{\\tilde{q_t}q_t} = 8\\sigma^2$ that we empirically derived is no longer applicable.\nOn top of the rubber band (pulling us toward the quantile estimate when we drift to far) and the random walk (because the observations are random in nature) aspects, we also now have a drift (bias) to account for.\n\nAfter repeating the experiment for different values of $q$ and refit the function we find that actually $\\frac{dq}{\\tilde{q_t}} = \\frac{4\\sigma^2}{\\tilde{q_t}}$.\nThis means that $dq$ is actually $q$-invariant and that the simplified formula gives $dq = 4\\sigma^2$ and that the $\\pm 2.5$ percentile CI $\\rightarrow M = 1536.4$ is universal.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_1.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_2.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_3.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_4.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_5.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNote that the $q = 0.01$ quantile looks a bit jumpy and $q = 0.999$ even more so. \nThat is because we have so few observations, meaning that the nudges become highly unbalanced.\nAnother factor is the fact that we used $\\pm 2.5$ percentile confidence interval for something like the 1st percentile.\nSo we accept quite a lot of noise in our estimate.\nHere is how the estimation of $q = 0.999$ looks for 10000 observations:\n\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_6.png)\n\nIf one counts the upward jumps one gets 11, which is close to the expected value of $(1-0.999) \\cdot 10000 = 10$.\nThere are some potential remedies for this, the first one is to adjust the confidence interval to match the quantile.\nThis would mean that $dq = 4\\sigma^2 = 4(\\frac{0.001}{1.96})^2 = \\frac{1}{960400}$.\nThis first approach will do the trick but it will take forever to converge.\n\nThe second potential remedy is just a sanity check. *Why are we allowing the quantile estimate to go beyond our maximum or minimum observed value?*\nThis was not a problem for the median estimation but here it is clearly an issue.\nBy bounding the estimate one gets something that looks like this.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_7.png)\n\nHowever, this leaves an issue on how to deal with the excess as it is required to balance the downward nudges.\nIf one allows the excess to carry over and nullify the opposite nudges then one ends up with flat segments like this.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_8.png)\n\nNeither of these are that satisfactory because they do not solve the jumpiness at its core.\nWe should also state that the extreme quantiles are something we will rarely need.\nLet us focus on another aspect of quantile estimation and we might be able to resove the issue in another way.\n\nThe jumpiness fundamentally comes from our frequentist approach to estimating the quantiles.\nTo approach the problem from a Bayesian point of view we can attempt correct our frequentist estimator $\\hat{q}$.\nThe $\\pm$ signs $x_i$ around our estimator $\\hat{q}$ follow a Bernoulli distribution $x_1, ..., x_n \\sim$ Bernoulli($p$). \nThe beta distribution $\\beta(a,b)$ is a conjugate prior distribution to Bernoulli($p$).\nThe posterior distibution becomes $p\\:|\\:x_1, ..., x_n \\sim \\beta(a + s,b + t)$, where $s$ are the number of successes and $t$ the total number of observations.\nThis means that $E(p\\:|\\:x_1, ..., x_n) = \\frac{a + s}{a + b + t}$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_9.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_10.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_11.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nOne way to use this is to track $|E(p\\:|\\:x_1, ..., x_n) - q|$ and use that to slow down the nudges.\nThe rescale nudges $u$ could be $u_{new} = u\\frac{|E(p\\:|\\:x_1, ..., x_n) - q|}{min(\\tilde{q},\\:1.96\\sigma)}$. \nThis means that is $E(p) = q$ then we stop nudging altogether and we have normal nudges when we are outside the confidence interval determined by our $\\sigma$ hyperparameter.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_12.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_13.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_14.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nAnd to make sure that we have not broken anything, this is what it looks for the median.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_15.png)\n\nA complete function for doing this estimation can be written as:\n\n```python\ndef estimate_quantiles(\n        data: np.array, \n        quantile: float, \n        m_initial_steps: int = 60, \n        quantile_sigma: float = 0.025\n    ) -> np.array:\n    '''\n    This function takes an array of observations, \n    treats them as streaming data and incrementally \n    estimates the q quantile. The estimate converges\n    over time as the estimate approaches the true\n    quantile.\n\n    :param data: Input data as a numpy array.\n    :param quantile: The desired quantile to estimate.\n    :param m_initial_steps: The desired number of steps until an initial guess.\n    :param quantile_sigma: A parameter specifies the confidence interval.\n    '''\n    # Derived quantities\n    q_tilde = min(quantile, 1-quantile)\n    bias = 1 - 2 * quantile\n    scale_bias = 4*q_tilde*(1-q_tilde)\n    dq = 4*(quantile_sigma/1.96)**2\n    m_initial_steps_adjusted = max(m_initial_steps, (1-(q_tilde))/(q_tilde))\n    posterior_diff_scale = min(quantile_sigma/1.96, q_tilde)\n\n    a = data[0] # Signal min\n    b = data[0] # Signal max\n    quantile_estimates = []\n    quantile_estimate = data[0] # Initialize to the first observation\n\n    total_number_of_observations = 0\n    number_of_negative_side_observations = 0\n    total_number_of_observations_prior = 1/q_tilde\n    number_of_negative_side_observations_prior = quantile*total_number_of_observations_prior\n    excess = 0\n    n = 1 # We have initialized on the first data point\n\n    for y in data[1:]:\n        n += 1\n        a = min(a, y)\n        b = max(b, y)\n        r = b - a\n        diff = quantile_estimate - y\n        sign = diff / abs(diff)\n        nudge = r * (n+1) / (m_initial_steps_adjusted * (n-1))\n    \n        if n \u003C= m_initial_steps_adjusted:\n            quantile_update = nudge * (sign + bias) / scale_bias\n            new_quantile_estimate = quantile_estimate - quantile_update\n            quantile_estimate = min(b, max(a, new_quantile_estimate))\n        else:\n            total_number_of_observations += 1\n            if sign > 0:\n                number_of_negative_side_observations += 1\n            quantile_posterior = (\n                (\n                  number_of_negative_side_observations_prior \n                  + number_of_negative_side_observations\n                ) / (\n                  number_of_negative_side_observations_prior \n                  + total_number_of_observations_prior \n                  + total_number_of_observations\n                )\n            )\n            posterior_diff = (quantile_posterior - quantile)\n            quantile_update = (\n              r \n              * dq \n              * (sign + bias) / scale_bias \n              * abs(posterior_diff) / posterior_diff_scale\n            )\n            if excess and (excess * quantile_update) > 0:\n                excess_sign = excess / abs(excess)\n                excess -= quantile_update\n                if excess_sign != excess / abs(excess):\n                    excess = 0\n                quantile_update = 0\n            new_quantile_estimate = quantile_estimate - quantile_update\n            quantile_estimate = min(b, max(a, new_quantile_estimate))\n            excess += new_quantile_estimate - quantile_estimate\n        quantile_estimates.append(quantile_estimate)\n\n    return quantile_estimates\n```\n\nAnd here is what it looks like when you fire of an array of them ($q \\in (0.1, 0.2, ..., 0.9)$):\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_16.png\",\n      alt: \"Quantile estimates for Gaussian noise\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_17.png\",\n      alt: \"Quantile estimates for Gaussian noise\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_18.png\",\n      alt: \"Quantile estimates for exponential noise\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n      {\n      src: \"/images/incremental_quantile/incremental_quantile_19.png\",\n      alt: \"Quantile estimates for exponential noise\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_20.png\",\n      alt: \"Quantile estimates for uniform noise\",\n      caption: \"Quantile estimates for uniform noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_21.png\",\n      alt: \"Quantile estimates for uniform noise\",\n      caption: \"Quantile estimates for uniform noise\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nBut what happens for non-stationary data, say that the distribution shifts?\nIn the example below we shift from $x \\sim N(0, 1)$ to $x \\sim N(2,1)$ after 1000 steps.\nThe estimates do follow along but the jaggedness is back.\nThis is because our Bayesian estimator is cumulative and it will therefore take a long time before those initial 1000 steps become neglible.\n\n![Shifted distribution](/images/incremental_quantile/incremental_quantile_22.png)\n\nA way to speed things up is to add a \"forgetting factor\" $\\lambda$ to the cumulative $s$ and $t$.\nBecause our quantile rate of change is $dq$ we define the forgetting factor $\\lambda = (1-c \\cdot dq)$.\nThis means that the forgetting matches the inertia induced by our desired confidence interval.\nSay that we want to forget 90% within one $\\frac{1}{dq}$ observations, \nsolving the expression $0.1 = (1 - c \\cdot dq)^\\frac{1}{dq} \\rightarrow c = \\frac{1 - e^{dq \\cdot log(0.1)}}{dq}$,\nwhich for our typical confidence interval means $c \\approx 2.3$.\nPutting this back into our forgetting factor simplifies the solution $\\lambda = (1- \\frac{1 - e^{dq \\cdot log(0.1)}}{dq} \\cdot dq) = e^{dq \\cdot log(0.1)}$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_23.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_24.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_25.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_26.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nThat is the gist of it.","src/content/blog/incremental-quantile-estimation.mdx","0c4d5b2a15cd81c5","incremental-quantile-estimation.mdx"]