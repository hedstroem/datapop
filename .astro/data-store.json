[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.11.0","content-config-digest","af38c04c4284c50b","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://yourusername.netlify.app\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,25,26,37,38,48,49,59,60,71,72,82,83],"incremental-median-estimation",{"id":11,"data":13,"body":20,"filePath":21,"digest":22,"legacyId":23,"deferredRender":24},{"title":14,"description":15,"pubDate":16,"tags":17},"Incremental median estimation: A frequency estimator","A frequentist ping-pong estimator for the median",["Date","2025-07-15T00:00:00.000Z"],[18,19],"statistics","streaming-data","import ImageGallery from '../../components/ImageGallery.astro';\n\nA couple of years ago I thought to myself:\n\n*\"I know how to estimate a cumulative average, but how does one estimate the median in the same way?\"*\n\nThis was an interesting question because while averages are great, they still suffer greatly from outliers. \nThis led me down a track that ended on a stack overflow thread of cumulative median esimation, and by extension, any quantile.\nThe algorithm is simple and uses a frequency approach to the problem.\n\nIn a nutshell, if the distribution is stationary then we expect a 50/50 split of higher and lower observations around our median.\nThe way we translate this into an algorithm is that higher/lower nudges our median estimate, and if the higher/lower observations are in balance then the nudges cancel out.\nIt can be coded up as this:\n\n```python\ndef update_median_estimate(\n        median_estimate: float, \n        observation: float, \n        nudge: float\n    ) -> float:\n    diff = median_estimate - observation\n    sign = diff / abs(diff)\n    new_median_estimate = median_estimate - nudge * sign\n    return new_median_estimate\n```\n\nThe median estimate can just be initialized to the first observation. This leave the nudge factor as a parameter. Here is a graph of the median estimations applied to Gaussian noise with two different nudges:\n\n![Two incremental median estimations](/images/incremental_median/inc_1.png)\n\nNote that nudge = 0.1 makes the estimate converge quicker than nudge = 0.01, but, the trade-off is that it is also more sensitive to noise. So, what is a good nudge factor?\n\nOne way is to approach is heuristically; say that we expect the estimator to approximately reach the median in M steps. \nSince the median is the 50th percentile; a worst case scenario where we start at the min or max means that we have to step 50 percentage units. \nThis means that we could choose $\\frac{max(max - median,\\: median - min)}{M}$ as the nudge factor. \nSince we do not know the median we can use the range $R = max - min$ as an upper bound $\\frac{R}{M}$. This is an extreme scenario where the median is close to either the min or max.\nFor the uniform scenario where the median lies dead in the middle between min and max it is twice as big, so then we will get to our estimate in $\\frac{M}{2}$ steps.\n\n![Incrementally estimating max and min](/images/incremental_median/inc_2.png)\n\nWith this method we have a bit more control, as can be seen in the following graph:\n\n![Comparison of different R/M nudges](/images/incremental_median/inc_3.png)\n\nEyeballing this particular example gives the M = 30, 16 steps and M = 120, 91 steps to get to the sample median. \nNow, there additional steps we can take to ensure a fair(er) nudging toward the median and it relates to $R$.\nWe do not expect our estimate of R to be close to the truth after only 3 or 10 observations, so what do we expect it to be and can we correct for that?\n\nFor $n$ samples selected uniformly on the interval [0,1) the expectation of the maximum is $\\frac{n}{n+1}$ and the minimum is $\\frac{1}{n+1}$.\nThis means the expectation of the range is\n$$\nE(R|n) = \\frac{n}{n+1} - \\frac{1}{n+1} = \\frac{n-1}{n+1}.\n$$\n\nThe intuition behind this is that $n$ samples split the line into $n+1$ sections and ordering them by size gives $1$ and $n$ as the first and final sections.\nOf course, our data is not likely uniformly distributed, but the quantiles are by definition.\nThis means that after, for example $n = 9$, we would have expected to seen 10% and 90% percentile with an 80 percentile range.\nBy $n = 99$ we will have seen from the 1st to the 99th percentile and subsequently the $n = 999$ sample gives us 0.001 to 0.999 quantile.\nThere are two take-aways from this observation.\n\nThe first is that we can invert the formula $R = \\frac{n-1}{n+1} \\rightarrow n = -\\frac{R+1}{R-1}$.\nThis is handy if we are looking for a specific quantile $q$, it corresponds to the range $R_q = 1-2q$ and by our formula $n = \\frac{1-q}{q}$. \nThis only makes sense for $q \u003C 0.5$ so for $q > 0.5$ we find its mirrored buddy $\\tilde{q} = 1 - q$. \nNow, specifically for the median this collapses to $n = 1$ but if we are going to look at any quantile then this will come in handy.\nIn essence, this estimate of $n$ will tell you when we it is likely to have seen the quantile we are estimating.\n\nThe second take away is that we can use the formula to create a better estimate $R_n$ out of our naive incremental $R$ as $R_n = \\frac{R}{\\frac{n-1}{n+1}} = \\frac{R(n+1)}{n-1}$. \nThis means that the range will scaled up to conpensate for likely unobserved range. This means that the full nudge heuristic is now $\\frac{R(n+1)}{M(n-1)}$. \nSee two examples below of the difference, one cherry-picked (note that the correction is only relevant when we need the nudges to be large) and one regular:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_median/inc_4.png\",\n      alt: \"Improved convergence example 1\",\n      caption: \"Cherry-picked example\"\n    },\n    {\n      src: \"/images/incremental_median/inc_5.png\",\n      alt: \"Improved convergence example 2\",\n      caption: \"Regular convergence\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nIt is not much but it is something.\n\nThe elephant in the room are the variations in the estimate. \nSince we are observing random noise the estimates will follow the random sequences of nudges like a random walk.\nWhat we know about random walks of $n$ steps is that the expectation $E(\\mathcal{Z}_n) = 0$ with the variance $E(\\mathcal{Z}_n^2) = n$.\nSo if we have a random walk then we would expect the standard deviation to grow like $\\sqrt{n}$, meaning that we should shrink the nudges to compensate for this effect.\nOf course, we would only apply this after $M$ steps, when we have actually expected to converge to the median.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_median/inc_6.png\",\n      alt: \"Test of random walk penalty of the nudge amplitude\",\n      caption: \"Example 1: Gaussian\"\n    },\n    {\n      src: \"/images/incremental_median/inc_7.png\",\n      alt: \"Test of random walk penalty of the nudge amplitude on an exponential distribution\",\n      caption: \"Example 2: Exponential\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nBut, our system is not a true random walk because is it still an approximation of the median, so it will have some drift towards the true distribution median. \nSay that we are targeting the $q_t$ quantile (0.5 in case of the median) with $dq$ nudges (0.1 for our example) of our estimate $q$. \nIf $q = q_t$ then we have a 50/50 of being nudged in either direction, say + happens, then we are at $q + dq$. \nThis has now shifted the probabilities as we now have a 40/60 chance in favor of going back towards $q_t$.\nIf + happens again then we are at $q + 2dq$, meaning now a 30/70 chance next time. \nThe most extreme scenario has $dq = q_t$ when $q_t = 0.5$ as it always reverses its previous step, meaning that the distribution is 0: 25%, $q_t$: 50%, 1: 25%.\n\nOf course, we want to be able to assume any $dq$. \nTo get an intution behind this we simulated using $dq = \\frac{1}{50}$ and noted that is approximated a Gaussian distribution.\nIn the limit where $dq \\rightarrow 0$ it makes sense because then the drift back towards the median is less dominant. \n![Histogram of quantile estimations](/images/incremental_median/inc_8.png)\n\nWe run more experiments with different fractions of $\\frac{dq}{q_t}$, calculate the standard deviation, resulting in:\n![Experiments logging standard deviation of quantile estimations as a function of dq/qt](/images/incremental_median/inc_9.png)\n\nThe curve fitting is easy as it is a square root of the fraction and $x = 0.5$ neatly intersects $y = 0.25$. \nThis means that empirically we have arrived at the rubber band random walk standard deviation $\\sigma = \\frac{1}{\\sqrt{8}}\\sqrt{\\frac{dq}{q_t}} = \\sqrt{\\frac{dq}{8q_t}}$. \nNote that this does not depend on $n$, which we conjectured above. \nSimiarly to what we have done before, we can shuffle expressions around to find the fraction given a target standard deviation $\\sigma$ as $\\frac{dq}{q_t} = 8\\sigma^2$.\nIf we were to calibrate a 95% confidence interval ($\\pm 1.96\\sigma$) over $\\pm 1$ percentiles ($\\rightarrow \\sigma = \\frac{0.01}{1.96}$) then $\\frac{dq}{q_t} \\approx 0.00021 \\rightarrow dq = 1/9604$ ($q_t = 0.5$).\nFor a more liberal $\\pm 2.5$ percentiles $\\frac{dq}{q_t} \\approx 0.0013 \\rightarrow dq = 1/1536.64$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_median/inc_10.png\",\n      alt: \"Example 1 of random walk confidence interval nudges\",\n      caption: \"Example 1\"\n    },\n    {\n      src: \"/images/incremental_median/inc_11.png\",\n      alt: \"Example 2 of random walk confidence interval nudges\",\n      caption: \"Example 2\"\n    },\n  ]}\n  columns={2}\n  layout=\"carousel\"\n/>\n\nThat has (kind-of) solved it! Now there are two possibilities to take this further, the first is to generalize to any quantile and the second is to adress non-stationarity.\nBut we save those for later posts :)","src/content/blog/incremental-median-estimation.mdx","9d1f38a3eebae25e","incremental-median-estimation.mdx",true,"reading-list",{"id":25,"data":27,"body":33,"filePath":34,"digest":35,"legacyId":36,"deferredRender":24},{"title":28,"description":29,"pubDate":30,"tags":31},"Reading list","Interesting books and articles",["Date","2025-07-10T00:00:00.000Z"],[32],"other","This reading list is updated sporadically.\n\n## Books\n\n- **Good to great** - *Jim Collins* \u003Cbr> \nThis book made me understand just *how* fundamental certain challenges can be to an organisation.\nBefore I used to think that if I, an employee, worked harder and spoke my mind; \nthat it would change the course of the group or the company for the better. \nAfter this book I realize that changing things to get positive development means reworking the DNA of the business.\nAnd that's not something a single contributor can do. \u003C/br>\n\n- **Build** - *Tony Fadell* \u003Cbr> \nLots of nuggets for how to think about products and organisation. \u003C/br>\n\n- **Measure what matters** - *John Doerr* \u003Cbr>\nThis book is the book on Objectives & Key Results, OKR's for short. \nI read this because a department manager wanted to introduce it to our group.\nI was intrigued how goals can progression can be tracked over different hierarchy levels.\nUnfortunately this manager did not do the theory justice nor did he actually follow up the scappy goals we set for the quarter.\u003C/br>\n\n- **Bullshit jobs** - *David Groeber* \u003Cbr> \nThe first half outlines job archetypes that passes the time without long-term productivity. \nIt is a dark and funny characterisation of modern, mostly white-collar, work.\nThe second half attempts to diagnose why we have gotten to this point.\nIt is an interesting perspective and worth the read. \u003C/br>\n\n## Take aways\n* No strategy can save a lack of culture and that no culture can save a lack of leadership.","src/content/blog/reading-list.mdx","e3988be7b3a2d769","reading-list.mdx","incremental-quantile-estimation",{"id":37,"data":39,"body":44,"filePath":45,"digest":46,"legacyId":47,"deferredRender":24},{"title":40,"description":41,"pubDate":42,"tags":43},"Extending the frequency estimator to any quantile","A frequentist ping-pong estimator extended to any quantile",["Date","2025-07-18T00:00:00.000Z"],[18,19],"import ImageGallery from '../../components/ImageGallery.astro';\n\nThis post continues from the incremental median estimation.\n\nTo estimate any quantile instead of the median one adds a bias to the sign of the nudge.\nWhat does this represent? \nConsider the case of the 90th percentile, if our estimate is accurate then we expect 90% of the incoming observations to be below the estimate.\nThis means that the 90 / 10 split must be the equilibrium between the 90% downward nudges and 10% upward nudges.\nThe bias $b = 1 - 2q$ for the 90% percentile is $b = -0.8$, this means that a positive sign $\\rightarrow 0.2$ and a negative $\\rightarrow -1.8$.\nIf we multiply these nudges by their frequency we see that they are in balance: $0.9 \\cdot 0.2 + 0.1 \\cdot -1.8 = 0.18 - 0.18 = 0$.\n\nOne thing to note is that the average absolute nudge is $0.18 + 0.18 = 0.36$, which is lower than $\\|0.5 \\cdot 1\\| + \\|0.5 \\cdot -1\\| = 1.0$ for the median.\nThis means that if we use a bias we also want to scale the size of the nudge by the inverse of $s_b = 4\\tilde{q}(1-\\tilde{q})$ where $\\tilde{q} = min(q, 1-q)$. \nThis means that an updated algorithm can be written as:\n\n```python\ndef update_quantile_estimate(\n        quantile_to_estimate: float,\n        quantile_estimate: float, \n        observation: float, \n        nudge: float\n    ) -> float:\n    bias = 1 - 2 * quantile_to_estimate\n    q_tilde = min(quantile_to_estimate, 1-quantile_to_estimate)\n    scale_bias = 4*q_tilde*(1-q_tilde)\n    diff = quantile_estimate - observation\n    sign = diff / abs(diff)\n    new_quantile_estimate = quantile_estimate - nudge * (sign + bias) / scale_bias\n    return new_quantile_estimate\n```\n\nThe nudges can utilize the same schema as for the median $\\frac{R(n+1)}{M(n-1)}$ for quick converge.\nBut the rubber band random walk confidence interval $\\frac{dq}{\\tilde{q_t}q_t} = 8\\sigma^2$ that we empirically derived is no longer applicable.\nOn top of the rubber band (pulling us toward the quantile estimate when we drift to far) and the random walk (because the observations are random in nature) aspects, we also now have a drift (bias) to account for.\n\nAfter repeating the experiment for different values of $q$ and refit the function we find that actually $\\frac{dq}{\\tilde{q_t}} = \\frac{4\\sigma^2}{\\tilde{q_t}}$.\nThis means that $dq$ is actually $q$-invariant and that the simplified formula gives $dq = 4\\sigma^2$ and that the $\\pm 2.5$ percentile CI $\\rightarrow M = 1536.4$ is universal.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_1.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_2.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_3.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_4.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_5.png\",\n      alt: \"Example of incremental quantile estimation\",\n      caption: \"Example of incremental quantile estimation\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNote that the $q = 0.01$ quantile looks a bit jumpy and $q = 0.999$ even more so. \nThat is because we have so few observations, meaning that the nudges become highly unbalanced.\nAnother factor is the fact that we used $\\pm 2.5$ percentile confidence interval for something like the 1st percentile.\nSo we accept quite a lot of noise in our estimate.\nHere is how the estimation of $q = 0.999$ looks for 10000 observations:\n\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_6.png)\n\nIf one counts the upward jumps one gets 11, which is close to the expected value of $(1-0.999) \\cdot 10000 = 10$.\nThere are some potential remedies for this, the first one is to adjust the confidence interval to match the quantile.\nThis would mean that $dq = 4\\sigma^2 = 4(\\frac{0.001}{1.96})^2 = \\frac{1}{960400}$.\nThis first approach will do the trick but it will take forever to converge.\n\nThe second potential remedy is just a sanity check. *Why are we allowing the quantile estimate to go beyond our maximum or minimum observed value?*\nThis was not a problem for the median estimation but here it is clearly an issue.\nBy bounding the estimate one gets something that looks like this.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_7.png)\n\nHowever, this leaves an issue on how to deal with the excess as it is required to balance the downward nudges.\nIf one allows the excess to carry over and nullify the opposite nudges then one ends up with flat segments like this.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_8.png)\n\nNeither of these are that satisfactory because they do not solve the jumpiness at its core.\nWe should also state that the extreme quantiles are something we will rarely need.\nLet us focus on another aspect of quantile estimation and we might be able to resove the issue in another way.\n\nThe jumpiness fundamentally comes from our frequentist approach to estimating the quantiles.\nTo approach the problem from a Bayesian point of view we can attempt correct our frequentist estimator $\\hat{q}$.\nThe $\\pm$ signs $x_i$ around our estimator $\\hat{q}$ follow a Bernoulli distribution $x_1, ..., x_n \\sim$ Bernoulli($p$). \nThe beta distribution $\\beta(a,b)$ is a conjugate prior distribution to Bernoulli($p$).\nThe posterior distibution becomes $p\\:|\\:x_1, ..., x_n \\sim \\beta(a + s,b + t)$, where $s$ are the number of successes and $t$ the total number of observations.\nThis means that $E(p\\:|\\:x_1, ..., x_n) = \\frac{a + s}{a + b + t}$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_9.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_10.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_11.png\",\n      alt: \"Example of Bayesian posterior of quantile estimate\",\n      caption: \"Example of Bayesian posterior of quantile estimate\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nOne way to use this is to track $|E(p\\:|\\:x_1, ..., x_n) - q|$ and use that to slow down the nudges.\nThe rescale nudges $u$ could be $u_{new} = u\\frac{|E(p\\:|\\:x_1, ..., x_n) - q|}{min(\\tilde{q},\\:1.96\\sigma)}$. \nThis means that is $E(p) = q$ then we stop nudging altogether and we have normal nudges when we are outside the confidence interval determined by our $\\sigma$ hyperparameter.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_12.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_13.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_14.png\",\n      alt: \"Using posterior of quantile estimate to slow down convergence\",\n      caption: \"Using posterior of quantile estimate to slow down convergence\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nAnd to make sure that we have not broken anything, this is what it looks for the median.\n![Example of incremental quantile estimation](/images/incremental_quantile/incremental_quantile_15.png)\n\nA complete function for doing this estimation can be written as:\n\n```python\ndef estimate_quantiles(\n        data: np.array, \n        quantile: float, \n        m_initial_steps: int = 60, \n        quantile_sigma: float = 0.025\n    ) -> np.array:\n    '''\n    This function takes an array of observations, \n    treats them as streaming data and incrementally \n    estimates the q quantile. The estimate converges\n    over time as the estimate approaches the true\n    quantile.\n\n    :param data: Input data as a numpy array.\n    :param quantile: The desired quantile to estimate.\n    :param m_initial_steps: The desired number of steps until an initial guess.\n    :param quantile_sigma: A parameter specifies the confidence interval.\n    '''\n    # Derived quantities\n    q_tilde = min(quantile, 1-quantile)\n    bias = 1 - 2 * quantile\n    scale_bias = 4*q_tilde*(1-q_tilde)\n    dq = 4*(quantile_sigma/1.96)**2\n    m_initial_steps_adjusted = max(m_initial_steps, (1-(q_tilde))/(q_tilde))\n    posterior_diff_scale = min(quantile_sigma/1.96, q_tilde)\n\n    a = data[0] # Signal min\n    b = data[0] # Signal max\n    quantile_estimates = []\n    quantile_estimate = data[0] # Initialize to the first observation\n\n    total_number_of_observations = 0\n    number_of_negative_side_observations = 0\n    total_number_of_observations_prior = 1/q_tilde\n    number_of_negative_side_observations_prior = quantile*total_number_of_observations_prior\n    excess = 0\n    n = 1 # We have initialized on the first data point\n\n    for y in data[1:]:\n        n += 1\n        a = min(a, y)\n        b = max(b, y)\n        r = b - a\n        diff = quantile_estimate - y\n        sign = diff / abs(diff)\n        nudge = r * (n+1) / (m_initial_steps_adjusted * (n-1))\n    \n        if n \u003C= m_initial_steps_adjusted:\n            quantile_update = nudge * (sign + bias) / scale_bias\n            new_quantile_estimate = quantile_estimate - quantile_update\n            quantile_estimate = min(b, max(a, new_quantile_estimate))\n        else:\n            total_number_of_observations += 1\n            if sign > 0:\n                number_of_negative_side_observations += 1\n            quantile_posterior = (\n                (\n                  number_of_negative_side_observations_prior \n                  + number_of_negative_side_observations\n                ) / (\n                  number_of_negative_side_observations_prior \n                  + total_number_of_observations_prior \n                  + total_number_of_observations\n                )\n            )\n            posterior_diff = (quantile_posterior - quantile)\n            quantile_update = (\n              r \n              * dq \n              * (sign + bias) / scale_bias \n              * abs(posterior_diff) / posterior_diff_scale\n            )\n            if excess and (excess * quantile_update) > 0:\n                excess_sign = excess / abs(excess)\n                excess -= quantile_update\n                if excess_sign != excess / abs(excess):\n                    excess = 0\n                quantile_update = 0\n            new_quantile_estimate = quantile_estimate - quantile_update\n            quantile_estimate = min(b, max(a, new_quantile_estimate))\n            excess += new_quantile_estimate - quantile_estimate\n        quantile_estimates.append(quantile_estimate)\n\n    return quantile_estimates\n```\n\nAnd here is what it looks like when you fire of an array of them ($q \\in (0.1, 0.2, ..., 0.9)$):\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_16.png\",\n      alt: \"Quantile estimates for Gaussian noise\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_17.png\",\n      alt: \"Quantile estimates for Gaussian noise\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_18.png\",\n      alt: \"Quantile estimates for exponential noise\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n      {\n      src: \"/images/incremental_quantile/incremental_quantile_19.png\",\n      alt: \"Quantile estimates for exponential noise\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_20.png\",\n      alt: \"Quantile estimates for uniform noise\",\n      caption: \"Quantile estimates for uniform noise\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_21.png\",\n      alt: \"Quantile estimates for uniform noise\",\n      caption: \"Quantile estimates for uniform noise\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nBut what happens for non-stationary data, say that the distribution shifts?\nIn the example below we shift from $x \\sim N(0, 1)$ to $x \\sim N(2,1)$ after 1000 steps.\nThe estimates do follow along but the jaggedness is back.\nThis is because our Bayesian estimator is cumulative and it will therefore take a long time before those initial 1000 steps become neglible.\n\n![Shifted distribution](/images/incremental_quantile/incremental_quantile_22.png)\n\nA way to speed things up is to add a \"forgetting factor\" $\\lambda$ to the cumulative $s$ and $t$.\nBecause our quantile rate of change is $dq$ we define the forgetting factor $\\lambda = (1-c \\cdot dq)$.\nThis means that the forgetting matches the inertia induced by our desired confidence interval.\nSay that we want to forget 90% within one $\\frac{1}{dq}$ observations, \nsolving the expression $0.1 = (1 - c \\cdot dq)^\\frac{1}{dq} \\rightarrow c = \\frac{1 - e^{dq \\cdot log(0.1)}}{dq}$,\nwhich for our typical confidence interval means $c \\approx 2.3$.\nPutting this back into our forgetting factor simplifies the solution $\\lambda = (1- \\frac{1 - e^{dq \\cdot log(0.1)}}{dq} \\cdot dq) = e^{dq \\cdot log(0.1)}$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_23.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_24.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_25.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    },\n    {\n      src: \"/images/incremental_quantile/incremental_quantile_26.png\",\n      alt: \"Quantile estimations of shifting distributions\",\n      caption: \"Quantile estimations of shifting distributions\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nThe code additions are as follows.\n```python\n...\nforgetting_factor = np.exp(dq * np.log(0.1))\n...\nfor y in data[1:]:\n  ...\n  if n \u003C= m_initial_steps_adjusted:\n    ...\n  else:\n    total_number_of_observations = forgetting_factor*total_number_of_observations\n    number_of_negative_side_observations = forgetting_factor*number_of_negative_side_observations\n    total_number_of_observations += 1\n    if sign > 0:\n      number_of_negative_side_observations += 1\n    ...\n```\n\nOne added bonus to this framework is that one can extract the $\\delta_q = E(p\\:|\\:x_1, ..., x_n) - q$ and sum $\\sum_{q} \\delta_q$ as a measure of distribution deviation.\nThis aggregated signal can be used to indicate that the estimator is converging.\nIt can also be treated as an indicator for change but there are better ways to do that, plus\nas one can see in the example in the example below, it does not capture widening or narrowing of a distribution.\n\n![Shifted distribution](/images/incremental_quantile/incremental_quantile_27.png)\n\nThe topic of change detection warrants its own set of posts.\nIt is for example possible to build one on top of a quantile estimator.\nAnything that can be used as a reference for what the distribution used to look like.","src/content/blog/incremental-quantile-estimation.mdx","d0d91b479a2b543c","incremental-quantile-estimation.mdx","incremental-quantile-estimation-2",{"id":48,"data":50,"body":55,"filePath":56,"digest":57,"legacyId":58,"deferredRender":24},{"title":51,"description":52,"pubDate":53,"tags":54},"Focusing on Bayesian validation for alternative incremental quantile estimation","Shaky foundations of an alternative approach to incremental quantiles",["Date","2025-07-28T00:00:00.000Z"],[18,19],"import ImageGallery from '../../components/ImageGallery.astro';\n\nWhile finishing up the last post on incremental quantile estimation got intrigued by the Bayesian estimator.\nIt can basically verify any method, so the question is if one can make a simpler (and more robust) approach than the nudger?\nTo strengthen my intuition I ran an experiment where one takes the first 10 observations and estimate their quantiles.\nThe quantile we measure our error towards is $q = 0.9$.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_1.png)\n\nNotice that the 0 error line is between the yellow observation 5 (=1.64) and blue observation 0 (=1.01).\nThis is also where we find the red sample quantile.\nNow, what to do with this information?\nEither one can start throwing away the lousy estimates for better ones (like, in between the blue and yellow), or one can be smarter about their initial selection.\n\nTo address the latter.\nRemember from the previous posts that one expects to see the range $R$ after $n$ observations $R = (n-1)/(n+1)$.\nThis also meant that one had to wait until $n = \\frac{1-\\tilde{q}}{\\tilde{q}}$ where $\\tilde{q} = min(q, 1-q)$\nto match the expected range of observed quantiles. \nSo for our example $q = 0.9 \\rightarrow n = 9$ we basically already did this.\nHowever, with this reasoning we should pick our first observation as our median estimate and that does not always work out.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_2.png\",\n      alt: \"Using the first observation to approximate the median\",\n      caption: \"Using the first observation to approximate the median\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_3.png\",\n      alt: \"Using the first observation to approximate the median\",\n      caption: \"Using the first observation to approximate the median\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_4.png\",\n      alt: \"Using the first observation to approximate the median\",\n      caption: \"Using the first observation to approximate the median\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nSo there is clearly some value in having more than 1 observation in order to learn something more about the spread of quantiles.\nProbably the more the merrier but we are interested in small solutions, so $n = $ 2, 3, 5 or even 10 would be nice.\nTo make our task easier to manage we can start by giving ourselves a calibration window $w$ where we get to pick our $n$ observations.\n\nOne thing to keep track of if $w \u003C \\frac{1-\\tilde{q}}{\\tilde{q}}$. \nIn that case we know that our estimate is the maximum or minimum observation times a factor $\\delta$ for the potentially missing range $\\delta = \\frac{R_q}{R_w} = (1-2\\tilde{q})\\frac{w+1}{w-1}$.\nFor the case where we have seen the observed the range we can sort the observations and pick out the quantile estimate from there.\nOf course, that would require us to buffer the observations, in which case we might as well call $n = w$.\nThen we pick the ith indexed observations in a sorted list which matches our quantile (for $q = 0.9$ we pick the 9th out of 10), then we pick $\\pm 1$ indices around it and cram out estimates into that range.\nThis does not work out that nice as can be seen below:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_5.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Using the first n observations to naively estimate a quantile range\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_6.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Using the first n observations to naively estimate a quantile range\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_7.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Using the first n observations to naively estimate a quantile range\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nThis is a good example where trying to be clever can be severly punishing, as in example 2.\nIf we put that idea on pause and instead consider another dimension of our estimate, *how long does it take until we can trust it?*\nFor both the median and $q = 0.9$ it seems to take at least 100 observations before the Bayesian estimate stabilizes.\nThis is also a limitation to be way of because it means that we can not shuffle our observations around and reseting the Bayesian estimates too often.\n\nAnother fact that to note is the **correlation** between the quantile estimation errors. \nThis is because they are estimated using the same random sequence of observations.\nFor two observations $y_1, y_2$ that are close to each other $\\Delta y = |y_2-y_1| \u003C\u003C 1$ this means that differentiating between the two would require an observation that lies between them.\nThis means that one probably want to randomly drop observations to avoid correlation in the estimates and that one wants to estimate relatively distinct observations.\n\nFor a dropout probability of 50%:\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_8.png)\n\nSo, back to the algorithm crafting. \nIf we start with the first $M$ observations that we did before (better not try to be clever).\nThe next step is after $W$ observations to reset the observations based on what we have learned.\nIf we take a look at the quantile estimation errors vs the observation values at $n = 100$ we have the following scatter plot.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_9.png)\n\nFor this example we do not even have an observation on either side of the 0.\nThis means that the method we use need to extrapolate.\nA linear regression will do a decent job for this example.\nHowever, even this demo distribution (Gaussian) is not linear as it is only linear-ish.\nTherefore multiple shooting is required to narrow down the interval of shooting.\n\nOne important choice will be the how to take the linear regression $y = kx + m$ and use that information.\nThe most likely position of the quantile is the intercept $m$ but only one estimate can have that position.\nWe also know that we do not want to agressively cluster the estimates either.\nA fair initial assumption is to assume some $\\pm \\Delta q$ interval, like $\\pm 0.1$ quantiles or $\\pm 0.05$ quantiles.\nThis means that for $W = 100$, $M = 10$, $\\Delta q = 0.1$ every 100 observations we will reset the estimates to $(-0.1, -0.0\\bar{7}, -0.0\\bar{5}, ..., 0.1)$ quantile errors according to a linear fit of error and observations.\nThe result for a few different $\\Delta q$ can be seen below.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_10.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Interval +/-0.1\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_11.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Interval +/-0.05\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_12.png\",\n      alt: \"Using the first $n$ observations to estimate a quantile range\",\n      caption: \"Interval +/-0.01\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNote that only $\\pm 0.1$ quantiles does not collapse to a single value unlike $\\pm 0.05$ and $\\pm 0.01$ quantiles.\nAnother observation is that $\\Delta q = 0.1$ collapses to a single value for the median, so that property is dependent on $q$.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_13.png)\n\nHere is what its corresponding linear fit looks like. \nIt basically collapses into a ball where quantiles and estimates no longer correlate.\nThis is a symptom of what we disussed before, longer periods are needed to differentiate between the points.\nIt is also information if we do not get a positive slope, because by definition it should always be positive.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_14.png)\n\nBy doubling the $W$ we give the observations more time to converge, avoiding collapse for the median.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_15.png\",\n      alt: \"...\",\n      caption: \"Doubling the reset interval W after every reset.\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_16.png\",\n      alt: \"...\",\n      caption: \"It corresponding linear fit at 1000 observations\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nAfter experimenting with $\\Delta q$ I thought of an even simpler way of using the result of the linear model.\nIf all we care about is the intercept $m$, then lets just make our new batch of estimates $y_{i,new} = (1-\\gamma) y_{i,old} + \\gamma m$ where $y_i$ is the $i$th observation.\nThis keeps the behavior of the median the same and also makes $q = 0.9$ more sensible. See the difference below for the same data.\nIn this example $\\gamma = 0.5$, which seems to be a good compromise between converge and keeping the spread of estimates.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_17.png\",\n      alt: \"...\",\n      caption: \"Linear model\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_18.png\",\n      alt: \"...\",\n      caption: \"Smoothing model\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nOf course, this method runs the risk of getting stuck below or over the estimate. \nThis tends to happen if all the $M$ initializing observations are on one side of the quantile $q$ we are trying to estimate. \nIf our estimations are biased (one-sided), we should correct for it as $y_{i,new} = (1-\\gamma) y_{i,old} + \\gamma m - \\gamma bias$,\nwhere the $bias = \\frac{1}{M}\\sum_{i=1}^{M} (q - y_i)$ and $y_i$ are estimates. See the difference below.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_19.png\",\n      alt: \"...\",\n      caption: \"Without bias\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_20.png\",\n      alt: \"...\",\n      caption: \"With bias\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNow the final step is to actually put a number of what we think the quantile is.\nThere are a few simple options like the intercept $m$ that is used to update the estimates.\nAnother option is to take the estimate with the lowest quantile estimation error.\nYet another option is to do the linear regression after every observation and mix it with the intercept as $m = (1 - \\frac{c_W}{W}) m_{reset} + \\frac{c_W}{W} m_{online}$, \nwhere $c_W$ is the counter towards the reset window $W, m_{reset}$ is the intercept that determines the estimates and $m_{online}$ is always calculated.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_21.png\",\n      alt: \"...\",\n      caption: \"q = 0.9 intercept as quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_22.png\",\n      alt: \"...\",\n      caption: \"q = 0.5 intercept as quantile estimate\"\n    },\n      {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_23.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 intercept as quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_24.png\",\n      alt: \"...\",\n      caption: \"q = 0.9 lowest quantile estimation error estimate as quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_25.png\",\n      alt: \"...\",\n      caption: \"q = 0.5 lowest quantile estimation error estimate as quantile estimate\"\n    },\n      {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_26.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 lowest quantile estimation error estimate as quantile estimate\"\n    },\n      {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_27.png\",\n      alt: \"...\",\n      caption: \"q = 0.9 lowest mix method as quantile estimate\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_28.png\",\n      alt: \"...\",\n      caption: \"q = 0.5 lowest mix method as quantile estimate\"\n    },\n      {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_29.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 lowest mix method as quantile estimate\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nOut of the three the mix method provides a seemingly stable output.\nNote that all methods have troubles converging when $q \\geq 0.9$.\nThis is due to the design of our algorithm.\nWe are not making sure to include our maximum and minimum in our estimation ranges. Instead we are relying on our initial $M$.\nWe are also using pretty aggressive reset windows, we would not expect to see a $q = 0.999$ observation within $W = 100, 200, ...$.\nIt is also possible to play around with the bias correction and convert it to PI-regulator, meaning that we accumulate biases over time.\nAnother way would also be to amplify the bias factor when $q$ is close to 0 or 1, effectively adding a P-factor. \n\nA few combined steps include:\n* Track the min $a$ and max $b$.\n* Make sure the slope $k \\geq 0$.\n* Scale the bias by $k$ as $y_{i,new} = (1-\\gamma) y_{i,old} + \\gamma (m -  \\cdot k \\cdot bias)$.\n* Calculate the $\\Delta y_{overstep} = max(0, max_i(y_{i,new}) - b)$ and $\\Delta y_{understep} = min(0, min_i(y_{i,new}) - a)$.\n* Adjust the estimates by any overstep or understep $y_{i,adj} = y_{i,new} - \\Delta y_{overstep} - \\Delta y_{understep}$.\n\nThis is basically a sanity check (are we outside of bounds) and using the slope which should be the correct P-factor.\nIt is not a perfect solve as can be observed by the third example in the graphs below.\nThe weirdness seems to appear if we are unfortunate enough to put all of our estimates above the true quantile.\nThis can potentially be mitigated but similarly to the previous posts, lets reserve extreme quantiles for later.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_30.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 with proposed k * bias and over/understep correction\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_31.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 with proposed k * bias and over/understep correction\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_32.png\",\n      alt: \"...\",\n      caption: \"q = 0.99 with proposed k * bias and over/understep correction\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nLet's write down the algorithm so far:\n\n```python\ndef linear_regression(x, y):\n    n = len(x)\n    xm = sum(x) / n\n    ym = sum(y) / n\n    variance = sum((x - xm)**2)\n    covariance = sum((x - xm)*(y - ym))\n    slope = covariance / variance\n    intercept = ym - slope * xm\n    return slope, intercept\n\ndef estimate_quantiles(\n        data: np.array, \n        quantile: float,\n        number_of_estimators: int = 10,\n        dropout_probability: float = 0.5,\n        reset_window: int = 100,\n        reset_window_scaling: float = 2,\n        update_factor: float = 0.5\n    ) -> np.array:\n    '''\n    This function takes an array of observations, \n    treats them as streaming data and incrementally \n    estimates the q quantile. The estimate converges\n    over time as the estimate approaches the true\n    quantile.\n\n    :param data: Input data as a numpy array.\n    :param quantile: The desired quantile to estimate.\n    :param m_initial_steps: The desired number of steps until an initial guess.\n    :param quantile_sigma: A parameter specifies the confidence interval.\n    '''\n\n    q_tilde = min(quantile, 1-quantile)\n    a = min(data[:number_of_estimators]) # Signal min\n    b = max(data[:number_of_estimators]) # Signal max\n    estimates = data[:number_of_estimators]\n\n    n = np.zeros(number_of_estimators)\n    s = np.zeros(number_of_estimators)\n    alpha = quantile * 1/q_tilde * np.ones(number_of_estimators)\n    beta = (1-quantile) * 1/q_tilde * np.ones(number_of_estimators)\n\n    quantile_estimates = []\n    quantile_estimate = 0\n    counter = 0\n    reset_window_scale = 1\n    reset_intercept = 0\n    reset_slope = 0\n\n    for y in data[number_of_estimators:]:\n        a = min(a, y)\n        b = max(b, y)\n        dropout = 1*(np.random.rand(number_of_estimators) \u003C dropout_probability)\n        n += dropout*1\n        s += dropout*1*((estimates - y) >= 0)\n        quantile_posterior_expected_value = (alpha + s)  / (alpha + beta + n)\n        posterior_expected_value_diff = (quantile_posterior_expected_value - quantile)\n\n        slope, intercept = linear_regression(posterior_expected_value_diff, estimates)\n        slope = max(0, slope)\n        bias = sum(posterior_expected_value_diff)/number_of_estimators\n\n        if counter >= reset_window_scale * reset_window:\n            reset_intercept = intercept\n            reset_slope = slope\n            estimates = (\n              (1 - update_factor) * estimates \n              + update_factor * reset_intercept \n              - update_factor * bias * reset_slope\n            )\n            overstep = max(0, max(estimates) - b)\n            understep = min(0, min(estimates) - a)\n            estimates = estimates - overstep - understep\n            n = np.zeros(number_of_estimators)\n            s = np.zeros(number_of_estimators)\n            reset_window_scale = reset_window_scaling*reset_window_scale\n            counter = 0\n\n        if reset_window_scaling == 1:\n            quantile_estimate = intercept\n        else:\n            reset_alpha = counter/(reset_window*reset_window_scale)\n            quantile_estimate = (1-reset_alpha) * reset_intercept + reset_alpha * intercept\n\n        counter += 1\n        quantile_estimates.append(quantile_estimate)\n      \n    return quantile_estimates\n```\n\nHere are a few examples of running it for multiple quantiles:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_33.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_34.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for Gaussian noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_35.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_36.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for exponential noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_37.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for uniform noise\"\n    },\n    {\n      src: \"/images/incremental_quantile_2/incremental_quantile_2_38.png\",\n      alt: \"...\",\n      caption: \"Quantile estimates for uniform noise\"\n    }\n  ]}\n  layout=\"carousel\"\n/>\n\nNow, if one designing this algorithm for monitoring a distribution like this then one might can probably share estimators.\nThat can be the topic of another post together with other methods for estimating distributions.\nOne question we looked into in the previous post about incremental quantile estimation was how the estimator behaves when the distribution shifts.\nThe short answer is that it can not follow it. \nThe estimator breaks and even if we add in a forgetting factor we have the problem that this approach does not fare well when the observations are one sided.\nThis is what happens when our distribution shifts far, like in the second shift in the example below. \nAll estimates are far beyond the new maximum and thus the linear fit will be uninformative.\n\n![...](/images/incremental_quantile_2/incremental_quantile_2_39.png)\n\nI will continue to meditate on this approach and how it could be robustified.","src/content/blog/incremental-quantile-estimation-2.mdx","fa2536f9c02b38b7","incremental-quantile-estimation-2.mdx","nonlinear-linear-regression",{"id":59,"data":61,"body":67,"filePath":68,"digest":69,"legacyId":70,"deferredRender":24},{"title":62,"description":63,"pubDate":64,"tags":65},"Nonlinear linear regression","Extending whitebox modeling",["Date","2025-07-31T00:00:00.000Z"],[66],"machine-learning","import ImageGallery from '../../components/ImageGallery.astro';\n\nConsider linear regression $y(\\textbf{x}) = \\sum_{i = 0}^{m} (w_i x_i) + b$ \nwhere $y \\in \\rm I\\!R$ is the output, \n$\\textbf{x} \\in \\rm I\\!R^m$ the input with $m$ features,\n$w_i$ is the weight corresponding to the $i$th feature and $b$ is the intercept.\nThe neat thing about this model is that it has a closed form maximum likelihood solution.\n\nThere is also the extention to the generalized linear model where $E(y|\\textbf{x}) = \\mu = g^{-1}(\\textbf{x}\\beta)$,\nthe way to read this is that $y$ is sampled from some distribution $y \\sim Y$ (does not have to be Gaussian) and it's\nexpected value $\\mu$ depends on some link function $g$ of a linear combinations of the inputs $\\textbf{x}\\beta$.\nThese types of models generally don't have closed forms and require iterative maximum likelihood or monte carlo methods.\n\nBut where we are going we need yet another extension to generalized additive models where $g(E(y|\\textbf{x})) = \\sum_{i = 0}^{m} f_i(x_i) + b$.\nNow instead of using linear weights we allow for any smooth transformation $f_i$ of the input signals $x_i$.\nIf one has $g$ as the identity function then this is simply called additative models $y = \\sum_{i = 0}^{m} f_i(x_i) + b$.\n\nA few years ago I implemented additative models myself, only to discover that someone already proposed it in 1981.\nThat's machine learning for you. While researching this topic I also found this [article of comparative setups of\nlinear models, generalized linear models, additative models and generalized additative models](https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf).\nBasically, these models will never beat your average black box model (the article compares to Random Forests TM).\nBut they do offer some unique interpretability.\n\nMy implementation was in Pytorch and used 1 layer multilayer perceptrons instead of splines or trees.\nThe model fitting was not done using backfitting but rather gradient descent.\n\n```python\ndef create_weights(n_in, n_out):\n    scale = ((6 / (n_in + n_out))**0.5)\n    return scale*2*(torch.rand(n_in, n_out)-0.5)\n\nclass univariate_nllr_model(nn.Module):\n    def __init__(\n            self, \n            n_input, \n            n_hidden = 10, \n            activation_function = nn.Tanh(),\n            loss_function = nn.MSELoss()\n        ):\n        super().__init__()\n        self.hidden_nodes_weights = nn.Parameter(create_weights(n_input, n_hidden))\n        self.hidden_nodes_bias = nn.Parameter(create_weights(n_input, n_hidden))\n        self.aggregration_weights = nn.Parameter(create_weights(n_input, n_hidden))\n        self.linear = nn.Linear(n_input, 1)\n        self.activation_function = activation_function\n        self.loss_function = loss_function\n        self.double()\n        \n    def f(self, x):\n        hidden = self.activation_function(\n            x.unsqueeze(2) * self.hidden_nodes_weights + self.hidden_nodes_bias\n        )\n        return (hidden * self.aggregration_weights).sum(-1)\n\n    def forward(self, x):\n        return self.linear(self.f(x)).squeeze(1)\n    \n    def evaluate(self, x, y, l2_last_layer = 1e-3):\n        fx = self.f(x)\n        yp = self.linear(fx).squeeze(1)\n        loss = self.loss_function(y, yp)\n        if l2_last_layer:\n            loss += l2_last_layer * ((self.linear.weight**2).sum() + (self.linear.bias**2).sum())\n        return loss\n```\n\nFor a toy problem where $x_{1,2,3,4,5} \\sim N(0,1)$ and $y = \\frac{5}{1 + e^{-2x_1}} + 3e^{-x_2^2}$ one gets the following $w_i f_i(x_i)$:\n![...](/images/nonlinear_linear_regression/nllr_1.png)\n\nOf course these are nice smooth functions that fits the tanh activation function well. \n\nThis is what ReLU looks like ($n_{hidden} = 10$):\n![...](/images/nonlinear_linear_regression/nllr_2.png)\n\nRegardless, playing around with the MLP parameters (layer size, depth, activation functions, regularization) will determine the ability of the $f_i(x_i)$ transformation.\nOne can also regularize $f_i(x_i)$ to make sure that uninteresting features (2, 3, 4 in this example) becomes close to zero.\n\nIf we take a real dataset, like the concrete compressive strength dataset [available here](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength).\nThe goal is to predict the strength as measured in MPa using different blends of concrete. \nUsing 32 hidden neurons in 1 layer to learn the nonlinearities yields for different activation functions:\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_3.png\",\n      alt: \"...\",\n      caption: \"Tanh activation function, training MSE\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_4.png\",\n      alt: \"...\",\n      caption: \"Tanh activation function, learned nonlinearities\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_5.png\",\n      alt: \"...\",\n      caption: \"ReLU activation function, training MSE\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_6.png\",\n      alt: \"...\",\n      caption: \"ReLU activation function, learned nonlinearities\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_7.png\",\n      alt: \"...\",\n      caption: \"Sin activation function, training MSE\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_8.png\",\n      alt: \"...\",\n      caption: \"Sin activation function, learned nonlinearities\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_9.png\",\n      alt: \"...\",\n      caption: \"Binary step activation function, training MSE\"\n    },\n    {\n      src: \"/images/nonlinear_linear_regression/nllr_10.png\",\n      alt: \"...\",\n      caption: \"Binary step activation function, learned nonlinearities\"\n    },\n  ]}\n  layout=\"carousel\"\n/>\n\nFirst of all, one would typically not use sine or binary step as activation functions in neural nets.\nThey do yield some interesting observations. The sine seems to overfit and come up with some interleaving effects. \nThe binary finds almost nothing but two effects.\n\n| Activation | MSE | No effect fields | Comment |\n| -------- | ------- |  ------- | ------- |\n| tanh | 53.3 | Coarse aggregate, fine aggregate, water, fly ash | Increasing cement generally makes it stronger. Superplasticizer and age both have a tapering effect. Slag has a minor effect. |\n| ReLU | 48.6 | - | Same cement, age and slag observations from tanh. Excessive water has a negative effect. Superplasticizer appears to even have a negative effect after its initial usefulness. |\n| sin | 31.2 | - | Age, superplasticizer and water behave the same as ReLU. Cement is a shared observation but aggregates are slight down in excessive amounts. Slag also seems to have no effect on the whole.|\n| Binary step | 159.2 | All except age and superplasticizer | When we remove nuanced expressions from the model and study on-off effects one only captures effects from superplasticizer and age. |\n\nThe linear(ish) observations seem to be:\n* Higher cement content means a stronger concrete.\n* Avoid excessive superplasticizer and water.\n* 30 to 90 days to cure.\n\nSurely there are cross-effects and trade-offs that one does when balancing a recipe for concrete.\nFor example more cement does require more water and superplasticizer is make the concrete workable for higher cement content in the absence of water.\nThis type of model does not answer that unless we add those factors explicitly.\nBut if where are truly interested in learning more about concrete then there are better models.\n\nComing back to the topic of nonlinear linear models.\nThe downside of my approach is that there are no confidence bounds...\nThat said, as I was writing this, then I looked closer at the article from before and found that they do theirs using bootstrapping.\nSo then it does not really count.\n\nHowever, when working with generalized linear models one can work with proper distributions in a Bayesian fashion.\nWe do lose that feature nonlinearity but itstead we gain how the linear coefficients distribute.\n\nOf course, another way to approach these problem is to take a truly black-box model such as a neural network or boosted trees,\nand try to figure out which patterns in the data it reacts to.\nThat is where one would turn to sensitivity analysis.","src/content/blog/nonlinear-linear-regression.mdx","1fb87587299579c8","nonlinear-linear-regression.mdx","sensitivity-analysis",{"id":71,"data":73,"body":78,"filePath":79,"digest":80,"legacyId":81,"deferredRender":24},{"title":74,"description":75,"pubDate":76,"tags":77},"Sensitivity analysis & feature importance using gradients","Understanding SHAP and when to use it",["Date","2025-08-03T00:00:00.000Z"],[66],"import ImageGallery from '../../components/ImageGallery.astro';\n\nIn the post on nonlinear linear regression we mentioned the topic of sensitivity analysis.\nThis made me want to try a method that I have been thinking about but never tried.\nInstead of using SHAP (more on that later) or black-box probers, why not use forward differentiation?\nOf course, this is maily handy for models implemented in an automatic differentiation framework like Pytorch.\n\nWriters update: \n\u003Cbr>*Turns out that SHAP has a gradient-based explainer, we will compare and reason about it* :)\u003C/br>\n\nIf we implemented a simple, dense univarate neural network in Pytorch like this:\n\n```python\nfrom torch import nn\n\nclass univariate_dense_net(nn.Module):\n    def __init__(\n            self, \n            n_input, \n            n_hidden = [16, 16], \n            activation_function = nn.Tanh()\n        ):\n        super().__init__()\n        self.nodes_per_layer = [n_input] + n_hidden + [1]\n        self.layers = nn.ParameterList()\n        depth = len(n_hidden)\n        for i in range(depth+1):\n            self.layers.append(nn.Linear(self.nodes_per_layer[i], self.nodes_per_layer[i+1]))\n            if i \u003C depth:\n                self.layers.append(activation_function)\n        self.double()\n        \n    def forward(self, x):\n        y = x\n        for layer in self.layers:\n            y = layer(y)\n        return y\n```\n\nAnd throw a problem at it like this\n $x_{1,2,3,4,5,6,7} \\sim N(0,1)$ and $y = \\frac{5}{1 + e^{-2x_1}} + 3e^{-x_2^2} + x_3x_4$.\n\n![...](/images/sensitivity/sens_1.png)\n\nGiven that we store our $x_i$ in a matrix $X$ and targets in a column vector $y$. \nThen one can calculate the gradient of the model output by $\\pm 1$ unit with regards to $x_i$ using`torch.autograd.grad`for Pytorch.\nOne note is that one needs to copy the input data and make it require a gradient for this calculation to work.\n\n```python\nimport torch\nX_grad = X.clone().requires_grad_()\ngrad = torch.autograd.grad(\n    model.forward(X_grad), \n    X_grad, \n    torch.ones_like(y)\n)\n```\n\nWith this we have received an $n \\times m$ matrix where each element is $\\frac{\\partial{y}}{\\partial{x_i}}(k)$,\nwhere $k$ is our row index.\nUsing this derivative of the model at every observation, we can look at things like the average power per feature.\nThis hints at which of the signals the model things are strong contender for nudging the target.\n\n![...](/images/sensitivity/sens_2.png)\n\nIt is a simple feature importance but it has done the trick.\nNote that there was a cross-effect between $x_3x_4$, \nthe question is if that can be found when analyzing the gradients.\nBy doing principal component analysis (PCA) on the gradients we find four components.\nThe rest PC5, PC6, PC7 just capture noise.\n\n![...](/images/sensitivity/sens_3.png)\n\nAnd by looking at the weights of the components we find that PC1 belongs to $x2$, PC4 belongs to $x_1$ but PC2 and PC3 references both $x_3$ and $x_4$.\nThe first time (PC2) they both have positive weight and the second time (PC3) they positive-negative weights. \nPerhaps this hints at the underlying relationship.\nMaybe it does not specifically.\n\n![...](/images/sensitivity/sens_4.png)\n\nHere are the corresponding SHAP values using the official library:\n```python\nimport shap\nshap_values = shap.GradientExplainer(model, data).shap_values(eval_data)\n```\n\n![...](/images/sensitivity/sens_5.png)\n\nNote that the SHAP values seems to think that $x_1$ is more important than $x_2$, \nwhen the size of the gradients say the opposite.\nThe SHAP values are indeed correct when it comes to feature importance.\nThe MSE increases to 70 when $x_1$ is blocked out while it only increases to 30 for $x_2$.\n\n\u003CImageGallery \n  images={[\n    {\n      src: \"/images/sensitivity/sens_6.png\",\n      alt: \"...\",\n      caption: \"x1 excluded from training\"\n    },\n    {\n      src: \"/images/sensitivity/sens_7.png\",\n      alt: \"...\",\n      caption: \"x2 excluded from training\"\n    },\n  ]}\n  layout=\"carousel\"\n/>\n\nSo feature importance != sensitivity.\nThe gradients tell us something about how to probe the model to change the output.\nIn this case prodding $x_2$ has a better marginal effect than $x_1$.\nHowever, it does not (yet) tell us how removing this parameter would hurt the model.\nFor $x_5, x_6, x_7$ we have learned that lack of sensitivity can be interpreted as feature uninportance.\n\nTo my understanding the SHAP gradient explainer starts by sampling a subset of the data, called the baseline observations $x'$.\nTo calculate the SHAP value $\\phi_i$ for feature $i$ of observation $x_i[k]$ \none calculates the path integral of the gradient from $x_i[k]$ to each baseline $x_i'$\nand then averages over the baselines.\nThe points along the path are parametrized using $\\alpha$ as $x_{\\alpha}'[k] = x' - \\alpha (x[k] - x')$\n$$$\n\\phi_i[k] = (x_i[k] - x'_i)\\int_{\\alpha = 0}^{1} \\frac{\\partial f(x_{\\alpha}'[k])}{\\partial x_i} d\\alpha.\n$$$\n\nTo make life easy for our implementation we can approximate the integral $\\int_0^1 f(x) dx \\approx \\sum_{i=0}^h \\frac{1}{h}f(x+\\frac{i}{h})$.\nThis gives us our own homebrew SHAP values.\nOf course, it is not identical to the real SHAP library. They probably have a much nicer implementation.\n\n![...](/images/sensitivity/sens_8.png)\n\nSo how does this work? Well, basically we are interested in feature importance.\nThe simplest way to answer that question would be just run leave-one-out and document where model performance tanks.\nFor the SHAP values to become large, signaling feature importance, two things must happen:\n1. The path must have some variation in the feature $i$, otherwise $x_i[k] - x'_i = 0$.\n2. The gradient should be consistently positive or negative so that the magnitude of the integral grows.\nThe means that not only must the gradient be stable for changes in feature $i$ but also other features along the path.\nThis might also explain why we see $x_3$ and $x_4$ cluster closer to zero as their effect is dependent.\n\nThe second (messy) observation is related to [this nice post on SHAP titled \"Be careful when intrepreting predictive models in search of causal insights.](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html)\nIt essentially states that SHAP values to not infer causality and they show this with a nice subscriber retention example.\nIn their example there are both measured and unmeasured confounding features, both causing skewed insights regarding discounts and ad spend.\n\nTo verify that the neural net suffers from the same issues we recreate the dataset, train a classifier neural net and calculate its gradients.\nBecause care about the directionality of our gradients we average instead of looking at the power. \nThis gives us the aggregated sensivity in the bar chart below:\n\n![...](/images/sensitivity/sens_9.png)\n\nIndeed, it suffers from the same problem as highlighted by their example.\nSo... feature importance and sensitivity analys can be bogus if the causality is whack.\nWhat can be done about this?\nThe article talks about the double-ML approach for observed cofounders, which is the case for ad spend.\nThe steps are:\n1. Train a model to predict ad spend using possible confounders.\n2. Train a model to predict the target (did renew) using the same set of possible confounders.\n3. Train a model to predict the residual of 2. using the residual from 1.\n\nUsing this methodology one finds that there is no additional information in ad spend after what has been predicted from the confounders.\nThe article also mentions redudancy which is a problem of sales calls as a subset of interactions.\nThis means that the model thinks that sales calls are redudant for the modeling but the simplest way to test this is to simply fit a univariate model.\nFor unobserved confounders however, like product need which influences discount, we are out of luck.\nThe article talks about experimental design to identify the true causal nature.\n\nAll of this has inspired me to dive into causal modeling in my next post.\n\nAnother thing to also reflect upon; now we have only looked inwards at the models but another approach is simulation.\nThis is what sensivitity-analysis-through-probing frameworks do. It is often referred to what-if simulations,\nwhich can act as basis of decision for policy or in a control loop.\n\nHowever, with what we have learned about models missing causality or lacking confidence \none understands that it is never as simple as \"train a model and start using it\".\nA lot of effort can be put towards understanding a problem, models of it and the failure modes of those models.","src/content/blog/sensitivity-analysis.mdx","5d540ef3a7db69e8","sensitivity-analysis.mdx","causal-modeling",{"id":82,"data":84,"filePath":89,"digest":90,"legacyId":91,"deferredRender":24},{"title":85,"description":86,"pubDate":87,"tags":88},"Causal modeling","...",["Date","2025-08-05T00:00:00.000Z"],[66],"src/content/blog/causal-modeling.mdx","e44fb983d003b02e","causal-modeling.mdx"]