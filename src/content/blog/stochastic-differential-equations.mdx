---
title: 'Stochastic differential equations in practice'
description: 'I regret skipping this course in university'
pubDate: 2025-09-06
tags: ['learning', 'statistics', 'sde']
---

As anyone with an engineering background I have had my fair share of differential equations.
However, those were only limited to ordinary (ODE) and partial (PDE).
I have noted that stochatic differential equations (SDE) are typically used in finance 
but I never got a chance to sit down with them.
As someone who has looked at time series through the lens of dynamical systems, 
time series models, state space models, (not Markov chain models... yet!),
Fourier transform, wavelets, Bayesian modeling, neural networks and so forth
it only seemed natural to take the next step.

Starting off with the basics, geometric Brownian motion (GBM)
$$
dS_t = \mu S_t dt + \sigma S_t dW_t,
$$
is the classical example of an SDE.
It can be used to describe the Brownian motion $W_t$ with volatility $\sigma$
of a stock price $S_t$ with a drift $\mu$.
The standard form of SDEs is defined with two generic functions $f$ and $g$:
$$
x_t = f(x_t, t) dt + g(x_t, t) dW_t,
$$

In order to numerically integrate this one can use the Euler-Maruyama method as:
$$
x_{t + \Delta t} = x_t + f(x_t, t)\Delta t + g(x_t, t) \Delta W_t,
$$
where the order of the error is $O(\sqrt{\Delta t})$.
When integrating one can sample $\Delta W_t$ from a Gaussian distribution like $\Delta W_t = \sqrt{\Delta t} Z, Z \sim \mathcal{N}(0,1)$.
This is the method that the package `sdeint` uses by default.
There are also other methods for integrating called the Milstein method,
it adds a second-order correction term from Taylor expansion
$$
x_{t + \Delta t} = x_t + f(x_t, t)\Delta t + g(x_t, t)\Delta W_t, + \frac{1}{2} g'(x_t) g(x_t)((\Delta W_t)^2 - \Delta t).
$$

In order to get positivity preservation one can log-transform the problem and apply Euler-Maruyama (called log-Euler).
Alternatively one can truncate $x_t = max(x_t, 0)$ either Euler-Maruyama or Milstein to get truncated methods.
This prevents one weakness of these methods which is that they can become negative, which for example a stock price can not.

Now, all of this is nice but GBM has a closed form solution
$$
S_t = S_0 e^{(\mu - \frac{1}{2}\sigma^2)t + \sigma W_t},
$$
which can be simulated using the code:
```python
M = 5
N = 256
dt = 0.01

X0 = 1
mu = 1
sigma = 1
t = np.arange(dt, N*dt + dt, dt)

Y = np.zeros((N, M))
for i in range(M):
    # Create Brownian Motion
    np.random.seed(i)
    dB = np.sqrt(dt) * np.random.randn(N)
    B  = np.cumsum(dB)
    # Compute exact solution
    Y[:,i] = X0 * np.exp((mu - 0.5 * sigma**2) * t + sigma * B)
```

![...](/images/sde/sde_1.png)

Another SDE is the Vasicek / Ornsteinâ€“Uhlenbeck (OU) process which looks like
$$
dr_t = a(b - r_t)dt + \sigma dW_t,
$$
which is a mean reversion model.
Whereas GBM models growth the mean reversion type models are suitable for long-term stability like interest rates and volatility.
The parameter $a$ is the speed of mean reversion, $b$ is the long term mean and $\sigma$ is the volatility.
It, like GBM, also has an exact solution:
$$
r_{t + \Delta t} = r_t e^{-a \Delta t} + b(1 - e^{-a \Delta t}) + \sigma \sqrt{\frac{1 - e^{-2a \Delta t}}{2a}} Z,\quad Z \sim \mathcal{N}(0,1).
$$

```python
# Vasicek parameters
a = 2.0       # mean reversion speed
b = 0.05      # long-term mean
sigma = 0.1   # volatility
r0 = 0.03     # initial rate
T = 2.56      # horizon
N = 256       # steps
dt = T / N
M = 5         # number of paths

t = np.linspace(0, T, N+1)

paths = np.zeros((N+1 , M))
paths[0,:] = r0
np.random.seed(5)
for m in range(M):
    rt = r0
    for i in range(N):
        z = np.random.randn()
        rt = (
            rt * np.exp(-a*dt) 
            + b * (1 - np.exp(-a*dt)) 
            + sigma * np.sqrt((1 - np.exp(-2*a*dt))/(2*a)) * z
        )
        paths[i+1, m] = rt
```

![...](/images/sde/sde_2.png)

CIR is also a mean reverting process where the diffusion is different from a Vasicek process
$$
dr_t = a(b - r_t)dt + \sigma \sqrt{r_t} dW_t.
$$
The diffusion scales with $\sqrt{r_t}$, which ensures positivity of the process.
While the distribution for Vasicek is normal the CIR distibution is a noncentral chi-squared one.
In order to simulate this one can sample $r_t$ from $r_s$ where $\Delta t = t - s$ as
$$
\begin{align*}
r_t &\sim c \cdot \chi_d^{\prime 2}(\lambda),\\
c &= \frac{\sigma^2 (1 - e^{-a(t-s)})}{4a},\\
d &= \frac{4ab}{\sigma^2},\\
\lambda &= \frac{4a e^{-a(t-s)} r_s}{\sigma^2 (1 - e^{-a(t-s)})}.
\end{align*}
$$

The code for this can look like:

```python
from scipy.stats import ncx2

# CIR parameters
a = 2.0       # mean reversion speed
b = 0.05      # long-term mean
sigma = 0.1   # volatility
r0 = 0.03     # initial rate
T = 2.56      # horizon
N = 256       # steps
dt = T / N
M = 5         # number of paths

t = np.linspace(0, T, N+1)

paths = np.zeros((N+1 , M))
paths[0,:] = r0
np.random.seed(5)
for m in range(M):
    rt = r0
    for i in range(N):
        c = (sigma**2 * (1 - np.exp(-a*dt))) / (4*a)
        d = 4*a*b/sigma**2
        lam = (4*a*np.exp(-a*dt)*rt) / (sigma**2*(1-np.exp(-a*dt)))
        # sample from noncentral chi-square
        rt = c * ncx2.rvs(d, lam)
        paths[i+1, m] = rt
```

![...](/images/sde/sde_3.png)



Heston is an extension for GBM where one assumes that the variance can drift
$$
\begin{align*}
dS_t &= \mu S_t dt + \sqrt{v_t} S_t dW_t^S,\\
dv_t &= \kappa(\theta - v_t) dt + \xi \sqrt{v_t} dW_t^v,\\
dW_t^S dW_t^v &= \rho dt.
\end{align*}
$$

Note that the variance $v_t$ is modelled by a CIR process and the noise $dW_t^S, dW_t^v$ is correlated by a factor $\rho$.
Heston does not have a closed form solution but neither Euler-Maruyama nor Milstein is optimal for integration.
Instead there is a specific scheme called the Andersen quadratic exponential (QE) scheme.
It starts off by approximating the non-central chi-square distribution for $v_t$ using a QE distribution,
depending on the conditional mean $m$ and variance $s^2$ and using them to define the ratio $\psi$:
$$
\begin{align*}
m &= \theta + (v_t - \theta) e^{-\kappa \Delta t},\\
s^2 &= \frac{v_t \sigma^2 e^{-\kappa \Delta t}}{\kappa}(1 - e^{-\kappa \Delta t}) + \frac{\theta \sigma^2}{2\kappa}(1 - e^{-\kappa \Delta t})^2,\\
\psi &= \frac{m^2}{s^2}.
\end{align*}
$$

Then based on a threshold $\psi_c \approx 1.5$ one either samples from the quadratic when $\psi \leq \psi_c$:
$$
\begin{align*}
v_{t + \Delta t} &= a(b + Z)^2,\quad Z \sim \mathcal{N}(0,1),\\
b &= \frac{\psi - 1}{2},\\
a &= \frac{m}{1 + b^2},
\end{align*}
$$

or from the exponential when $\psi > \psi_c$:
$$
\begin{align*}
v_{t + \Delta t} &= 
\begin{cases}
0,\text{with probability}\:p\\
\frac{1}{\beta} ln(\frac{1-p}{U}),\>\text{with probability}\:1-p
\end{cases}\\
p &= \frac{\psi-1}{\psi+1},\\
\beta &= \frac{1-p}{m}.
\end{align*}
$$

Once we have this $v_t$ estimate the estimation of $S_t$ is done using log Euler, given $X_t = ln(S_t)$
$$
X_{t + \Delta t} = X_t + (r - \frac{1}{2}\bar{v})\Delta t + \sqrt{\bar{v} \Delta t}Z_x,
$$
where $Z_x$ is a standard normal correlated with the variance driver $\rho$ and $\bar{v}$ is the average of $v_t$ and $v_{t + \Delta t}$ (for stability).
To create correlated noise, draw two samples $Z_1, Z_2 \sim \mathcal{N}(0,1)$ and define $Z_v = Z_1$ and $Z_s = \rho \Z_1 + \sqrt{1 - \rho^2} Z_2$.
We can use $Z_v$ for $v_t$ and $Z_s$ for $S_t$/$X_t$.
For the exponential branch of QE the recommendation is to do an independent draw rather than running $Z_s$ through the inverse cumulative density function.

```python
# Parameters and simulation
S0 = 100.0
v0 = 0.04
r = 0.03
q = 0.0
kappa = 1.5
theta = 0.04
sigma = 0.3
rho = -0.7
T = 1.0
N = 252  # daily steps for a year
M = 5    # requested number of paths
psi_c = 1.5

dt = T / N
sqrt_dt = np.sqrt(dt)
times = np.linspace(0, T, N+1)

S = np.zeros((N+1, M))
v = np.zeros((N+1, M))
S[0, :] = S0
v[0, :] = v0

for n in range(N):
    # generate independent normals for each path
    U1 = np.random.normal(size=M)  # will be used as the "variance normal" (Zv)
    U2 = np.random.normal(size=M)  # independent, to build the asset normal (Zs)
    
    # correlated normal for asset's Brownian step
    Z_v = U1
    Z_s = rho * U1 + np.sqrt(1 - rho**2) * U2
    
    v_n = v[n, :]
    
    # Precompute exponentials used in moment formulas
    exp_kdt = np.exp(-kappa * dt)
    m = theta + (v_n - theta) * exp_kdt
    # s2 (variance of v_{n+1} conditional on v_n)
    s2 = (v_n * sigma**2 * exp_kdt * (1 - exp_kdt) / kappa
            + theta * sigma**2 / (2 * kappa) * (1 - exp_kdt)**2)
    
    # Ensure non-negative moments numerically
    m = np.maximum(m, 0.0)
    s2 = np.maximum(s2, 0.0)
    
    psi = np.zeros_like(m)
    # avoid division by zero
    idx_nonzero = m > 0
    psi[idx_nonzero] = s2[idx_nonzero] / (m[idx_nonzero]**2)
    psi[~idx_nonzero] = np.inf  # if m==0, force exponential branch
    
    v_next = np.zeros_like(v_n)
    
    # Branch 1: psi <= psi_c  (central region) -> Non-central chi-square approx via squared normal
    idx1 = psi <= psi_c
    if np.any(idx1):
        psi1 = psi[idx1]
        m1 = m[idx1]
        # compute b and a as in Andersen
        tmp = 2.0 / psi1 - 1.0
        # safe positive sqrt for the nested expression
        b = np.sqrt(tmp + np.sqrt(tmp * (tmp + 2.0)))
        a = m1 / (1.0 + b**2)
        Zv = Z_v[idx1]
        v_next[idx1] = a * (b + Zv)**2
    
    # Branch 2: psi > psi_c -> mixture with a point mass at zero and exponential tail
    idx2 = psi > psi_c
    if np.any(idx2):
        psi2 = psi[idx2]
        m2 = m[idx2]
        p = (psi2 - 1.0) / (psi2 + 1.0)
        beta = (1.0 - p) / m2
        # draw uniforms to decide zero vs exp
        U = np.random.uniform(size=p.size)
        is_zero = U <= p
        # where it's zero
        v_next[idx2][is_zero] = 0.0
        # else exponential sample
        not_zero_idx = ~is_zero
        if np.any(not_zero_idx):
            U2_exp = np.random.uniform(size=not_zero_idx.sum())
            v_next[idx2][not_zero_idx] = -np.log(U2_exp) / beta[not_zero_idx]
    
    # Numerical floor to avoid tiny negative values
    v_next = np.maximum(v_next, 0.0)
    
    # Asset update using v_bar = 0.5*(v_n + v_next)
    v_bar = 0.5 * (v_n + v_next)
    # To avoid negative variance under sqrt due to numerical issues
    v_bar = np.maximum(v_bar, 0.0)
    
    # Log-asset update (Euler-like with correlated normals)
    # dX = (r - q - 0.5 v_bar) dt + sqrt(v_bar * dt) * Z_s
    diffusion = np.sqrt(v_bar * dt) * Z_s
    drift = (r - q - 0.5 * v_bar) * dt
    S[n+1, :] = S[n, :] * np.exp(drift + diffusion)
    v[n+1, :] = v_next
```

![...](/images/sde/sde_4.png)

![...](/images/sde/sde_5.png)



While Heston adds complexity to GBM using variance drift; there are other ways.
Merton Jump-Diffusion includes a "jump" process $J_tdN_t$ where $J_t$ is the jump size and $dN_t$ is a Poisson process.
The SDE looks like
$$
dS_t = \mu S_t dt + \sigma S_t dW_t + J_t S_t dN_t.
$$ 

The discrete-time update over a step $\Delta t$ is
$$
S_{t + \Delta t} = S_t e^{(\mu - \lambda \kappa - \frac{1}{2}\sigma^2) \Delta t + \sigma \Delta W} \prod_{i = 1}^{N_{\Delta t}} e^{Y_i},
$$
where $\Delta W \sim \mathcal{N}(0,\Delta t)$, $N_{\Delta t} \sim \text{Poisson}(\lambda \Delta t)$, $Y_i \sim \mathcal{N}(\mu_J, \sigma_J^2)$
and $\kappa = e^{\mu_J + \frac{1}{2}\sigma_J^2}-1$.
There are multiple ways to simulate this; using a fix $\Delta t$ using the method above, 
if $\lambda \Delta t$ is small one can approximate the Poisson process using Bernoulli($\lambda \Delta t$),
or one can generate all the jumps and integrate between those times. Some example code:

```python
S0 = 100 # initial price
mu = 0.05 # drift (real-world or risk-neutral depending on use)
sigma = 0.2 # diffusion volatility
lam = 0.3 # jump intensity (lambda)
mu_j = -0.1 # normal params for jump log-size Y ~ N(mu_j, sigma_j^2)
sigma_j = 0.25 # normal params for jump log-size Y ~ N(mu_j, sigma_j^2)
T = 2.52*3

M = 5
N = 1000

rng = np.random.default_rng(5)
dt = T / N
t = np.arange(N)*dt
kappa = np.exp(mu_j + 0.5 * sigma_j**2) - 1.0

S = np.full((N+1, M), S0, dtype=float)
sqrt_dt = np.sqrt(dt)

for i in range(N):
    # diffusion increments
    dW = rng.normal(0.0, sqrt_dt, size=M)
    # Poisson counts for each path in interval
    Nt = rng.poisson(lam * dt, size=M)
    # jump multipliers
    J = np.ones(M, dtype=float)
    # for paths with at least one jump, sample sum of Y's
    idx = np.where(Nt > 0)[0]
    if idx.size:
        # sample total jump (sum of Nt normals) efficiently:
        # total Y_sum ~ Normal(Nt*mu_j, Nt*sigma_j^2)
        Ns = Nt[idx]
        Y_sums = rng.normal(loc=Ns*mu_j, scale=np.sqrt(Ns)*sigma_j)
        J[idx] = np.exp(Y_sums)

    # continuous update (compensated drift)
    cont = (mu - lam * kappa - 0.5 * sigma**2) * dt + sigma * dW
    S[i+1,:] = S[i,:] * np.exp(cont) * J
```

![...](/images/sde/sde_6.png)

Another type of SDE is the SABR (stochastic alpha, beta, rho) volatility model.
Instead of asset prices $S_t$ or volatility $v_t$ it models a forward $F_t$.
One does not typically not integrate it directly, instead there are asymptotic solutions that are used for, e.g., call option pricing.
The SDE looks like
$$
\begin{align*}
dF_t &= \alpha_t F_t^\beta dW_t,\\
d\alpha_t &= \nu \alpha_t dZ_t,\\
w_t Z_t &= \rho dt,\\
\end{align*}
$$
where $\alpha_t$ is the volatility, $\beta$ is an elasticity parameter, $\nu$ is the volatility of the volatility
and $\rho$ is the correlation between the Browian motions.
The SDE is designed to capture the "smile", the U-shape of volatility meaning.
For forward simulation one can use the following code:
```python
# SABR parameters
F0 = 0.03      # initial forward rate (3%)
alpha0 = 0.2   # initial volatility
beta = 0.5     # elasticity parameter
nu = 0.4       # vol-of-vol
rho = -0.3     # correlation
T = 1.0        # maturity (in years)
N = 252        # time steps (daily)
dt = T / N
n_paths = 5

# Pre-allocate arrays
F_paths = np.zeros((N+1, n_paths))
alpha_paths = np.zeros((N+1, n_paths))
t = dt*np.arange(N)

# Initial conditions
F_paths[:, 0] = F0
alpha_paths[:, 0] = alpha0

# Cholesky for correlation
cov = np.array([[1, rho], [rho, 1]])
L = np.linalg.cholesky(cov)

np.random.seed(5)
# Simulate paths
for m in range(n_paths):
    F, alpha = F0, alpha0
    for i in range(1, N+1):
        z = np.random.randn(2)
        dW, dZ = L @ z * np.sqrt(dt)
        # Update alpha (lognormal diffusion)
        alpha = alpha * np.exp(-0.5 * nu**2 * dt + nu * dZ)
        # Update forward
        F = F + alpha * (F**beta) * dW
        # Reflection to prevent negative values
        F = abs(F)
        # Store
        F_paths[i, m] = F
        alpha_paths[i, m] = alpha
```

Note that with our reflection or truncation $F_t$ can be come negative, which becomes a problem for $\beta \neq 0, 1$.
To avoid complex solutions we go for the reflection method in this example, yielding the following curves.

![...](/images/sde/sde_7.png)

Hagan's SABR implied volatility asymptotic solution can be implemented in code as:
```python
def sabr_implied_vol(
        F, # Initial F
        K, # Strike
        T, # Time period
        alpha, 
        beta, 
        rho, 
        nu
    ):
    if abs(F - K) < 1e-12:
        term1 = ((1 - beta)**2 * alpha**2) / (24 * (F**(2 - 2*beta)))
        term2 = (rho * beta * nu * alpha) / (4 * (F**(1 - beta)))
        term3 = ((2 - 3*rho**2) * nu**2) / 24
        vol = (alpha / (F**(1 - beta))) * (1 + (term1 + term2 + term3) * T)
        return vol

    logFK = np.log(F / K)
    A = (F * K)**((1 - beta) / 2)
    D = 1 + ((1 - beta)**2 / 24) * logFK**2 + ((1 - beta)**4 / 1920) * logFK**4
    z = (nu / alpha) * A * logFK
    x_z = np.log((np.sqrt(1 - 2*rho*z + z**2) + z - rho) / (1 - rho))
    term1 = ((1 - beta)**2 * alpha**2) / (24 * A**2)
    term2 = (rho * beta * nu * alpha) / (4 * A)
    term3 = ((2 - 3*rho**2) * nu**2) / 24
    vol = (alpha / (A * D)) * (z / x_z) * (1 + (term1 + term2 + term3) * T)
    return vol
```

## Fitting stochastic differential equations to data

While generating data using models is fun, using said models to understand data is *funner*.

For GBM and Vasicek there are closed form maximum likelihood estimators (MLE).
For Vasicek it is:
```python
n = len(X)
X_prev, X_next = X[:-1], X[1:]
X_prev_mean = X_prev.mean()
X_next_mean = X_next.mean()

# Regression for AR(1) representation
Sxx = np.sum((X_prev - X_prev_mean) ** 2)
Sxy = np.sum((X_prev - X_prev_mean) * (X_next - X_next_mean))

a_hat = Sxy / Sxx
b_hat = X_next_mean - a_hat * X_prev_mean

# Residual variance
residuals = X_next - (a_hat * X_prev + b_hat)
sigma_eps2 = np.sum(residuals ** 2) / (n-1)

# Map back to SDE parameters
kappa_hat = -np.log(a_hat) / dt
theta_hat = b_hat / (1 - a_hat)
sigma_hat = np.sqrt(2 * kappa_hat * sigma_eps2 / (1 - a_hat**2))

print(kappa_hat, theta_hat, sigma_hat)
```

And for GBM:
```python
n = len(S)
R = np.diff(np.log(S))  # log-returns

R_bar = np.mean(R)
sigma2_hat = np.sum((R - R_bar) ** 2) / (n * dt)
sigma_hat = np.sqrt(sigma2_hat)
mu_hat = R_bar / dt + 0.5 * sigma2_hat

print(mu_hat, sigma_hat)
```

Note that these are just point estimates.
To get confidence intervals one can derive them analytically for these examples or we look at another solution scheme.
A more general way to approach SDE calibration is through a Bayesian framework.
Take the CIR process, which does not have a closed form MLE because of its noncentral chi-squared distribution
$$
dr_t = a(b - r_t)dt + \sigma \sqrt{r_t} dW_t,
$$
if we throw the Euler-Maruyama method at it we get
$$
\begin{align*}
r_{t + \Delta t} &= r_t + a(b - r_t) \Delta t + \sigma \sqrt{r_t \Delta t} Z, \quad Z \sim \mathcal{N}(0,1),\\
r_{t + \Delta t} &\sim \mathcal{N}(a(b - r_t) \Delta t, \sigma^2 r_t \Delta t).
\end{align*}
$$

This is something we can construct in, say PyMC, and sample posterior distributions for $a, b, \sigma$.
```python
import pymc as pm
import arviz as az

with pm.Model() as cir_model:
    # Priors
    theta = pm.HalfNormal("theta", sigma=1.0)
    mu = pm.HalfNormal("mu", sigma=0.1)
    sigma = pm.HalfNormal("sigma", sigma=0.1)
    
    # Likelihood
    X_prev = X[:-1]
    X_next = X[1:]
    mean = X_prev + theta*(mu - X_prev)*dt
    sd = sigma*np.sqrt(np.maximum(X_prev, 0)*dt)
    
    pm.Normal("obs", mu=mean, sigma=sd, observed=X_next)
    
    # Inference
    trace = pm.sample(2000, tune=1000, target_accept=0.9, return_inferencedata=True)

az.plot_trace(trace)
az.summary(trace, hdi_prob=0.95)
```

The output for one simulated CIR with $a = 0.3, b = 0.05, \sigma = 0.06$ is:
|           | **mean** | **sd** | **hdi_2.5%** | **hdi_97.5%** |
|-----------|----------|--------|--------------|---------------|
| **a**     | 0.701    | 0.441  | 0.001        | 1.492         |
| **b**     | 0.040    | 0.017  | 0.010        | 0.066         |
| **sigma** | 0.057    | 0.001  | 0.054        | 0.059         |

By sampling from the posterior distributions we can see alternate trajectories for this SDE:
```python
# Extract posterior samples
a_samples = trace.posterior['a'].stack(sample=("chain","draw")).values
b_samples = trace.posterior['b'].stack(sample=("chain","draw")).values
sigma_samples = trace.posterior['sigma'].stack(sample=("chain","draw")).values
```
![...](/images/sde/sde_8.png)

This method even works for jump processes.
Here example of a Vasicek + jumps process
$$
\begin{align*}
dC_t &= \alpha(\mu - C_t) dt + \sigma dW_t + dJ_t,\\
\xrightarrow[\text{+ jump-as-mixture}]{\text{Euler-Maruyama}}\:&\approx \begin{cases}
C_{t+\Delta t} = C_t + \alpha(\mu - C_t) \Delta t + \sigma \sqrt{\Delta t} Z, \text{with probability}\:1-p_J\\
C_{t+\Delta t} = C_t + \alpha(\mu - C_t) \Delta t + \sigma_J Z, \text{with probability}\:p_J
\end{cases}\\
&= \begin{cases}
C_{t+\Delta t} \sim \mathcal{N}(C_t + \alpha(\mu - C_t) \Delta t, \sigma^2 \Delta t), \text{with probability}\:1-p_J\\
C_{t+\Delta t} \sim \mathcal{N}(C_t + \alpha(\mu - C_t) \Delta t, \sigma_J^2 \Delta t), \text{with probability}\:p_J
\end{cases}\\
\end{align*}
$$

The approximation of Euler-Maruyama has now been baked together with the assumption that diffusion $\ll$ jump
and that the jumps are rare enough to be modelled by a Bernoulli distribution rather than a Poisson distribution.
These assumptions can be tuned further but the code for this example turns out nice and simple:

```python
import pymc as pm
import pytensor.tensor as pt
import arviz as az

with pm.Model() as ou_jump_model:
    # Priors
    alpha = pm.Exponential('alpha', 1/0.1)
    mu = pm.Normal('mu', mu=1e5, sigma=5e4)
    sigma = pm.HalfNormal('sigma', sigma=1e4)

    p_jump = pm.Beta('p_jump', alpha=1, beta=20)
    mu_jump = pm.Normal('mu_jump', mu=-2e4, sigma=2e4)
    sigma_jump = pm.HalfNormal('sigma_jump', sigma=2e4)

    # Mixture likelihood
    C_prev = C[:-1]
    C_next = C[1:]
    mean = C_prev + alpha * (mu - C_prev) * dt
    y = pm.Mixture(
        "delta_C_obs",
        w=[1 - p_jump, p_jump],
        comp_dists=[
            pm.Normal.dist(mu=mean, sigma=sigma*np.sqrt(dt)),
            pm.Normal.dist(mu=mean + mu_jump, sigma=sigma_jump)
        ],
        observed=C_next
    )

    # Inference
    trace_jump = pm.sample(draws=2000, tune=1500, target_accept=0.95)

# Summarize
print(az.summary(trace_jump, var_names=['alpha','mu','sigma','p_jump','mu_jump','sigma_jump']))
az.plot_trace(trace_jump, var_names=['alpha','mu','sigma','p_jump'])
plt.show()
```

So if we simulate some data:
```python
# --- Parameters ---
T = 365           # days
alpha = 0.05      # mean reversion speed
mu = 100000       # long-run mean
sigma = 5000      # diffusion volatility
dt = 0.25         # timestep
C0 = 80000        # initial
N = int(T/dt)+1
t = np.arange(N)*dt

# Jump parameters
jump_prob = 0.02   # daily prob of a jump
jump_mean = -20000 # average jump size (defaults, negative)
jump_sd = 10000    # variability in jump size

# --- Simulation ---
C = [C0]
for t in range(N):
    prev = C[-1]
    drift = alpha * (mu - prev) * dt
    diffusion = sigma * np.sqrt(dt) * np.random.randn()
    jump = 0
    if np.random.rand() < jump_prob*dt:
        jump = np.random.normal(jump_mean, jump_sd)
    new_val = prev + drift + diffusion + jump
    C.append(max(new_val, 0))
```

![...](/images/sde/sde_9.png)

We can see that the code above estimates good ranges for this SDE, despite the jumps:
| ****           | **mean**   | **sd**   | **hdi_3%** | **hdi_97%** |
|----------------|------------|----------|------------|-------------|
| **alpha**      | 0.050      | 0.014    | 0.023      | 0.077       |
| **mu**         | 99456.161  | 7529.641 | 86656.148  | 113070.527  |
| **sigma**      | 5095.827   | 98.195   | 4916.529   | 5284.484    |
| **p_jump**     | 0.007      | 0.003    | 0.003      | 0.013       |
| **mu_jump**    | -22737.511 | 4577.910 | -30153.094 | -13448.753  |
| **sigma_jump** | 10147.951  | 3702.079 | 4136.082   | 16752.985   |